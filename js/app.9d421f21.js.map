{"version":3,"sources":["webpack:///webpack/bootstrap","webpack:///./public/assets/adjacent-projects.png","webpack:///./public/assets/human-risk-label-distribution.png","webpack:///./src/App.vue?7e02","webpack:///./src/components/project-pages/ClimateChangeNews.vue?f5a3","webpack:///./public/assets/identified-elbow.png","webpack:///./public/assets/IDS131_Final_Report.pdf","webpack:///./public/assets/svm-hyperparameters.png","webpack:///./public/assets/informativeness_per_class_metric.png","webpack:///./public/assets/text-analysis-module.png","webpack:///./public/assets/flood_presence_per_class_metric.png","webpack:///./public/assets/human-risk-data-splits.png","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue?0850","webpack:///./public/assets/text-features-for-clustering.png","webpack:///./src/components/project-pages/Boomerang.vue?f98c","webpack:///./public/assets/fp-train.png","webpack:///./public/assets sync ^\\.\\/.*$","webpack:///./public/assets/human-risk-model-evaluation.png","webpack:///./public/assets/6_864_Project.pdf","webpack:///./public/assets/nested-cv-graph.png","webpack:///./public/assets/fp-fc.png","webpack:///./src/components/NavBar.vue?d132","webpack:///./public/assets/int-dev-results.png","webpack:///./public/assets/human-risk-diagram.png","webpack:///./public/assets/ner-res-tools.png","webpack:///./public/assets/agg-metrics-fc.png","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue?072d","webpack:///./src/components/NotFoundComponent.vue?dc5a","webpack:///./src/components/Home.vue?3443","webpack:///./src/components/Header.vue?5705","webpack:///./public/assets/ner-results.png","webpack:///./public/assets/FrequencyPlot.png","webpack:///./public/assets/workshop-insights.png","webpack:///./public/assets/n-gram-preprocessing.png","webpack:///./public/assets/final-assessment.png","webpack:///./public/assets/negative-positive.jpg","webpack:///./src/components/project-pages/GNNsTaxiPrediction.vue?c985","webpack:///./public/assets/RelativeWordFrequencyDiff.png","webpack:///./public/assets/17_835_Poster.pdf","webpack:///./public/assets/bert-preprocessing.png","webpack:///./public/assets/portrait.jpg","webpack:///./public/assets/ds-fc.png","webpack:///./public/assets/int-dev-gray-lit.pdf","webpack:///./public/assets/unlabeled-clusters.png","webpack:///./public/assets/humanitarian_categories_per_class_metric.png","webpack:///./public/assets/human-risk-aucpr.png","webpack:///./src/components/Resume.vue?fbd6","webpack:///./src/App.vue?ee38","webpack:///./src/constants.js","webpack:///./src/components/Header.vue?70d1","webpack:///src/components/Header.vue","webpack:///./src/components/Header.vue?4c35","webpack:///./src/components/Header.vue","webpack:///./src/components/NavBar.vue?dea2","webpack:///src/components/NavBar.vue","webpack:///./src/components/NavBar.vue?e8e4","webpack:///./src/components/NavBar.vue","webpack:///src/App.vue","webpack:///./src/App.vue?1160","webpack:///./src/App.vue?bff9","webpack:///./src/components/Home.vue?1e14","webpack:///src/components/Home.vue","webpack:///./src/components/Home.vue?705d","webpack:///./src/components/Home.vue","webpack:///./src/components/About.vue?754c","webpack:///src/components/About.vue","webpack:///./src/components/About.vue?f71c","webpack:///./src/components/About.vue","webpack:///./src/components/Projects.vue?d662","webpack:///./src/components/ProjectCard.vue?19dd","webpack:///src/components/ProjectCard.vue","webpack:///./src/components/ProjectCard.vue?df61","webpack:///./src/components/ProjectCard.vue","webpack:///src/components/Projects.vue","webpack:///./src/components/Projects.vue?e76a","webpack:///./src/components/Projects.vue","webpack:///./src/components/Artwork.vue?cecf","webpack:///src/components/Artwork.vue","webpack:///./src/components/Artwork.vue?f604","webpack:///./src/components/Artwork.vue?b935","webpack:///./src/components/Resume.vue?0f48","webpack:///src/components/Resume.vue","webpack:///./src/components/Resume.vue?960c","webpack:///./src/components/Resume.vue","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue?4550","webpack:///src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue?6e85","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue?dfab","webpack:///src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue?0ff0","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue?14c9","webpack:///src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue?8e4b","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue","webpack:///./src/components/project-pages/NLPIntDevGrayLit.vue?4ff4","webpack:///src/components/project-pages/NLPIntDevGrayLit.vue","webpack:///./src/components/project-pages/NLPIntDevGrayLit.vue?6524","webpack:///./src/components/project-pages/NLPIntDevGrayLit.vue","webpack:///./src/components/project-pages/GNNsTaxiPrediction.vue?5aeb","webpack:///src/components/project-pages/GNNsTaxiPrediction.vue","webpack:///./src/components/project-pages/GNNsTaxiPrediction.vue?b393","webpack:///./src/components/project-pages/GNNsTaxiPrediction.vue","webpack:///./src/components/project-pages/TrumpSpeechAnalysis.vue?4ad6","webpack:///src/components/project-pages/TrumpSpeechAnalysis.vue","webpack:///./src/components/project-pages/TrumpSpeechAnalysis.vue?ec8f","webpack:///./src/components/project-pages/TrumpSpeechAnalysis.vue","webpack:///./src/components/project-pages/ClimateChangeNews.vue?7cca","webpack:///src/components/project-pages/ClimateChangeNews.vue","webpack:///./src/components/project-pages/ClimateChangeNews.vue?de66","webpack:///./src/components/project-pages/ClimateChangeNews.vue","webpack:///./src/components/project-pages/Boomerang.vue?cc67","webpack:///src/components/project-pages/Boomerang.vue","webpack:///./src/components/project-pages/Boomerang.vue?f111","webpack:///./src/components/project-pages/Boomerang.vue","webpack:///./src/components/NotFoundComponent.vue?7a6e","webpack:///src/components/NotFoundComponent.vue","webpack:///./src/components/NotFoundComponent.vue?7b7a","webpack:///./src/components/NotFoundComponent.vue","webpack:///./src/router.js","webpack:///./src/main.js","webpack:///./public/assets/iaa.png","webpack:///./public/assets/human-risk-per-class-metric.png","webpack:///./public/assets/boomerang-home.jpg","webpack:///./public/assets/bert-features.png","webpack:///./public/assets/test-set-eval.png","webpack:///./public/assets/network_tfidf_wordclouds.png","webpack:///./public/assets/character_box_and_whisk_fc_rm.png","webpack:///./public/assets/clustering-pipeline.png","webpack:///./public/assets/elbow-plot.png","webpack:///./public/assets/in-fc.png","webpack:///./public/assets/flood_confusion_matrix.png","webpack:///./public/assets/informativeness_confusion_matrix.png","webpack:///./src/components/About.vue?e37a","webpack:///./public/assets/cluster-labels.png","webpack:///./src/components/project-pages/NLPIntDevGrayLit.vue?b8aa","webpack:///./public/assets/text-preprocessing.png","webpack:///./public/assets/tf-idf-features.png","webpack:///./public/assets/image-analysis-module-modified.png","webpack:///./public/assets/NER.png","webpack:///./public/assets/Dylan_Lewis_Resume.pdf","webpack:///./public/assets/workshop-preface-questions.png","webpack:///./public/assets/human-risk-cm-svm.png","webpack:///./public/assets/qualitative-summaries.png","webpack:///./public/assets/fare-surge-graph-pred.png","webpack:///./public/assets/preliminary-assessment.png","webpack:///./public/assets/years_cosine_similarity.png","webpack:///./public/assets/nested-cv-table.png","webpack:///./public/assets/trump-campaign.png","webpack:///./public/assets/project-collage.png","webpack:///./public/assets/tfidf-networks.png","webpack:///./public/assets/hc-fc.png","webpack:///./public/assets/damage_severity_per_class_metric.png","webpack:///./public/assets/ds-train.png","webpack:///./public/assets/txt-data-collection.png","webpack:///./public/assets/pika-gif.gif","webpack:///./public/assets/join-communities.png","webpack:///./public/assets/resume.png","webpack:///./public/assets/reptile.png","webpack:///./public/assets/masters-thesis-overview.png","webpack:///./public/assets/damage_severity_confusion_matrix.png","webpack:///./public/assets/network_cosine_similarity.png","webpack:///./public/assets/IDS131_Poster.pdf","webpack:///./public/assets/in-train.png","webpack:///./public/assets/hc-train.png","webpack:///./public/assets/bag-of-word-features.png","webpack:///./public/assets/feelings.jpg","webpack:///./public/assets/dylan-n-leo.jpg","webpack:///./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue?abca","webpack:///./public/assets/networks_and_years_cosine_similarity.png","webpack:///./public/assets/linkedin-profpic.jpg","webpack:///./public/assets/query-subset.png","webpack:///./src/components/project-pages/TrumpSpeechAnalysis.vue?ec50","webpack:///./src/components/ProjectCard.vue?9882","webpack:///./public/assets/humanitarian_categories_confusion_matrix.png","webpack:///./public/assets/image-analysis-module.png","webpack:///./public/assets/final-project-overview.png","webpack:///./public/assets/create-account.png","webpack:///./public/assets/algo-selection-table.png","webpack:///./public/assets/taxi-proj-thumbnail.png","webpack:///./public/assets/leo_n_me.jpg","webpack:///./public/assets/labeled-clusters.png","webpack:///./public/assets/tfidf-features.png","webpack:///./public/assets/RelativeWordFreqDiffFlorida.png","webpack:///./public/assets/taxi-fare-and-surge-pred.png","webpack:///./public/assets/clustering-hyperparameters.png","webpack:///./src/components/Artwork.vue?73f1","webpack:///./public/assets/nested-cv.png"],"names":["webpackJsonpCallback","data","moduleId","chunkId","chunkIds","moreModules","executeModules","i","resolves","length","Object","prototype","hasOwnProperty","call","installedChunks","push","modules","parentJsonpFunction","shift","deferredModules","apply","checkDeferredModules","result","deferredModule","fulfilled","j","depId","splice","__webpack_require__","s","installedModules","exports","module","l","m","c","d","name","getter","o","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","p","jsonpArray","window","oldJsonpFunction","slice","map","webpackContext","req","id","webpackContextResolve","e","Error","code","keys","resolve","render","_vm","this","_h","$createElement","_c","_self","staticClass","attrs","$route","path","_e","staticRenderFns","MAIN_PROJECTS","link","src","imgFilename","title","desc","projectWebsite","btnText","url","sizeClass","ML_MODULES","projectBtnText","scrollUpFunc","scrollTo","enableScrollUpOnCarousel","carouselID","$","on","enableSwipeOnCarousel","event","xClick","originalEvent","touches","pageX","one","xMove","sensitivityInPx","Math","floor","carousel","off","goHome","_v","_m","methods","component","components","Header","NavBar","goToAbout","goToResume","goToProjects","goToArtwork","windowWidth","innerWidth","$router","scrollUp","_l","project","_s","class","getImgURL","$event","goToProjectPage","ProjectCard","Vue","use","VueRouter","router","Home","About","Projects","Boomerang","MLForCrowdsourcedCrisisData","ImageAnalysisCarousel","TextAnalysisCarousel","NLPIntDevGrayLit","TrumpSpeechAnalysis","GNNsTaxiPrediction","ClimateChangeNews","Artwork","Resume","NotFoundComponent","redirect","vueRouter","routes","config","productionTip","BootstrapVue","IconsPlugin","h","App","$mount"],"mappings":"aACE,SAASA,EAAqBC,GAQ7B,IAPA,IAMIC,EAAUC,EANVC,EAAWH,EAAK,GAChBI,EAAcJ,EAAK,GACnBK,EAAiBL,EAAK,GAIHM,EAAI,EAAGC,EAAW,GACpCD,EAAIH,EAASK,OAAQF,IACzBJ,EAAUC,EAASG,GAChBG,OAAOC,UAAUC,eAAeC,KAAKC,EAAiBX,IAAYW,EAAgBX,IACpFK,EAASO,KAAKD,EAAgBX,GAAS,IAExCW,EAAgBX,GAAW,EAE5B,IAAID,KAAYG,EACZK,OAAOC,UAAUC,eAAeC,KAAKR,EAAaH,KACpDc,EAAQd,GAAYG,EAAYH,IAG/Be,GAAqBA,EAAoBhB,GAE5C,MAAMO,EAASC,OACdD,EAASU,OAATV,GAOD,OAHAW,EAAgBJ,KAAKK,MAAMD,EAAiBb,GAAkB,IAGvDe,IAER,SAASA,IAER,IADA,IAAIC,EACIf,EAAI,EAAGA,EAAIY,EAAgBV,OAAQF,IAAK,CAG/C,IAFA,IAAIgB,EAAiBJ,EAAgBZ,GACjCiB,GAAY,EACRC,EAAI,EAAGA,EAAIF,EAAed,OAAQgB,IAAK,CAC9C,IAAIC,EAAQH,EAAeE,GACG,IAA3BX,EAAgBY,KAAcF,GAAY,GAE3CA,IACFL,EAAgBQ,OAAOpB,IAAK,GAC5Be,EAASM,EAAoBA,EAAoBC,EAAIN,EAAe,KAItE,OAAOD,EAIR,IAAIQ,EAAmB,GAKnBhB,EAAkB,CACrB,IAAO,GAGJK,EAAkB,GAGtB,SAASS,EAAoB1B,GAG5B,GAAG4B,EAAiB5B,GACnB,OAAO4B,EAAiB5B,GAAU6B,QAGnC,IAAIC,EAASF,EAAiB5B,GAAY,CACzCK,EAAGL,EACH+B,GAAG,EACHF,QAAS,IAUV,OANAf,EAAQd,GAAUW,KAAKmB,EAAOD,QAASC,EAAQA,EAAOD,QAASH,GAG/DI,EAAOC,GAAI,EAGJD,EAAOD,QAKfH,EAAoBM,EAAIlB,EAGxBY,EAAoBO,EAAIL,EAGxBF,EAAoBQ,EAAI,SAASL,EAASM,EAAMC,GAC3CV,EAAoBW,EAAER,EAASM,IAClC3B,OAAO8B,eAAeT,EAASM,EAAM,CAAEI,YAAY,EAAMC,IAAKJ,KAKhEV,EAAoBe,EAAI,SAASZ,GACX,qBAAXa,QAA0BA,OAAOC,aAC1CnC,OAAO8B,eAAeT,EAASa,OAAOC,YAAa,CAAEC,MAAO,WAE7DpC,OAAO8B,eAAeT,EAAS,aAAc,CAAEe,OAAO,KAQvDlB,EAAoBmB,EAAI,SAASD,EAAOE,GAEvC,GADU,EAAPA,IAAUF,EAAQlB,EAAoBkB,IAC/B,EAAPE,EAAU,OAAOF,EACpB,GAAW,EAAPE,GAA8B,kBAAVF,GAAsBA,GAASA,EAAMG,WAAY,OAAOH,EAChF,IAAII,EAAKxC,OAAOyC,OAAO,MAGvB,GAFAvB,EAAoBe,EAAEO,GACtBxC,OAAO8B,eAAeU,EAAI,UAAW,CAAET,YAAY,EAAMK,MAAOA,IACtD,EAAPE,GAA4B,iBAATF,EAAmB,IAAI,IAAIM,KAAON,EAAOlB,EAAoBQ,EAAEc,EAAIE,EAAK,SAASA,GAAO,OAAON,EAAMM,IAAQC,KAAK,KAAMD,IAC9I,OAAOF,GAIRtB,EAAoB0B,EAAI,SAAStB,GAChC,IAAIM,EAASN,GAAUA,EAAOiB,WAC7B,WAAwB,OAAOjB,EAAO,YACtC,WAA8B,OAAOA,GAEtC,OADAJ,EAAoBQ,EAAEE,EAAQ,IAAKA,GAC5BA,GAIRV,EAAoBW,EAAI,SAASgB,EAAQC,GAAY,OAAO9C,OAAOC,UAAUC,eAAeC,KAAK0C,EAAQC,IAGzG5B,EAAoB6B,EAAI,IAExB,IAAIC,EAAaC,OAAO,gBAAkBA,OAAO,iBAAmB,GAChEC,EAAmBF,EAAW3C,KAAKsC,KAAKK,GAC5CA,EAAW3C,KAAOf,EAClB0D,EAAaA,EAAWG,QACxB,IAAI,IAAItD,EAAI,EAAGA,EAAImD,EAAWjD,OAAQF,IAAKP,EAAqB0D,EAAWnD,IAC3E,IAAIU,EAAsB2C,EAI1BzC,EAAgBJ,KAAK,CAAC,EAAE,kBAEjBM,K,yFCvJTW,EAAOD,QAAU,IAA0B,sC,uBCA3CC,EAAOD,QAAU,IAA0B,kD,oCCA3C,yBAAwb,EAAG,G,oCCA3b,yBAAkgB,EAAG,G,uBCArgBC,EAAOD,QAAU,IAA0B,qC,uBCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,qD,uBCA3CC,EAAOD,QAAU,IAA0B,yC,uBCA3CC,EAAOD,QAAU,IAA0B,oD,uBCA3CC,EAAOD,QAAU,IAA0B,2C,oCCA3C,yBAAwhB,EAAG,G,gDCA3hBC,EAAOD,QAAU,IAA0B,iD,oCCA3C,yBAA0f,EAAG,G,uBCA7fC,EAAOD,QAAU,IAA0B,6B,qBCA3C,IAAI+B,EAAM,CACT,sBAAuB,OACvB,sBAAuB,OACvB,2BAA4B,OAC5B,sBAAuB,OACvB,4BAA6B,OAC7B,sBAAuB,OACvB,YAAa,OACb,oCAAqC,OACrC,kCAAmC,OACnC,0BAA2B,OAC3B,uBAAwB,OACxB,6BAA8B,OAC9B,6BAA8B,OAC9B,sBAAuB,OACvB,2BAA4B,OAC5B,uBAAwB,OACxB,sCAAuC,OACvC,uBAAwB,OACxB,mCAAoC,OACpC,4BAA6B,OAC7B,uBAAwB,OACxB,yCAA0C,OAC1C,yCAA0C,OAC1C,cAAe,OACf,iBAAkB,OAClB,oBAAqB,OACrB,mBAAoB,OACpB,8BAA+B,OAC/B,iBAAkB,OAClB,yBAA0B,OAC1B,+BAAgC,OAChC,+BAAgC,OAChC,wCAAyC,OACzC,cAAe,OACf,iBAAkB,OAClB,cAAe,OACf,iBAAkB,OAClB,yBAA0B,OAC1B,0BAA2B,OAC3B,+BAAgC,OAChC,2BAA4B,OAC5B,sCAAuC,OACvC,oCAAqC,OACrC,oCAAqC,OACrC,iDAAkD,OAClD,iDAAkD,OAClD,YAAa,OACb,yBAA0B,OAC1B,uCAAwC,OACxC,8BAA+B,OAC/B,cAAe,OACf,iBAAkB,OAClB,yCAA0C,OAC1C,yCAA0C,OAC1C,yBAA0B,OAC1B,wBAAyB,OACzB,yBAA0B,OAC1B,yBAA0B,OAC1B,iBAAkB,OAClB,yBAA0B,OAC1B,gCAAiC,OACjC,6BAA8B,OAC9B,0BAA2B,OAC3B,sBAAuB,OACvB,oBAAqB,OACrB,wBAAyB,OACzB,wBAAyB,OACzB,kBAAmB,OACnB,kCAAmC,OACnC,iCAAkC,OAClC,6CAA8C,OAC9C,iBAAkB,OAClB,iBAAkB,OAClB,+BAAgC,OAChC,wBAAyB,OACzB,8BAA+B,OAC/B,qBAAsB,OACtB,gBAAiB,OACjB,eAAgB,OAChB,4BAA6B,OAC7B,iCAAkC,OAClC,4BAA6B,OAC7B,sBAAuB,OACvB,6BAA8B,OAC9B,qCAAsC,OACtC,2BAA4B,OAC5B,wBAAyB,OACzB,uBAAwB,OACxB,uBAAwB,OACxB,uBAAwB,OACxB,4BAA6B,OAC7B,2BAA4B,OAC5B,0BAA2B,OAC3B,mCAAoC,OACpC,gCAAiC,QAIlC,SAASC,EAAeC,GACvB,IAAIC,EAAKC,EAAsBF,GAC/B,OAAOpC,EAAoBqC,GAE5B,SAASC,EAAsBF,GAC9B,IAAIpC,EAAoBW,EAAEuB,EAAKE,GAAM,CACpC,IAAIG,EAAI,IAAIC,MAAM,uBAAyBJ,EAAM,KAEjD,MADAG,EAAEE,KAAO,mBACHF,EAEP,OAAOL,EAAIE,GAEZD,EAAeO,KAAO,WACrB,OAAO5D,OAAO4D,KAAKR,IAEpBC,EAAeQ,QAAUL,EACzBlC,EAAOD,QAAUgC,EACjBA,EAAeE,GAAK,Q,uBCpHpBjC,EAAOD,QAAU,IAA0B,gD,uBCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,oC,uBCA3CC,EAAOD,QAAU,IAA0B,0B,oCCA3C,yBAAqe,EAAG,G,uBCAxeC,EAAOD,QAAU,IAA0B,oC,qBCA3CC,EAAOD,QAAU,IAA0B,uC,qBCA3CC,EAAOD,QAAU,IAA0B,kC,uBCA3CC,EAAOD,QAAU,IAA0B,mC,oCCA3C,yBAAuhB,EAAG,G,oCCA1hB,yBAAgf,EAAG,G,oCCAnf,yBAAme,EAAG,G,kCCAte,yBAAqe,EAAG,G,uBCAxeC,EAAOD,QAAU,IAA0B,gC,uBCA3CC,EAAOD,QAAU,IAA0B,kC,uBCA3CC,EAAOD,QAAU,IAA0B,sC,qBCA3CC,EAAOD,QAAU,IAA0B,yC,yECA3CC,EAAOD,QAAU,IAA0B,qC,uBCA3CC,EAAOD,QAAU,IAA0B,sC,oCCA3C,yBAAmgB,EAAG,G,uBCAtgBC,EAAOD,QAAU,IAA0B,8C,8CCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,uC,qBCA3CC,EAAOD,QAAU,IAA0B,6B,8CCA3CC,EAAOD,QAAU,IAA0B,0B,uBCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,uC,gDCA3CC,EAAOD,QAAU,IAA0B,6D,uBCA3CC,EAAOD,QAAU,IAA0B,qC,oCCA3C,yBAAqe,EAAG,G,mFCApeyC,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iCAAiCC,MAAM,CAAC,GAAK,QAAQ,CAACH,EAAG,UAAgC,MAArBH,KAAKO,OAAOC,MAAqC,SAArBR,KAAKO,OAAOC,KAAiBL,EAAG,UAAUJ,EAAIU,KAAKN,EAAG,gBAAgB,IACxRO,EAAkB,G,qDCCf,MAAMC,EAAgB,CACzB,CACIpB,GAAI,kCACJqB,KAAM,4CACNC,IAAK,CAACC,YAAa,+BACnBC,MAAO,2GACPC,KAAM,qPACNC,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,sBACTC,IAAK,gDAGb,CACI5B,GAAI,2BACJqB,KAAM,qCACNC,IAAK,CAACC,YAAa,uBACnBC,MAAO,mIACPC,KAAM,6PACNC,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,mBACTC,IAAK,kCAGb,CACI5B,GAAI,sBACJqB,KAAM,gCACNC,IAAK,CAACC,YAAa,8BACnBC,MAAO,4DACPC,KAAM,kNACNC,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,aACTC,IAAK,+BAGb,CACI5B,GAAI,gBACJqB,KAAM,iCACNC,IAAK,CAACC,YAAa,6BACnBC,MAAO,oEACPC,KAAM,kJAEV,CACIzB,GAAI,YACJqB,KAAM,kCACNC,IAAK,CAACC,YAAa,oBAAqBM,UAAW,QACnDL,MAAO,iCACPC,KAAM,mJACNC,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,aACTC,IAAK,+BAGb,CACI5B,GAAI,gBACJqB,KAAM,sBACNC,IAAK,CAACC,YAAa,sBACnBC,MAAO,YACPC,KAAM,wIACNC,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,8BACTC,IAAK,sDAKJE,EAAa,CACtB,CACI9B,GAAI,mBACJqB,KAAM,kEACNC,IAAK,CAACC,YAAa,sCACnBC,MAAO,wBACPC,KAAM,wSACNM,eAAgB,qBAChBL,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,mBACTC,IAAK,gIAGb,CACI5B,GAAI,kBACJqB,KAAM,iEACNC,IAAK,CAACC,YAAa,4BACnBC,MAAO,uBACPC,KAAM,6PACNM,eAAgB,qBAChBL,eAAgB,CACZ1B,GAAI,gBACJ2B,QAAS,mBACTC,IAAK,gIAKV,SAASI,IACZtC,OAAOuC,SAAS,EAAG,GAGhB,SAASC,EAAyBC,GACrCC,IAAED,GAAYE,GAAG,oBAAoB,WACjCL,OAID,SAASM,EAAsBH,GAClCC,IAAED,GAAYE,GAAG,cAAc,SAASE,GACpC,MAAMC,EAASD,EAAME,cAAcC,QAAQ,GAAGC,MAC9CP,IAAE3B,MAAMmC,IAAI,aAAa,SAASL,GAC9B,MAAMM,EAAQN,EAAME,cAAcC,QAAQ,GAAGC,MACvCG,EAAkB,GAEpBC,KAAKC,MAAMR,EAASK,GAASC,EAC7BV,IAAE3B,MAAMwC,SAAS,QAEZF,KAAKC,MAAMR,EAASK,IAAUC,GACnCV,IAAE3B,MAAMwC,SAAS,WAGzBb,IAAE3B,MAAM4B,GAAG,YAAY,WACnBD,IAAE3B,MAAMyC,IAAI,mBC9HxB,IAAI,EAAS,WAAa,IAAI1C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,2BAA2B,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,KAAK,CAACG,MAAM,CAAC,GAAK,4BAA4BsB,GAAG,CAAC,MAAQ7B,EAAI2C,SAAS,CAAC3C,EAAI4C,GAAG,qBAAqB5C,EAAI6C,GAAG,MAC5U,EAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,mBAAmBC,MAAM,CAAC,GAAK,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,iEAAiE,CAACF,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gCAAgC,CAACH,EAAG,IAAI,CAACE,YAAY,2BAA2BF,EAAG,IAAI,CAACE,YAAY,6BAA6BF,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,sCAAsC,OAAS,WAAW,CAACH,EAAG,IAAI,CAACE,YAAY,gCAAgCF,EAAG,IAAI,CAACE,YAAY,kCAAkCF,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACH,EAAG,IAAI,CAACE,YAAY,uBAAuBF,EAAG,IAAI,CAACE,YAAY,+BCwB/tB,GACE1C,KAAM,SACNkF,QAAS,CACP,SACE,KAAN,qBC7BgV,I,wBCQ5UC,EAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIa,EAAAA,E,QCnBX,EAAS,WAAa,IAAI/C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,YAAY,CAACH,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,WAAW,CAACP,EAAI4C,GAAG,cAAc5C,EAAI4C,GAAG,OAAOxC,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,cAAc,CAACP,EAAI4C,GAAG,cAAc5C,EAAI4C,GAAG,OAAOxC,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,aAAa,CAACP,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,OAAOxC,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,YAAY,CAACP,EAAI4C,GAAG,aAAa,MACnmB,EAAkB,GCWtB,GACEhF,KAAM,UCbwU,ICQ5U,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCCf,GACEA,KAAM,MACNoF,WAAY,CACVC,OAAJ,EACIC,OAAJ,GAEE,UAEE,IAAJ,0CACM,MAAMlB,EAASD,EAAME,cAAcC,QAAQ,GAAGC,MAC9C,IAAN,mCACQ,MAAR,mCACA,IAEA,kBACU,IAAV,uBAEA,oBACU,IAAV,uBAEQ,OAEF,IAAN,gCACQ,IAAR,+BC3C8T,ICQ1T,G,UAAY,eACd,EACApC,EACAY,GACA,EACA,KACA,KACA,OAIa,I,oBCnBX,EAAS,WAAa,IAAIX,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,wDAAwD,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,UAAU,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,cAAcxC,EAAG,MAAM,CAACE,YAAY,aAAauB,GAAG,CAAC,MAAQ7B,EAAImD,YAAY,CAAC/C,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAM,EAAQ,iBAAgDH,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,WAAW,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,YAAYxC,EAAG,MAAM,CAACE,YAAY,aAAauB,GAAG,CAAC,MAAQ7B,EAAIoD,aAAa,CAAChD,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAM,EAAQ,iBAA2CH,EAAG,MAAM,CAACE,YAAY,WAAWC,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,cAAcxC,EAAG,MAAM,CAACE,YAAY,aAAauB,GAAG,CAAC,MAAQ7B,EAAIqD,eAAe,CAACjD,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,eAAkDH,EAAG,MAAM,CAACE,YAAY,sBAAsB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,MAAM,CAACE,YAAY,aAAauB,GAAG,CAAC,MAAQ7B,EAAIsD,cAAc,CAAClD,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAM,EAAQ,iBAA6CH,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,YAAY,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,MAAM,CAACE,YAAY,aAAauB,GAAG,CAAC,MAAQ7B,EAAIsD,cAAc,CAAClD,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAM,EAAQ,iBAA6CH,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,aAAa,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,cAAcxC,EAAG,MAAM,CAACE,YAAY,aAAauB,GAAG,CAAC,MAAQ7B,EAAIqD,eAAe,CAACjD,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAM,EAAQ,oBACp+D,EAAkB,GCsEtB,GACE3C,KAAM,OACN,OACE,MAAO,CACL2F,YAAarE,OAAOsE,aAGxBV,QAAS,CACP,YACE7C,KAAKwD,QAAQnH,KAAK,WAEpB,eACE2D,KAAKwD,QAAQnH,KAAK,cAEpB,cACE2D,KAAKwD,QAAQnH,KAAK,aAEpB,aACE2D,KAAKwD,QAAQnH,KAAK,cCzFsT,ICQ1U,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCnBX,EAAS,WAAa,IAAI0D,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,6DAA6DC,MAAM,CAAC,GAAK,mBAAmB,CAACP,EAAI6C,GAAG,GAAGzC,EAAG,MAAM,CAACE,YAAY,oCAAoCC,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,oBAAoB,CAACP,EAAI4C,GAAG,kSAAkSxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8CsB,GAAG,CAAC,MAAQ7B,EAAI0D,WAAW,CAAC1D,EAAI4C,GAAG,WAAW5C,EAAI4C,GAAG,6CAA6C5C,EAAI6C,GAAG,GAAGzC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4PACv0B,EAAkB,CAAC,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,4CAA4C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,GAAK,oBAAoB,IAAM,EAAQ,QAAoC,IAAM,sBAAsB,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6HAA6HxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gCAAgC,CAACP,EAAI4C,GAAG,YAAY5C,EAAI4C,GAAG,aAAaxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,sCAAsC,OAAS,WAAW,CAACP,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,kBAAkBxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACP,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,2FC+Br2B,GACEhF,KAAM,QACNkF,QAAS,CACP,WACEtB,OCpCyU,ICQ3U,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCnBX,EAAS,WAAa,IAAIxB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiDN,EAAI2D,GAAI3D,EAAY,UAAE,SAAS4D,GAAS,OAAOxD,EAAG,MAAM,CAACzB,IAAIiF,EAAQ/C,KAAKP,YAAY,iCAAiC,CAACF,EAAG,cAAc,CAACG,MAAM,CAAC,QAAUqD,MAAY,MAAK,IAC/U,EAAkB,GCDlB,EAAS,WAAa,IAAI5D,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,KAAK,CAACE,YAAY,eAAe,CAACN,EAAI4C,GAAG5C,EAAI6D,GAAG7D,EAAI4D,QAAQ5C,UAAUZ,EAAG,MAAM,CAACE,YAAY,eAAewD,MAAM9D,EAAI4D,QAAQ9C,IAAIO,UAAYrB,EAAI4D,QAAQ9C,IAAIO,UAAY,GAAGd,MAAM,CAAC,GAAKP,EAAI4D,QAAQpE,GAAG,IAAMQ,EAAI+D,UAAU/D,EAAI4D,QAAQ9C,IAAIC,aAAa,IAAM,mBAAmBX,EAAG,MAAM,CAACE,YAAY,aAAa,CAACF,EAAG,IAAI,CAACE,YAAY,aAAa,CAACN,EAAI4C,GAAG5C,EAAI6D,GAAG7D,EAAI4D,QAAQ3C,SAAUjB,EAAI4D,QAAsB,eAAExD,EAAG,MAAM,CAACG,MAAM,CAAC,GAAKP,EAAI4D,QAAQ1C,eAAe1B,KAAK,CAACY,EAAG,IAAI,CAACE,YAAY,sBAAsBC,MAAM,CAAC,KAAOP,EAAI4D,QAAQ1C,eAAeE,IAAI,OAAS,WAAW,CAACpB,EAAI4C,GAAG5C,EAAI6D,GAAG7D,EAAI4D,QAAQ1C,eAAeC,YAAYf,EAAG,QAAQA,EAAG,IAAI,CAACE,YAAY,mCAAmCuB,GAAG,CAAC,MAAQ,SAASmC,GAAQ,OAAOhE,EAAIiE,gBAAgBjE,EAAI4D,QAAQ/C,SAAS,CAACb,EAAI4C,GAAG5C,EAAI6D,GAAG7D,EAAIuB,qBAAqBnB,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,IAAI,CAACE,YAAY,6BAA6BuB,GAAG,CAAC,MAAQ,SAASmC,GAAQ,OAAOhE,EAAIiE,gBAAgBjE,EAAI4D,QAAQ/C,SAAS,CAACb,EAAI4C,GAAG,gCACjoC,EAAkB,GCwBtB,GACE,KAAF,cACE,MAAF,YACE,QAAF,CACI,gBAAJ,GACM,KAAN,gBACM,KAEF,UAAJ,GACM,OAAN,oBAGE,SAAF,CACI,iBACE,OAAN,iFCvCqV,ICQjV,G,UAAY,eACd,EACA,EACA,GACA,EACA,KACA,WACA,OAIa,I,QCPf,GACEhF,KAAM,WACNoF,WAAY,CACVkB,YAAJ,GAEE,OACE,MAAJ,CACM,SAAN,KCnBkV,KCO9U,GAAY,eACd,GACA,EACA,GACA,EACA,KACA,KACA,MAIa,M,QClBX,GAAS,WAAa,IAAIlE,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,mFAAmF,CAACF,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,UAAU,CAACN,EAAI4C,GAAG,iBAAiBxC,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,aAAyCH,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,UAAU,CAACN,EAAI4C,GAAG,oBAAoBxC,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,iBC2B1lB,IACE3C,KAAM,WC7ByU,MCQ7U,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,KACA,OAIa,M,QCnBX,GAAS,WAAa,IAAIoC,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,wDAAwD,CAACF,EAAG,MAAM,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,eAAe,CAACH,EAAG,IAAI,CAACE,YAAY,6BAA6BC,MAAM,CAAC,OAAS,SAAS,KAAO,oCAAoC,CAACP,EAAI4C,GAAG,uBAAuBxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,QAAQ,CAACE,YAAY,MAAMC,MAAM,CAAC,IAAM,2CCY9d,IACE3C,KAAM,UCdwU,MCQ5U,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAIoC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACN,EAAI6C,GAAG,GAAGzC,EAAG,MAAM,CAACE,YAAY,aAAa,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,sCAAsC,YAAY,WAAW,gBAAgB,QAAQ,aAAa,SAAS,CAACP,EAAI6C,GAAG,GAAGzC,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACN,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,GAAGzC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kCAAkC5C,EAAI4C,GAAG,iHAAiHxC,EAAG,MAAMJ,EAAI4C,GAAG,iDAAiDxC,EAAG,MAAMA,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kRAAkRxC,EAAG,MAAM,CAACE,YAAY,sDAAsDN,EAAI2D,GAAI3D,EAAW,SAAE,SAASzC,GAAQ,OAAO6C,EAAG,MAAM,CAACzB,IAAIpB,EAAOsD,KAAKP,YAAY,iCAAiC,CAACF,EAAG,cAAc,CAACG,MAAM,CAAC,QAAUhD,MAAW,MAAK,GAAG6C,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uJAAuJ5C,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,GAAG7C,EAAI6C,GAAG,aACvhD,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iHAAiH,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,MAAM,CAACH,EAAG,MAAM,CAACE,YAAY,WAAW,CAACF,EAAG,OAAO,CAACE,YAAY,eAAe,CAACN,EAAI4C,GAAG,gCAAgCxC,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uCAAuC,gBAAgB,UAAU,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,GAAK,eAAe,IAAM,EAAQ,QAAyD,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oEAAoExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kfAAkfxC,EAAG,MAAMA,EAAG,MAAMJ,EAAI4C,GAAG,gDAAgDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8C,OAAS,WAAW,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,MAAMA,EAAG,MAAMJ,EAAI4C,GAAG,6EAA6ExC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0FAA0F,OAAS,WAAW,CAACP,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAMJ,EAAI4C,GAAG,gRAAgRxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oDAAoD,OAAS,WAAW,CAACP,EAAI4C,GAAG,sBAAsB5C,EAAI4C,GAAG,qMAAqMxC,EAAG,KAAK,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,kDAAkD,OAAS,WAAW,CAACP,EAAI4C,GAAG,qBAAqB5C,EAAI4C,GAAG,sLAAsLxC,EAAG,MAAMJ,EAAI4C,GAAG,6FAA6F,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gBAAgBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uOAAuOxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,qCAAqC,OAAS,WAAW,CAACP,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,SAASxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,+BAA+B,OAAS,WAAW,CAACP,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,sfAAsfxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,w0CAAw0CxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,SAAS,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACP,EAAI4C,GAAG,qQAAqQ,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oDAAoD5C,EAAI4C,GAAG,0IAA0IxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oGAAoG5C,EAAI4C,GAAG,wCAAwCxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mEAAmE5C,EAAI4C,GAAG,mDAAmDxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yEAAyE5C,EAAI4C,GAAG,0FAA0F5C,EAAI4C,GAAG,wSAAwSxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wIAAwIxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kCAAkC5C,EAAI4C,GAAG,sIAAsIxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+BAA+B5C,EAAI4C,GAAG,yKAAyKxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wDAAwD5C,EAAI4C,GAAG,qQAAqQxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sEAAsE5C,EAAI4C,GAAG,2HAA2HxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2EAA2E5C,EAAI4C,GAAG,iUAAiUxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iEAAiE5C,EAAI4C,GAAG,iZAAiZ5C,EAAI4C,GAAG,6BAA6BxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sCAAsC5C,EAAI4C,GAAG,iIAAiIxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sCAAsC5C,EAAI4C,GAAG,6DAA6D,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wCAAwCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qFAAqFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4bAA4bxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+CAA+CxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+PAA+PxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mFAAmFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,olBAAolB,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kCAAkCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6vBAA6vBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sjBAAsjBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,0FAA0F,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qFAAqFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,42BAA42BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sCAAsCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yHAAyHxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oDAAoD,OAAS,WAAW,CAACP,EAAI4C,GAAG,2BAA2B5C,EAAI4C,GAAG,2BAA2BxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,kDAAkD,OAAS,WAAW,CAACP,EAAI4C,GAAG,0BAA0B5C,EAAI4C,GAAG,ocAAocxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kDAAkDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0FAA0F,OAAS,WAAW,CAACP,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,mMAAmMxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oFAAoFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4tBAA4tBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wEAAwExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ygBAAygBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yiBAAyiB,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wEAAwExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8uBAA8uBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sFAAsFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qoBAAqoBxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+DAA+DxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iHAAiH5C,EAAI4C,GAAG,6LAA6LxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mEAAmExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kxBAAkxBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8EAA8ExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,muBAAmuBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qpBAAqpB,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8EAA8ExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yzBAAyzBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kpCAAkpC,WAAa,IAAI5C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,GAAK,eAAe,IAAM,EAAQ,QAAmD,IAAM,iBAAiBH,EAAG,IAAI,CAACJ,EAAI4C,GAAG,gNAAgNxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qDAAqDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kYAAkYxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wCAAwCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sPAAsPxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4GAA4GxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+UAA+UxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2GAA2GxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8ZAA8ZxC,EAAG,MAAM,CAACE,YAAY,iBCoZru3B,IACE1C,KAAM,8BACNoF,WAAY,CACVkB,YAAJ,GAEE,OACE,MAAO,CACL,QAAN,IAGE,UACExC,EAAyB,wCACzBI,EAAsB,0CCjayW,MCQ/X,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,SAAS,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,wBAAwB,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,yBAAyB,gBAAgB,UAAUH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAM,EAAQ,QAAuD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yCAAyCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+vBAA+vBxC,EAAG,MAAMA,EAAG,MAAMJ,EAAI4C,GAAG,osCAAosCxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+EAA+ExC,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,oVAAoVxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oDAAoDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,0GAA0GxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,4DAA4D,OAAS,WAAW,CAACP,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,+JAA+JxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wDAAwDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yZAA6ZxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wCAAwC5C,EAAI4C,GAAG,mHAAmHxC,EAAG,QAAQ,CAACG,MAAM,CAAC,GAAK,aAAa,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAexC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iBAAiBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAexC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iCAAiCxC,EAAG,MAAMJ,EAAI4C,GAAG,MAAMxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACP,EAAI4C,GAAG,sBAAsB5C,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,WAAWxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kCAAkCxC,EAAG,MAAMJ,EAAI4C,GAAG,MAAMxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACP,EAAI4C,GAAG,sBAAsB5C,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,YAAYxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAMJ,EAAI4C,GAAG,MAAMxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACP,EAAI4C,GAAG,sBAAsB5C,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,WAAWxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAMJ,EAAI4C,GAAG,MAAMxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACP,EAAI4C,GAAG,sBAAsB5C,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,YAAYxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,WAAWxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,UAAUxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,WAAWxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,MAAM,CAACE,YAAY,kDAAkD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA+CH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA+CH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA+CH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAAiDH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,gfAAsfxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8EAA8ExC,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,oJAAoJxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,MAAMA,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,gBAAgB,IAAM,EAAQ,eAAsDH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kCAAkCxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,6SAA6SxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8FAA8F5C,EAAI4C,GAAG,yTAAyTxC,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,0EAA0ExC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,uCAAuC,OAAS,WAAW,CAACP,EAAI4C,GAAG,uGAAuG5C,EAAI4C,GAAG,kHAAkHxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,eAAexC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0CAA0CxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,0KAA0KxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gBAAgB5C,EAAI4C,GAAG,wjBAAwjBxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4GAA4GxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,uKAAuK,OAAS,WAAW,CAACP,EAAI4C,GAAG,gBAAgB5C,EAAI4C,GAAG,iCAAiCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4CAA4CxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gCAAgC5C,EAAI4C,GAAG,uBAAuBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uHAAuHxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2dAA2dxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wEAAwExC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iJAAkJxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,aAAaxC,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qBAAqB5C,EAAI4C,GAAG,4EAA4ExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2CAA2C5C,EAAI4C,GAAG,oEAAoExC,EAAG,MAAMA,EAAG,MAAMJ,EAAI4C,GAAG,+DAA+DxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+CAA+CxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kDAAkDxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0CAA0CxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+DAA+DxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6CAA6C5C,EAAI4C,GAAG,4FAA4FxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sJAAsJxC,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,kFAAkFxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACP,EAAI4C,GAAG,2GAA2G5C,EAAI4C,GAAG,qCAAqCxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0EAA0ExC,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACN,EAAI4C,GAAG,+DAA+DxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAA4CH,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,gIAAgIxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6B5C,EAAI4C,GAAG,oLAAsLxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oNAAoNxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+SAA+SxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8KAA8KxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2CAA2CxC,EAAG,MAAM,CAACE,YAAY,kDAAkD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA4CH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA4CH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA4CH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAA8CH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ilBAAilBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAAuEH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAAyEH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uZAA2axC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA+EH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAAiFH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,g+BAAk+BxC,EAAG,UAAUA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAAuEH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAAyEH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,0RAAgSxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,uyBAAqzBxC,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,0EAA0ExC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,uCAAuC,OAAS,WAAW,CAACP,EAAI4C,GAAG,uGAAuG5C,EAAI4C,GAAG,kHAAkHxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,eAAexC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oBAAoBxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,aAA6DH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAAwEH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qTAAqTxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gEAAgExC,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACN,EAAI4C,GAAG,oFAAoFxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,cAAc,IAAM,EAAQ,eAAuDH,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2WAA2WxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,olBAAolBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uDAAuDxC,EAAG,MAAMA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qCAAqCxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2BAA2B5C,EAAI4C,GAAG,2DAA2DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,mNAAuNxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0NAA0NxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0BAA0BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,sCAAsCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gBAAgB5C,EAAI4C,GAAG,oDAAoDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oBAAoBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8CAA8CxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,sCAAsCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oCAAoC5C,EAAI4C,GAAG,sFAAsFxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8BAA8B5C,EAAI4C,GAAG,qFAAqFxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6B5C,EAAI4C,GAAG,gCAAgCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iBAAiB5C,EAAI4C,GAAG,0BAA0BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qBAAqB5C,EAAI4C,GAAG,2BAA2BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6B5C,EAAI4C,GAAG,iBAAiBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iCAAiC5C,EAAI4C,GAAG,oDAAoDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4PAA4PxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uGAAuGxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uLAAuLxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iBAAiB5C,EAAI4C,GAAG,2DAA2DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uEAAuExC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4EAA4ExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2PAA2PxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8GAA8GxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wBAAwB5C,EAAI4C,GAAG,mCAAmCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mCAAmC5C,EAAI4C,GAAG,WAAWxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wEAAwExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qsBAAqsBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mEAAmExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,wGAAwGxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6JAA6JxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gDAAgDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+GAA+G5C,EAAI4C,GAAG,kJAAkJxC,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,sFAAsFxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,2DAA2DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qCAAqC5C,EAAI4C,GAAG,kBAAkBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yCAAyC5C,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAMA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8EAA8E5C,EAAI4C,GAAG,OAAOxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0BAA0B5C,EAAI4C,GAAG,4BAA4BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6CAA6C5C,EAAI4C,GAAG,6BAA6BxC,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,qmBAAqmBxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,SAASxC,EAAG,MAAMA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,QAAQxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0BAA0B5C,EAAI4C,GAAG,qKAAqKxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,eAAexC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wCAAwCxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,eAAexC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+BAA+B5C,EAAI4C,GAAG,+GAA+GxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uDAAuD5C,EAAI4C,GAAG,wDAAwDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uBAAuB5C,EAAI4C,GAAG,sBAAsBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0BAA0BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yCAAyC5C,EAAI4C,GAAG,+EAA+ExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4aAA4axC,EAAG,MAAMA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAM,EAAQ,eAAmEH,EAAG,UAAUA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2yBAA2yBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uDAAuDxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qCAAqCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oBAAoBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iCAAiC5C,EAAI4C,GAAG,gBAAgBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6DAA6DxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8EAA8ExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oBAAoB5C,EAAI4C,GAAG,yFAAyFxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4CAA4CxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8BAA8B5C,EAAI4C,GAAG,gCAAgCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kCAAkC5C,EAAI4C,GAAG,iIAAiIxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2EAA2ExC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iCAAiC5C,EAAI4C,GAAG,mBAAmBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sCAAsC5C,EAAI4C,GAAG,iBAAiBxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8CAA8CxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gDAAgDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0GAA0GxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,iEAAiExC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yEAAyExC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iFAAiFxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+EAA+ExC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4EAA4ExC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sEAAsExC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iDAAiD5C,EAAI4C,GAAG,+DAA+DxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+HAA+HxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oEAAoExC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6JAA6JxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8FAA8F5C,EAAI4C,GAAG,QAAQxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qCAAqC5C,EAAI4C,GAAG,+FAA+FxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2HAA6HxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6RAA6RxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,iHAAiHxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+FAA+F5C,EAAI4C,GAAG,kFAAkFxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iFAAiF5C,EAAI4C,GAAG,6HAA6HxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4NAA4NxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uVAAuV5C,EAAI4C,GAAG,uMAAuMxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oEAAoE,CAACP,EAAI4C,GAAG,0BAA0B5C,EAAI4C,GAAG,sBC+sB5syC,IACEhF,KAAM,wBACN,UACE8D,EAAyB,0BACzBI,EAAsB,4BCptBmW,MCQzX,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,SAAS,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4BAA4BxC,EAAG,MAAM,CAACE,YAAY,aAAa,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,uBAAuB,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,UAAUH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAAsD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wCAAwCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ynBAAynBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gGAAgGxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8dAA8dxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gDAAgDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sWAAsWxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oDAAoDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qjBAAqjBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+DAA+DxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,iVAAiVxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gBAAgBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+yBAA+yBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mEAAmExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8eAA8exC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oCAAoCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6dAA6dxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2PAA2P5C,EAAI4C,GAAG,gEAAgExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kIAAkIxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sCAAsCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,48CAAk9CxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mDAAmDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ucAAucxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ktBAAouBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oDAAoDxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAAqD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sEAAsExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wCAAwC5C,EAAI4C,GAAG,kCAAkCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gCAAgC5C,EAAI4C,GAAG,+DAA+DxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2BAA2BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qCAAqCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qCAAqC5C,EAAI4C,GAAG,yGAAyGxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4DAA4DxC,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,QAAQ,CAACG,MAAM,CAAC,GAAK,+BAA+B,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sCAAsCxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oFAAoFxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4CAA4CxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,eAAexC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8LAA8LxC,EAAG,MAAMA,EAAG,MAAMJ,EAAI4C,GAAG,oZAAoZxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,qBAAqB,IAAM,EAAQ,QAA+D,IAAM,mBAAmBH,EAAG,MAAM,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4CAA4CxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oCAAoCxC,EAAG,MAAM,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gDAAgDxC,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,oBAAoB,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oBAAoBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kCAAkCxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+DAA+DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oBAAoB5C,EAAI4C,GAAG,cAAcxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,yHAAyHxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,oBAAoBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,4EAA4ExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,oBAAoBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,sCAAsCxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8NAA+NxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,aAAaxC,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uKAAuKxC,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,gFAAgFxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oGAAoG,OAAS,WAAW,CAACP,EAAI4C,GAAG,uGAAuG5C,EAAI4C,GAAG,iBAAiBxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iDAAiDxC,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,gBAAgB,IAAM,EAAQ,QAAoD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4SAA4SxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2DAA2D5C,EAAI4C,GAAG,cAAcxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wBAAwB5C,EAAI4C,GAAG,gDAAgDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wDAAwD5C,EAAI4C,GAAG,mCAAmCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kDAAkD5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4XAA4XxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yQAAyQxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iDAAiDxC,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kDAAkDxC,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAAsD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yLAAyLxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,mDAAmDxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iBAAiB5C,EAAI4C,GAAG,qBAAqBxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iFAAiFxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uGAAuGxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,UAAUxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,WAAW5C,EAAI4C,GAAG,cAAcxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,qDAAqDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,QAAQxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,sEAAsExC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iBAAiB5C,EAAI4C,GAAG,iBAAiBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gBAAgB5C,EAAI4C,GAAG,kBAAkBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2qBAAqrBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,iCAAiCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,qIAAyIxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAM,EAAQ,QAAsD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8GAA8GxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,kDAAkDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iDAAiD5C,EAAI4C,GAAG,mTAAuTxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,mBAAmB,IAAM,EAAQ,QAAgD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iCAAiCxC,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0CAA0CxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wBAAwB5C,EAAI4C,GAAG,kBAAkBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uBAAuB5C,EAAI4C,GAAG,uEAAuExC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8BAA8B5C,EAAI4C,GAAG,gBAAgBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wFAAwFxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2tBAA2tBxC,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,oDAAoDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0CAA0C,OAAS,WAAW,CAACP,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,yDAAyDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gCAAgC5C,EAAI4C,GAAG,+EAA+ExC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8C,OAAS,WAAW,CAACP,EAAI4C,GAAG,iBAAiB5C,EAAI4C,GAAG,kHAAkHxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0HAA0H,OAAS,WAAW,CAACP,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,KAAKxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iDAAiDxC,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,mFAAmFxC,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,8BAA8B,IAAM,EAAQ,QAAoD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qBAAqBxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2DAA2DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,gBAAgBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kBAAkB5C,EAAI4C,GAAG,kBAAkBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,yCAAyCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2BAA2B5C,EAAI4C,GAAG,sCAAsCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,YAAYxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mCAAmC5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,gIAAgIxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,qCAAqCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0BAA0BxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,aAAaxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4FAA4FxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,6EAA6ExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6CAA6C5C,EAAI4C,GAAG,4DAA4DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wBAAwB5C,EAAI4C,GAAG,0CAA0CxC,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,yLAAyLxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,mBAAmB,IAAM,EAAQ,QAA+C,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2CAA2CxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qBAAqB5C,EAAI4C,GAAG,+EAA+ExC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oCAAoC5C,EAAI4C,GAAG,oGAAoGxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8BAA8BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gBAAgB5C,EAAI4C,GAAG,yDAAyDxC,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iBAAiBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sBAAsBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yCAAyC5C,EAAI4C,GAAG,kEAAkExC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4SAA4SxC,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,OAAOxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,yDAAyD,OAAS,WAAW,CAACP,EAAI4C,GAAG,gDAAgDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oCAAoCxC,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,mFAAmFxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sCAAsC5C,EAAI4C,GAAG,mKAAmKxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sCAAsC5C,EAAI4C,GAAG,0JAA0JxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sBAAsBxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6CAA6CxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mHAAmH5C,EAAI4C,GAAG,iPAAiPxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yZAAyZxC,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gBAAgB5C,EAAI4C,GAAG,wBAAwBxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0DAA0DxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oDAAoDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8FAA8FxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4GAA4GxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kEAAkExC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sFAAsFxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oDAAoDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yFAAyFxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wCAAwCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4BAA4BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0EAA0ExC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mFAAmFxC,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,mBAAmB5C,EAAI4C,GAAG,wBAAwBxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gCAAgCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uCAAuCxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6CAA6CxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,4BAA4B,IAAM,EAAQ,QAAoD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8KAA8KxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+FAA+F5C,EAAI4C,GAAG,0CAA0CxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6DAA6D5C,EAAI4C,GAAG,iDAAiDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uCAAuC5C,EAAI4C,GAAG,oDAAoDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kBAAkB5C,EAAI4C,GAAG,iCAAiCxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6B5C,EAAI4C,GAAG,kHAAkHxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,aAAaxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6CAA6C5C,EAAI4C,GAAG,UAAUxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gDAAgDxC,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,mJAAmJxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oFAAoFxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6PAA6PxC,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,gCAAgC,IAAM,EAAQ,QAA+D,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iCAAiCxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0KAA8KxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8CAAgD5C,EAAI4C,GAAG,sBAAsBxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,cAAc5C,EAAI4C,GAAG,8DAAgExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,WAAW5C,EAAI4C,GAAG,qEAAuExC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4MAA4MxC,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,0BAA0B,IAAM,EAAQ,QAAmD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8CAA8CxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uHAAuHxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kHAAkH5C,EAAI4C,GAAG,mBAAmBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,YAAY5C,EAAI4C,GAAG,sBAAsBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yFAAyF5C,EAAI4C,GAAG,OAAOxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oDAAsD5C,EAAI4C,GAAG,QAAQxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,cAAc5C,EAAI4C,GAAG,yDAAyDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kBAAoB5C,EAAI4C,GAAG,WAAWxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wCAAwC5C,EAAI4C,GAAG,OAAOxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gCAAgC5C,EAAI4C,GAAG,0FAA4FxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2EAA2ExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,cAAc5C,EAAI4C,GAAG,4EAA4ExC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,cAAc5C,EAAI4C,GAAG,yDAAyDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wEAAwExC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,8BAA8BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gDAAgD5C,EAAI4C,GAAG,wGAAwGxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sBAAsBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mEAAmExC,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uBAAuBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,gNAAgNxC,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAM,EAAQ,QAAwD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qDAAqDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6kBAA6kBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4OAA4OxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,udAAudxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yGAAyGxC,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mBAAmBxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4BAA4BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6BAA6BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uNAAuNxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yPAAyPxC,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,YAAY,IAAM,EAAQ,QAA2C,IAAM,wBAAwBH,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,mCAAmCxC,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAAiD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAAiD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6SAA6SxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,iEAAiExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kCAAkC5C,EAAI4C,GAAG,oUAAoUxC,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,OAAOxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,wJAAwJ,OAAS,WAAW,CAACP,EAAI4C,GAAG,oDAAoDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8BAA8BxC,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,0BAA0B,IAAM,EAAQ,QAA6D,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0BAA0BxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uVAAuVxC,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,sBAAsB,IAAM,EAAQ,QAAqD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uIAAuIxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4DAA4D5C,EAAI4C,GAAG,qEAAqExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,qBAAqB5C,EAAI4C,GAAG,gBAAgBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,eAAexC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iCAAiC5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+IAA+IxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6B5C,EAAI4C,GAAG,uBAAuBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+DAA+D5C,EAAI4C,GAAG,oZAAoZxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wCAAwCxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yDAAyDxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+CAA+C5C,EAAI4C,GAAG,kGAAoGxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kFAAkF5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4oBAA4oBxC,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,cAAc,IAAM,EAAQ,QAAkD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yIAAyIxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oHAAoH5C,EAAI4C,GAAG,gDAAgDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,WAAW5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sCAAsC5C,EAAI4C,GAAG,oFAAoFxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4CAA4CxC,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,8BAA8B,IAAM,EAAQ,QAAmD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAA6D,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,oSAA0SxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,wJAA8JxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uBAAuB5C,EAAI4C,GAAG,gFAAgFxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iBAAiB5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wSAA4SxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iDAAiDxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gHAAgHxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+EAA+ExC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0GAA0GxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,oKAAoKxC,EAAG,SAAS,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wEAAwExC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iGAAiGxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0HAA0HxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iBAAiBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sDAAsD5C,EAAI4C,GAAG,mBAAmBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,+FAA+F5C,EAAI4C,GAAG,kIAAkIxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2HAA2HxC,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4NAA4NxC,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wKAAwKxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,mTAAmTxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,SAASxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,gEAAgE5C,EAAI4C,GAAG,wBAAwBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oGAAoG5C,EAAI4C,GAAG,0PAA0PxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,0DAA0DxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gBAAgBxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,4YAA4YxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ySAAySxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,yDAAyDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2CAA2C5C,EAAI4C,GAAG,0CAA0CxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yCAAyC5C,EAAI4C,GAAG,SAASxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yCAAyC5C,EAAI4C,GAAG,qOAAqOxC,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,sBAAsB,IAAM,EAAQ,QAA8D,IAAM,sBAAsBH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2BAA2BxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,IAAI,CAACJ,EAAI4C,GAAG,gFAAgFxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6CAA6C5C,EAAI4C,GAAG,gBAAgBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0CAA0C5C,EAAI4C,GAAG,sOAAwOxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oBAAoB5C,EAAI4C,GAAG,wDAAwDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,mEAAmExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0EAA0E5C,EAAI4C,GAAG,yEAAyExC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2CAA2C5C,EAAI4C,GAAG,4BAA4BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6B5C,EAAI4C,GAAG,8GAA8GxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4EAA4E5C,EAAI4C,GAAG,+BAA+BxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yCAAyCxC,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,sBAAsB,IAAM,EAAQ,QAAqD,IAAM,+BAA+BH,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+CAA+CxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+GAA+GxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kBAAkB5C,EAAI4C,GAAG,YAAYxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,wCAAwC5C,EAAI4C,GAAG,MAAMxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yDAAyD5C,EAAI4C,GAAG,YAAYxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,2CAA2C5C,EAAI4C,GAAG,8DAA8DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,aAAa5C,EAAI4C,GAAG,QAAQxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,eAAe5C,EAAI4C,GAAG,yJAAyJxC,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,6BAA6B,IAAM,EAAQ,QAA4D,IAAM,sCAAsCH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oCAAoCxC,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iEAAiE5C,EAAI4C,GAAG,+FAA+FxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0EAA0E5C,EAAI4C,GAAG,6UAA6UxC,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2DAA2DxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8IAA8IxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0BAA0B5C,EAAI4C,GAAG,SAASxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,6BAA6BxC,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,6PAA6PxC,EAAG,MAAM,CAACE,YAAY,QAAQ,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,aAAa,IAAM,EAAQ,QAA4C,IAAM,oBAAoBH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACP,EAAI4C,GAAG,gDAAgDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kCAAkC5C,EAAI4C,GAAG,6BAA6BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,yCAAyC5C,EAAI4C,GAAG,4DAA4DxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,kBAAkB5C,EAAI4C,GAAG,kBAAkBxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iDAAiDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kFAAkFxC,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,yBAAyB,CAACH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qCAAqCxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,kCAAkC,IAAM,EAAQ,QAAwD,IAAM,kBAAkBH,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,eAAe,IAAM,EAAQ,QAA8C,IAAM,oDAAoDH,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2DAA2DxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAM,EAAQ,QAAuD,IAAM,sEAAsEH,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,aAAa,CAACH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qCAAqCxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,kCAAkC,IAAM,EAAQ,QAAwD,IAAM,oBAAoBH,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,iEAAiE5C,EAAI4C,GAAG,oGAAoGxC,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,mBAAmB,IAAM,EAAQ,QAAkD,IAAM,kBAAkBH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACH,EAAG,IAAI,CAACJ,EAAI4C,GAAG,0CAA0CxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8EAA8E5C,EAAI4C,GAAG,2BAA2BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,cAAcxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4EAA4ExC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sGAAsGxC,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,SAAS,CAACJ,EAAI4C,GAAG,8IAA8IxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACH,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wFAAwFxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,uDAAuD5C,EAAI4C,GAAG,wJAAwJxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,iUAAiUxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oEAAoE5C,EAAI4C,GAAG,2BAA2BxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,YAAY5C,EAAI4C,GAAG,gBAAgBxC,EAAG,MAAM,CAACE,YAAY,mBAAmBC,MAAM,CAAC,GAAK,YAAY,CAACH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,kBAAkBxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,eAAe,IAAM,EAAQ,QAA8C,IAAM,oDAAoDH,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2DAA2DxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAM,EAAQ,QAAuD,IAAM,wEAAwEH,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,OAAOxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,sCAAsC,OAAS,WAAW,CAACP,EAAI4C,GAAG,oBAAoB5C,EAAI4C,GAAG,kHAAkHxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iDAAiDxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4IAA4IxC,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,0BAA0B,CAACH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wCAAwC,IAAM,EAAQ,QAAkD,IAAM,2CAA2CH,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAM,EAAQ,QAAoD,IAAM,0BAA0BH,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,8BAA8B,CAACH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,+BAA+BxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wCAAwC,IAAM,EAAQ,QAAkD,IAAM,6CAA6CH,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,GAAK,kCAAkC,CAACH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wHAAwHxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,oDAAoDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0EAA0E5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wGAAwGxC,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,yBAAyB,IAAM,EAAQ,QAAoD,IAAM,0BAA0BH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACH,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+FAA+FxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,4EAA4ExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6FAA6FxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAM,EAAQ,QAAgD,IAAM,wBAAwBH,EAAG,MAAM,CAACE,YAAY,kBAAkBC,MAAM,CAAC,GAAK,qCAAqC,CAACH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,wHAAwHxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,4BAA4B5C,EAAI4C,GAAG,oDAAoDxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,0EAA0E5C,EAAI4C,GAAG,QAAQxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,oGAAoGxC,EAAG,KAAK,CAACA,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+FAA+FxC,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACP,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,4EAA4ExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6FAA6FxC,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAM,EAAQ,QAAgD,IAAM,wBAAwBH,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,GAAK,qBAAqB,CAACH,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,kBAAkB,CAACP,EAAI4C,GAAG,OAAOxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sBAAsBxC,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAM,EAAQ,QAAkD,IAAM,0BAA0BH,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACP,EAAI4C,GAAG,mHAAmHxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,QAAQA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yGAAyGxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4GAA4GxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sFAAsF5C,EAAI4C,GAAG,2CAA2CxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wJAAwJxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yBAAyBxC,EAAG,SAAS,CAACJ,EAAI4C,GAAG,oFAAoF5C,EAAI4C,GAAG,4JCgpCr7kE,IACEhF,KAAM,uBACN,UACE8D,EAAyB,yBACzBI,EAAsB,2BCrpCkW,MCQxX,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wIAAwIxC,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,uBAAuB,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,wBAAwB,gBAAgB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,kDAAkD,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA8C,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4CAA4CxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6MAA6MxC,EAAG,MAAMJ,EAAI4C,GAAG,mFAAmFxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gCAAgC,OAAS,WAAW,CAACP,EAAI4C,GAAG,uBAAuBxC,EAAG,MAAMJ,EAAI4C,GAAG,+EAA+ExC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,MAAMJ,EAAI4C,GAAG,sDAAsDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,qCAAqC,OAAS,WAAW,CAACP,EAAI4C,GAAG,+BAA+BxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qJAAqJxC,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,cAAcxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,k/CAAk/CxC,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uFAAuFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,icAAicxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,oDAAoDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8GAA8GxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,wCAAwC,OAAS,WAAW,CAACP,EAAI4C,GAAG,qCAAqC5C,EAAI4C,GAAG,omBAAomBxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,2HAA2H,OAAS,WAAW,CAACP,EAAI4C,GAAG,oCAAoC5C,EAAI4C,GAAG,sbAAsbxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yFAAyFxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,uxBAAuxBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,iGAAiGxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2XAA2XxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,yEAAyExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,SAAS5C,EAAI4C,GAAG,wyBAAwyBxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0CAA0C,OAAS,WAAW,CAACP,EAAI4C,GAAG,yBAAyB5C,EAAI4C,GAAG,gXAAgXxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,YAAYxC,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA0C,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,uDAAuDxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,+VAA+VxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6lDAA6lDxC,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA4C,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2HAA2HxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6DAA6DxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2sBAA2sBxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,qFAAqF,OAAS,WAAW,CAACP,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,8BAA8BxC,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gCAAgCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,krCAAkrCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,UAAU5C,EAAI4C,GAAG,YAAYxC,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA8C,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gBAAgBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8kCCkL/uc,IACEhF,KAAM,mBACN,UACE8D,EAAyB,yBACzBI,EAAsB,2BCvL+U,MCQrW,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,yEAAyExC,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,eAAe,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,gBAAgB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,gBAAgB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,gBAAgB,gBAAgB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAuD,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,gBAAgBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,iuBAAiuBxC,EAAG,MAAM,CAACE,YAAY,oCAAoC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAoD,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,8DAA8DxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ktBAAktBxC,EAAG,MAAM,CAACE,YAAY,oCAAoC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAoD,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,6sBC4D7jG,IACEhF,KAAM,OACN,UACE8D,EAAyB,iBACzBI,EAAsB,mBCjEiV,MCQvW,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sCAAsCxC,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,gBAAgB,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,iBAAiB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,iBAAiB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,iBAAiB,gBAAgB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,4CAA4C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA6C,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iBAAiBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,gZAAgZxC,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAgD,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4EAA4ExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,oQAA6QxC,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,IAAM,EAAQ,WAA2DH,EAAG,MAAM,CAACE,YAAY,gCAAgCC,MAAM,CAAC,IAAM,EAAQ,WAA6DH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6FAA6FxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,mqBCyD19E,IACEhF,KAAM,QACN,UACE8D,EAAyB,kBACzBI,EAAsB,oBC9DkV,MCQxW,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iEAAiExC,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,sBAAsB,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,uBAAuB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uBAAuB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uBAAuB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uBAAuB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,uBAAuB,gBAAgB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAqD,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,sCAAsCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qNAAqNxC,EAAG,MAAMJ,EAAI4C,GAAG,uFAAuFxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACP,EAAI4C,GAAG,iBAAiBxC,EAAG,MAAMJ,EAAI4C,GAAG,yEAAyExC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACP,EAAI4C,GAAG,WAAWxC,EAAG,MAAMJ,EAAI4C,GAAG,sDAAsDxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,uCAAuC,OAAS,WAAW,CAACP,EAAI4C,GAAG,mCAAmCxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAI4C,GAAG,sCAAsCxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,qeAAqexC,EAAG,MAAMA,EAAG,SAAS,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACP,EAAI4C,GAAG,oPAAoPxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAuD,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,6CAA6CxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,2MAA2MxC,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oHAAoH,OAAS,WAAW,CAACP,EAAI4C,GAAG,oBAAoB5C,EAAI4C,GAAG,swCAAswCxC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAmE,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,qEAAqExC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,ugEAAugExC,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAAqD,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,aAAaxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,63BC4HjhP,IACEhF,KAAM,oBACN,UACE8D,EAAyB,wBACzBI,EAAsB,0BCjIgV,MCQtW,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAsBH,EAAIK,MAAMD,GAAO,OAAOJ,EAAI6C,GAAG,IACnG,GAAkB,CAAC,WAAa,IAAI7C,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,SAASC,MAAM,CAAC,GAAK,UAAU,CAACH,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iBAAiBxC,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,oBAAoB,YAAY,WAAW,gBAAgB,UAAU,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,cAAc,qBAAqB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,qBAAqB,gBAAgB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,cAAc,qBAAqB,gBAAgB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,gDAAgD,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA6C,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,wBAAwBxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,sTAAsTxC,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA6C,IAAM,kBAAkBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,2BAA2BxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,saAAsaxC,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAM,EAAQ,QAA+C,IAAM,iBAAiBH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,4BAA4BxC,EAAG,IAAI,CAACJ,EAAI4C,GAAG,8XCwDn0E,IACEhF,KAAM,YACN,UACE8D,EAAyB,sBACzBI,EAAsB,wBC7DwU,MCQ9V,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QCnBX,GAAS,WAAa,IAAI9B,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,KAAK,CAACJ,EAAI4C,GAAG,iFAAiFxC,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,MAAM,CAACP,EAAI4C,GAAG,kCAAkC,IAC/V,GAAkB,GCOtB,IACEhF,KAAM,qBCTmV,MCQvV,I,UAAY,eACd,GACA,GACA,IACA,EACA,KACA,WACA,OAIa,M,QChBfuG,aAAIC,IAAIC,QAmBR,MAAMC,GAAS,CACX,CACI7D,KAAM,IACNsC,UAAWwB,GAEf,CACI9D,KAAM,SACNsC,UAAWyB,GAEf,CACI/D,KAAM,YACNsC,UAAW0B,IAEf,CACIhE,KAAM,sBACNsC,UAAW2B,IAEf,CACIjE,KAAM,4CACNsC,UAAW4B,IAEf,CACIlE,KAAM,kEACNsC,UAAW6B,IAEf,CACInE,KAAM,iEACNsC,UAAW8B,IAEf,CACIpE,KAAM,qCACNsC,UAAW+B,IAEf,CACIrE,KAAM,kCACNsC,UAAWgC,IAEf,CACItE,KAAM,iCACNsC,UAAWiC,IAEf,CACIvE,KAAM,gCACNsC,UAAWkC,IAEf,CACIxE,KAAM,WACNsC,UAAWmC,IAEf,CACIzE,KAAM,UACNsC,UAAWoC,IAEf,CACI1E,KAAM,OACNsC,UAAWqC,IAEf,CACI3E,KAAM,IACN4E,SAAU,SAKZC,GAAY,IAAIjB,OAAU,CAC5B9F,KAAM,OACNgH,OAAQjB,KAGGgB,U,oBCnFfnB,aAAIqB,OAAOC,eAAgB,EAC3BtB,aAAIC,IAAIsB,QACRvB,aAAIC,IAAIuB,QAER,IAAIxB,aAAI,CACNG,UACAvE,OAAQ6F,GAAKA,EAAEC,KACdC,OAAO,S,qBCfVvI,EAAOD,QAAU,IAA0B,wB,gDCA3CC,EAAOD,QAAU,IAA0B,gD,uBCA3CC,EAAOD,QAAU,IAA0B,mC,uBCA3CC,EAAOD,QAAU,IAA0B,kC,uBCA3CC,EAAOD,QAAU,IAA0B,kC,uBCA3CC,EAAOD,QAAU,IAA0B,6C,uBCA3CC,EAAOD,QAAU,IAA0B,kD,qBCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,+B,uBCA3CC,EAAOD,QAAU,IAA0B,0B,uBCA3CC,EAAOD,QAAU,IAA0B,2C,uBCA3CC,EAAOD,QAAU,IAA0B,qD,kCCA3C,yBAAoe,EAAG,G,uBCAveC,EAAOD,QAAU,IAA0B,mC,oCCA3C,yBAAigB,EAAG,G,uBCApgBC,EAAOD,QAAU,IAA0B,uC,uBCA3CC,EAAOD,QAAU,IAA0B,oC,uBCA3CC,EAAOD,QAAU,IAA0B,mD,uBCA3CC,EAAOD,QAAU,IAA0B,wB,8CCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,+C,uBCA3CC,EAAOD,QAAU,IAA0B,sC,uBCA3CC,EAAOD,QAAU,IAA0B,0C,uBCA3CC,EAAOD,QAAU,IAA0B,0C,qBCA3CC,EAAOD,QAAU,IAA0B,2C,gDCA3CC,EAAOD,QAAU,IAA0B,4C,uBCA3CC,EAAOD,QAAU,IAA0B,oC,uBCA3CC,EAAOD,QAAU,IAA0B,mC,uBCA3CC,EAAOD,QAAU,IAA0B,oC,qBCA3CC,EAAOD,QAAU,IAA0B,mC,qBCA3CC,EAAOD,QAAU,IAA0B,0B,qBCA3CC,EAAOD,QAAU,IAA0B,qD,uBCA3CC,EAAOD,QAAU,IAA0B,6B,uBCA3CC,EAAOD,QAAU,IAA0B,wC,uBCA3CC,EAAOD,QAAU,IAA0B,6B,qBCA3CC,EAAOD,QAAU,IAA0B,qC,qBCA3CC,EAAOD,QAAU,IAA0B,2B,qBCA3CC,EAAOD,QAAU,IAA0B,4B,qBCA3CC,EAAOD,QAAU,IAA0B,4C,qBCA3CC,EAAOD,QAAU,IAA0B,qD,qBCA3CC,EAAOD,QAAU,IAA0B,8C,qBCA3CC,EAAOD,QAAU,IAA0B,wC,qBCA3CC,EAAOD,QAAU,IAA0B,6B,qBCA3CC,EAAOD,QAAU,IAA0B,6B,qBCA3CC,EAAOD,QAAU,IAA0B,yC,qBCA3CC,EAAOD,QAAU,IAA0B,6B,qBCA3CC,EAAOD,QAAU,IAA0B,gC,kCCA3C,yBAA8hB,EAAG,G,qBCAjiBC,EAAOD,QAAU,IAA0B,yD,qBCA3CC,EAAOD,QAAU,IAA0B,qC,qBCA3CC,EAAOD,QAAU,IAA0B,iC,kCCA3C,yBAAogB,EAAG,G,yDCAvgB,yBAA0e,EAAG,G,qBCA7eC,EAAOD,QAAU,IAA0B,6D,4CCA3CC,EAAOD,QAAU,IAA0B,0C,qBCA3CC,EAAOD,QAAU,IAA0B,2C,4CCA3CC,EAAOD,QAAU,IAA0B,mC,qBCA3CC,EAAOD,QAAU,IAA0B,yC,4CCA3CC,EAAOD,QAAU,IAA0B,wC,qBCA3CC,EAAOD,QAAU,IAA0B,6B,qBCA3CC,EAAOD,QAAU,IAA0B,qC,qBCA3CC,EAAOD,QAAU,IAA0B,mC,qBCA3CC,EAAOD,QAAU,IAA0B,gD,qBCA3CC,EAAOD,QAAU,IAA0B,6C,qBCA3CC,EAAOD,QAAU,IAA0B,+C,gFCA3C,yBAA8c,EAAG,G,qBCAjdC,EAAOD,QAAU,IAA0B,8B","file":"js/app.9d421f21.js","sourcesContent":[" \t// install a JSONP callback for chunk loading\n \tfunction webpackJsonpCallback(data) {\n \t\tvar chunkIds = data[0];\n \t\tvar moreModules = data[1];\n \t\tvar executeModules = data[2];\n\n \t\t// add \"moreModules\" to the modules object,\n \t\t// then flag all \"chunkIds\" as loaded and fire callback\n \t\tvar moduleId, chunkId, i = 0, resolves = [];\n \t\tfor(;i < chunkIds.length; i++) {\n \t\t\tchunkId = chunkIds[i];\n \t\t\tif(Object.prototype.hasOwnProperty.call(installedChunks, chunkId) && installedChunks[chunkId]) {\n \t\t\t\tresolves.push(installedChunks[chunkId][0]);\n \t\t\t}\n \t\t\tinstalledChunks[chunkId] = 0;\n \t\t}\n \t\tfor(moduleId in moreModules) {\n \t\t\tif(Object.prototype.hasOwnProperty.call(moreModules, moduleId)) {\n \t\t\t\tmodules[moduleId] = moreModules[moduleId];\n \t\t\t}\n \t\t}\n \t\tif(parentJsonpFunction) parentJsonpFunction(data);\n\n \t\twhile(resolves.length) {\n \t\t\tresolves.shift()();\n \t\t}\n\n \t\t// add entry modules from loaded chunk to deferred list\n \t\tdeferredModules.push.apply(deferredModules, executeModules || []);\n\n \t\t// run deferred modules when all chunks ready\n \t\treturn checkDeferredModules();\n \t};\n \tfunction checkDeferredModules() {\n \t\tvar result;\n \t\tfor(var i = 0; i < deferredModules.length; i++) {\n \t\t\tvar deferredModule = deferredModules[i];\n \t\t\tvar fulfilled = true;\n \t\t\tfor(var j = 1; j < deferredModule.length; j++) {\n \t\t\t\tvar depId = deferredModule[j];\n \t\t\t\tif(installedChunks[depId] !== 0) fulfilled = false;\n \t\t\t}\n \t\t\tif(fulfilled) {\n \t\t\t\tdeferredModules.splice(i--, 1);\n \t\t\t\tresult = __webpack_require__(__webpack_require__.s = deferredModule[0]);\n \t\t\t}\n \t\t}\n\n \t\treturn result;\n \t}\n\n \t// The module cache\n \tvar installedModules = {};\n\n \t// object to store loaded and loading chunks\n \t// undefined = chunk not loaded, null = chunk preloaded/prefetched\n \t// Promise = chunk loading, 0 = chunk loaded\n \tvar installedChunks = {\n \t\t\"app\": 0\n \t};\n\n \tvar deferredModules = [];\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"/\";\n\n \tvar jsonpArray = window[\"webpackJsonp\"] = window[\"webpackJsonp\"] || [];\n \tvar oldJsonpFunction = jsonpArray.push.bind(jsonpArray);\n \tjsonpArray.push = webpackJsonpCallback;\n \tjsonpArray = jsonpArray.slice();\n \tfor(var i = 0; i < jsonpArray.length; i++) webpackJsonpCallback(jsonpArray[i]);\n \tvar parentJsonpFunction = oldJsonpFunction;\n\n\n \t// add entry module to deferred list\n \tdeferredModules.push([0,\"chunk-vendors\"]);\n \t// run deferred modules when ready\n \treturn checkDeferredModules();\n","module.exports = __webpack_public_path__ + \"img/adjacent-projects.623df028.png\";","module.exports = __webpack_public_path__ + \"img/human-risk-label-distribution.7f43b45b.png\";","import mod from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&lang=css&\"; export default mod; export * from \"-!../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=style&index=0&lang=css&\"","import mod from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ClimateChangeNews.vue?vue&type=style&index=0&id=37456a1b&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ClimateChangeNews.vue?vue&type=style&index=0&id=37456a1b&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/identified-elbow.0a811f57.png\";","module.exports = __webpack_public_path__ + \"26b941884f7f4d6a0809608739be75ab.pdf\";","module.exports = __webpack_public_path__ + \"img/svm-hyperparameters.ce4c2b31.png\";","module.exports = __webpack_public_path__ + \"img/informativeness_per_class_metric.3013ed91.png\";","module.exports = __webpack_public_path__ + \"img/text-analysis-module.55d5baf1.png\";","module.exports = __webpack_public_path__ + \"img/flood_presence_per_class_metric.cf8e214a.png\";","module.exports = __webpack_public_path__ + \"img/human-risk-data-splits.2f568d84.png\";","import mod from \"-!../../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ImageAnalysisCarousel.vue?vue&type=style&index=0&id=a1aa68b2&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ImageAnalysisCarousel.vue?vue&type=style&index=0&id=a1aa68b2&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/text-features-for-clustering.baeaa85d.png\";","import mod from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Boomerang.vue?vue&type=style&index=0&id=461c2fc6&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Boomerang.vue?vue&type=style&index=0&id=461c2fc6&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/fp-train.88f043ab.png\";","var map = {\n\t\"./17_835_Poster.pdf\": \"4131\",\n\t\"./6_864_Project.pdf\": \"1a9a\",\n\t\"./Dylan_Lewis_Resume.pdf\": \"8050\",\n\t\"./FrequencyPlot.png\": \"339e\",\n\t\"./IDS131_Final_Report.pdf\": \"06be\",\n\t\"./IDS131_Poster.pdf\": \"b720\",\n\t\"./NER.png\": \"7d1a\",\n\t\"./RelativeWordFreqDiffFlorida.png\": \"f061\",\n\t\"./RelativeWordFrequencyDiff.png\": \"3ea6\",\n\t\"./adjacent-projects.png\": \"00b6\",\n\t\"./agg-metrics-fc.png\": \"2a76\",\n\t\"./algo-selection-table.png\": \"e2e6\",\n\t\"./bag-of-word-features.png\": \"c12f\",\n\t\"./bert-features.png\": \"5fbe\",\n\t\"./bert-preprocessing.png\": \"422e\",\n\t\"./boomerang-home.jpg\": \"5e93\",\n\t\"./character_box_and_whisk_fc_rm.png\": \"64f1\",\n\t\"./cluster-labels.png\": \"742e\",\n\t\"./clustering-hyperparameters.png\": \"f19a\",\n\t\"./clustering-pipeline.png\": \"6539\",\n\t\"./create-account.png\": \"e1a7\",\n\t\"./damage_severity_confusion_matrix.png\": \"aec1\",\n\t\"./damage_severity_per_class_metric.png\": \"9469\",\n\t\"./ds-fc.png\": \"4bd4\",\n\t\"./ds-train.png\": \"964e\",\n\t\"./dylan-n-leo.jpg\": \"c9e4\",\n\t\"./elbow-plot.png\": \"682a\",\n\t\"./fare-surge-graph-pred.png\": \"84e6\",\n\t\"./feelings.jpg\": \"c207\",\n\t\"./final-assessment.png\": \"3b51\",\n\t\"./final-project-overview.png\": \"d8cb\",\n\t\"./flood_confusion_matrix.png\": \"6e61\",\n\t\"./flood_presence_per_class_metric.png\": \"0c67\",\n\t\"./fp-fc.png\": \"1d5c\",\n\t\"./fp-train.png\": \"166a\",\n\t\"./hc-fc.png\": \"9409\",\n\t\"./hc-train.png\": \"be19\",\n\t\"./human-risk-aucpr.png\": \"540a\",\n\t\"./human-risk-cm-svm.png\": \"83a1\",\n\t\"./human-risk-data-splits.png\": \"0d12\",\n\t\"./human-risk-diagram.png\": \"2848\",\n\t\"./human-risk-label-distribution.png\": \"0207\",\n\t\"./human-risk-model-evaluation.png\": \"1a95\",\n\t\"./human-risk-per-class-metric.png\": \"5dc2\",\n\t\"./humanitarian_categories_confusion_matrix.png\": \"d54d\",\n\t\"./humanitarian_categories_per_class_metric.png\": \"4f00\",\n\t\"./iaa.png\": \"5984\",\n\t\"./identified-elbow.png\": \"0597\",\n\t\"./image-analysis-module-modified.png\": \"7b9c\",\n\t\"./image-analysis-module.png\": \"d730\",\n\t\"./in-fc.png\": \"6a64\",\n\t\"./in-train.png\": \"bb54\",\n\t\"./informativeness_confusion_matrix.png\": \"70f6\",\n\t\"./informativeness_per_class_metric.png\": \"0abc\",\n\t\"./int-dev-gray-lit.pdf\": \"4cdb\",\n\t\"./int-dev-results.png\": \"269c\",\n\t\"./join-communities.png\": \"a21f\",\n\t\"./labeled-clusters.png\": \"efe3\",\n\t\"./leo_n_me.jpg\": \"ee29\",\n\t\"./linkedin-profpic.jpg\": \"cbeb\",\n\t\"./masters-thesis-overview.png\": \"ab31\",\n\t\"./n-gram-preprocessing.png\": \"3690\",\n\t\"./negative-positive.jpg\": \"3bc0\",\n\t\"./ner-res-tools.png\": \"2981\",\n\t\"./ner-results.png\": \"332f\",\n\t\"./nested-cv-graph.png\": \"1cc0\",\n\t\"./nested-cv-table.png\": \"88ad\",\n\t\"./nested-cv.png\": \"f9f1\",\n\t\"./network_cosine_similarity.png\": \"afd1\",\n\t\"./network_tfidf_wordclouds.png\": \"64e1\",\n\t\"./networks_and_years_cosine_similarity.png\": \"cad8\",\n\t\"./pika-gif.gif\": \"9faf\",\n\t\"./portrait.jpg\": \"4906\",\n\t\"./preliminary-assessment.png\": \"8537\",\n\t\"./project-collage.png\": \"8b08\",\n\t\"./qualitative-summaries.png\": \"83ec\",\n\t\"./query-subset.png\": \"cc1a\",\n\t\"./reptile.png\": \"a2c3\",\n\t\"./resume.png\": \"a297\",\n\t\"./svm-hyperparameters.png\": \"0aa8\",\n\t\"./taxi-fare-and-surge-pred.png\": \"f124\",\n\t\"./taxi-proj-thumbnail.png\": \"e75b\",\n\t\"./test-set-eval.png\": \"621b\",\n\t\"./text-analysis-module.png\": \"0af6\",\n\t\"./text-features-for-clustering.png\": \"130d\",\n\t\"./text-preprocessing.png\": \"788c\",\n\t\"./tf-idf-features.png\": \"78b4\",\n\t\"./tfidf-features.png\": \"efe5\",\n\t\"./tfidf-networks.png\": \"9271\",\n\t\"./trump-campaign.png\": \"89e4\",\n\t\"./txt-data-collection.png\": \"9c7a\",\n\t\"./unlabeled-clusters.png\": \"4ed8\",\n\t\"./workshop-insights.png\": \"35dd\",\n\t\"./workshop-preface-questions.png\": \"835a\",\n\t\"./years_cosine_similarity.png\": \"878d\"\n};\n\n\nfunction webpackContext(req) {\n\tvar id = webpackContextResolve(req);\n\treturn __webpack_require__(id);\n}\nfunction webpackContextResolve(req) {\n\tif(!__webpack_require__.o(map, req)) {\n\t\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\t\te.code = 'MODULE_NOT_FOUND';\n\t\tthrow e;\n\t}\n\treturn map[req];\n}\nwebpackContext.keys = function webpackContextKeys() {\n\treturn Object.keys(map);\n};\nwebpackContext.resolve = webpackContextResolve;\nmodule.exports = webpackContext;\nwebpackContext.id = \"1913\";","module.exports = __webpack_public_path__ + \"img/human-risk-model-evaluation.d3492779.png\";","module.exports = __webpack_public_path__ + \"6b8bd84b37ff0d5c4d59da59cd2fd588.pdf\";","module.exports = __webpack_public_path__ + \"img/nested-cv-graph.1f656c95.png\";","module.exports = __webpack_public_path__ + \"img/fp-fc.ef341c25.png\";","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NavBar.vue?vue&type=style&index=0&id=bbe68fba&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NavBar.vue?vue&type=style&index=0&id=bbe68fba&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/int-dev-results.d88be6ab.png\";","module.exports = __webpack_public_path__ + \"img/human-risk-diagram.bc1e3e28.png\";","module.exports = __webpack_public_path__ + \"img/ner-res-tools.ffa2482a.png\";","module.exports = __webpack_public_path__ + \"img/agg-metrics-fc.ecfcaeb6.png\";","import mod from \"-!../../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TextAnalysisCarousel.vue?vue&type=style&index=0&id=753752de&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TextAnalysisCarousel.vue?vue&type=style&index=0&id=753752de&scoped=true&lang=css&\"","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NotFoundComponent.vue?vue&type=style&index=0&id=4ffa6c61&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NotFoundComponent.vue?vue&type=style&index=0&id=4ffa6c61&scoped=true&lang=css&\"","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=style&index=0&id=6eb776f2&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=style&index=0&id=6eb776f2&scoped=true&lang=css&\"","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=style&index=0&id=ea434f30&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=style&index=0&id=ea434f30&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/ner-results.0168e441.png\";","module.exports = __webpack_public_path__ + \"img/FrequencyPlot.970f0c80.png\";","module.exports = __webpack_public_path__ + \"img/workshop-insights.afcd08dd.png\";","module.exports = __webpack_public_path__ + \"img/n-gram-preprocessing.fa91db2c.png\";","module.exports = __webpack_public_path__ + \"img/final-assessment.4073dc4c.png\";","module.exports = __webpack_public_path__ + \"img/negative-positive.6d580dd8.jpg\";","import mod from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./GNNsTaxiPrediction.vue?vue&type=style&index=0&id=385cd9ad&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./GNNsTaxiPrediction.vue?vue&type=style&index=0&id=385cd9ad&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/RelativeWordFrequencyDiff.ea5b86c8.png\";","module.exports = __webpack_public_path__ + \"37b2baebe5028084933fc1d6ed8b2604.pdf\";","module.exports = __webpack_public_path__ + \"img/bert-preprocessing.bb955516.png\";","module.exports = __webpack_public_path__ + \"img/portrait.b2c89646.jpg\";","module.exports = __webpack_public_path__ + \"img/ds-fc.33490c85.png\";","module.exports = __webpack_public_path__ + \"6e061964729851cc0f11c268e292463e.pdf\";","module.exports = __webpack_public_path__ + \"img/unlabeled-clusters.d9e606fb.png\";","module.exports = __webpack_public_path__ + \"img/humanitarian_categories_per_class_metric.d3a64368.png\";","module.exports = __webpack_public_path__ + \"img/human-risk-aucpr.c545b91a.png\";","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Resume.vue?vue&type=style&index=0&id=28d22fa6&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Resume.vue?vue&type=style&index=0&id=28d22fa6&scoped=true&lang=css&\"","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"container-fluid p-3 min-vh-100\",attrs:{\"id\":\"app\"}},[_c('Header'),(this.$route.path !== \"/\" && this.$route.path !== \"/404\")?_c('NavBar'):_vm._e(),_c('router-view')],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","import $ from 'jquery';\n\nexport const MAIN_PROJECTS = [\n    {   \n        id: \"ml-for-crowdsourced-crisis-data\",\n        link: \"/projects/ml-for-crowdsourced-crisis-data\", \n        src: {imgFilename: \"masters-thesis-overview.png\"},\n        title: \"Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response\", \n        desc: \"Masters thesis/Research project on constructing labeled crowdsourced crisis datasets and developing machine learning models to assist crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"See Thesis Document\",\n            url: \"https://dspace.mit.edu/handle/1721.1/144911\"\n        }\n    },\n    {   \n        id: \"nlp-for-int-dev-gray-lit\",\n        link: \"/projects/nlp-for-int-dev-gray-lit\", \n        src: {imgFilename: \"int-dev-results.png\"},\n        title: \"Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature\", \n        desc: \"NLP project which investigates Named Entity Recognition (NER) and K-means Clustering on a corpus of 244 documents of International Development literature papers for streamlining evidence synthesis process on international development gray literature.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Presentation PDF\",\n            url: \"./assets/int-dev-gray-lit.pdf\"\n        }\n    },\n    {   \n        id: \"climate-change-news\",\n        link: \"/projects/climate-change-news\", \n        src: {imgFilename: \"final-project-overview.png\"},\n        title: \"Evolution of the U.S. TV News Narrative on Climate Change\", \n        desc: \"Data Science & NLP project in Python that investigated the evolution of climate change coverage frequency & content between popular U.S. TV News Networks CNN, Fox News, and MSNBC over July 2009-January 2020.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Poster PDF\",\n            url: \"./assets/IDS131_Poster.pdf\"\n        }\n    },\n    {   \n        id: \"taxi-pred-img\",\n        link: \"/projects/gnns-taxi-prediction\", \n        src: {imgFilename: \"fare-surge-graph-pred.png\"},\n        title: \"Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction\", \n        desc: \"Machine Learning project in Python which investigated graph neural networks (GNNs) for the tasks of NYC taxi fare and demand surge prediction.\"\n    },\n    {   \n        id: \"trump-img\",\n        link: \"/projects/trump-speech-analysis\", \n        src: {imgFilename: \"FrequencyPlot.png\", sizeClass: 'w-75'},\n        title: \"Trump Campaign Speech Analysis\", \n        desc: \"Data Science project in R which investigated how Donald Trump's 2016 campaign speeches may have influenced public sentiment on a regional level.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Poster PDF\",\n            url: \"./assets/17_835_Poster.pdf\"\n        }\n    },\n    {   \n        id: \"boomerang-img\",\n        link: \"/projects/boomerang\", \n        src: {imgFilename: \"boomerang-home.jpg\"},\n        title: \"Boomerang\",\n        desc: \"Boomerang is a full-stack web application where users can efficiently and reliably borrow items from others within their communities.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Go to the Boomerang website\",\n            url: \"https://team-aesthetech-boomerang.herokuapp.com/\"\n        }\n    },\n]\n\nexport const ML_MODULES = [\n    {   \n        id: \"image-module-img\",\n        link: \"/projects/ml-for-crowdsourced-crisis-data/image-analysis-module\", \n        src: {imgFilename: \"image-analysis-module-modified.png\"},\n        title: \"Image Analysis Module\", \n        desc: \"ML Module focused on Constructing human-annotated datasets and assessing the quality of the annotations, developing CNN models to classify the crowdsourced crisis image data for various classification tasks, and conducting image annotation workshops with crisis managers from various contexts.\",\n        projectBtnText: \"See Module Details\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"See Related Code\",\n            url: \"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Image%20Analysis%20Module\"\n        }\n    },\n    {   \n        id: \"text-module-img\",\n        link: \"/projects/ml-for-crowdsourced-crisis-data/text-analysis-module\", \n        src: {imgFilename: \"text-analysis-module.png\"},\n        title: \"Text Analysis Module\", \n        desc: \"ML Module focused on Classification of crowdsourced crisis text (in JA) for Human Risk informed from the identified information needs and priorities of crisis managers and Clustering of crisis text data for uncovering semantic categories in the data.\",\n        projectBtnText: \"See Module Details\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"See Related Code\",\n            url: \"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module\"\n        }\n    }\n]\n\nexport function scrollUpFunc() {\n    window.scrollTo(0, 0);\n}\n\nexport function enableScrollUpOnCarousel(carouselID) {\n    $(carouselID).on('slid.bs.carousel', function() {\n        scrollUpFunc();\n    });\n}\n\nexport function enableSwipeOnCarousel(carouselID) {\n    $(carouselID).on('touchstart', function(event){\n        const xClick = event.originalEvent.touches[0].pageX;\n        $(this).one('touchmove', function(event) {\n            const xMove = event.originalEvent.touches[0].pageX;\n            const sensitivityInPx = 20;\n  \n            if( Math.floor(xClick - xMove) > sensitivityInPx ){\n                $(this).carousel('next')\n            }\n            else if( Math.floor(xClick - xMove) < -sensitivityInPx ){\n                $(this).carousel('prev')\n            }\n        });\n        $(this).on('touchend', function(){\n            $(this).off('touchmove');\n        });\n      });\n}","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row justify-content-end\"},[_c('div',{staticClass:\"col-12 col-md-4 col-lg-4\",attrs:{\"id\":\"website-title\"}},[_c('h1',{attrs:{\"id\":\"name animate__fadeInDown\"},on:{\"click\":_vm.goHome}},[_vm._v(\" Dylan Lewis \")])]),_vm._m(0)])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-md-4 my-auto\",attrs:{\"id\":\"logos-col\"}},[_c('div',{staticClass:\"d-flex flex-row justify-content-center justify-content-md-end\"},[_c('a',{attrs:{\"href\":\"mailto: dylanrl97@gmail.com\"}},[_c('i',{staticClass:\"fa fa-envelope-o fa-3x\"}),_c('i',{staticClass:\"fa fa-envelope-o fa-2x\"})]),_c('a',{attrs:{\"href\":\"https://www.linkedin.com/in/dyllew/\",\"target\":\"_blank\"}},[_c('i',{staticClass:\"fa fa-linkedin-square fa-3x\"}),_c('i',{staticClass:\"fa fa-linkedin-square fa-2x\"})]),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/\",\"target\":\"_blank\"}},[_c('i',{staticClass:\"fa fa-github fa-3x\"}),_c('i',{staticClass:\"fa fa-github fa-2x\"})])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row justify-content-end\">\n        <div id=\"website-title\" class=\"col-12 col-md-4 col-lg-4\">\n            <h1 v-on:click=\"goHome\" id=\"name animate__fadeInDown\"> Dylan Lewis </h1>\n        </div>\n        <div id=\"logos-col\" class=\"col-md-4 my-auto\">\n            <div class=\"d-flex flex-row justify-content-center justify-content-md-end\">\n                <a href=\"mailto: dylanrl97@gmail.com\">\n                    <i class=\"fa fa-envelope-o fa-3x\"></i>\n                    <i class=\"fa fa-envelope-o fa-2x\"></i>\n                </a> \n                <a href=\"https://www.linkedin.com/in/dyllew/\" target=\"_blank\">\n                    <i class=\"fa fa-linkedin-square fa-3x\"></i>\n                    <i class=\"fa fa-linkedin-square fa-2x\"></i>\n                </a>\n                <a href=\"https://github.com/dyllew/\" target=\"_blank\">\n                    <i class=\"fa fa-github fa-3x\"></i>\n                    <i class=\"fa fa-github fa-2x\"></i>\n                </a>\n            </div>\n        </div>\n    </div>    \n</template>\n\n<script>\nexport default {\n  name: 'Header',\n  methods: {\n      goHome() {\n          this.$router.push('/');\n      }\n  }\n}\n</script>\n\n<style scoped>\nh1 {\n    font-size: 50px;\n}\n\nh1:hover {\n    cursor: pointer;\n}\n\na {\n    color: #61DAFB;\n    margin-left: 15px;\n    margin-right: 15px;\n}\n\na:hover {\n    color: white;\n}\n\n@media (max-width: 761px) {\n    .fa-3x {\n        display: none;\n    }\n}\n@media (min-width: 761px) {\n    .fa-2x {\n        display: none;\n    }\n}\n\n@media (min-width: 768px) {\n    #logos-col {\n        justify-content: flex-end; \n    }\n}\n\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Header.vue?vue&type=template&id=ea434f30&scoped=true&\"\nimport script from \"./Header.vue?vue&type=script&lang=js&\"\nexport * from \"./Header.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Header.vue?vue&type=style&index=0&id=ea434f30&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"ea434f30\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row pt-2 justify-content-center\"},[_c('div',{staticClass:\"col-md-12\",attrs:{\"id\":\"nav-bar\"}},[_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/about\"}},[_vm._v(\"About Me\")]),_vm._v(\" | \"),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/projects\"}},[_vm._v(\"Projects\")]),_vm._v(\" | \"),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/artwork\"}},[_vm._v(\"Artwork\")]),_vm._v(\" | \"),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/resume\"}},[_vm._v(\"Resume\")])],1)])}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row pt-2 justify-content-center\">\n      <div id=\"nav-bar\" class=\"col-md-12\">\n          <router-link class=\"router-link\" to=\"/about\">About Me</router-link> |\n          <router-link class=\"router-link\" to=\"/projects\">Projects</router-link> |\n          <router-link class=\"router-link\" to=\"/artwork\">Artwork</router-link> |\n          <router-link class=\"router-link\" to=\"/resume\">Resume</router-link>\n      </div>\n    </div>  \n</template>\n\n<script>\nexport default {\n  name: 'NavBar'\n}\n</script>\n\n<style scoped>\n\n#nav-bar {\n  font-size: 30px;\n}\n\n@media (max-width: 500px) {\n    #nav-bar {\n      font-size: 20px;\n    }\n}\n\n.router-link {\n  color: #61DAFB;\n}\n\n.router-link:hover {\n  color: whitesmoke;\n}\n\n.router-link-active {\n  color: white;\n  text-decoration: underline;\n}\n\n\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NavBar.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NavBar.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./NavBar.vue?vue&type=template&id=bbe68fba&scoped=true&\"\nimport script from \"./NavBar.vue?vue&type=script&lang=js&\"\nexport * from \"./NavBar.vue?vue&type=script&lang=js&\"\nimport style0 from \"./NavBar.vue?vue&type=style&index=0&id=bbe68fba&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"bbe68fba\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div id=\"app\" class=\"container-fluid p-3 min-vh-100\">\n    <Header />\n    <NavBar v-if=\"this.$route.path !== `/` && this.$route.path !== `/404`\" />\n    <router-view></router-view>\n  </div>\n</template>\n\n<script>\nimport 'bootstrap/dist/css/bootstrap.min.css';\nimport 'jquery/src/jquery.js';\nimport 'bootstrap/dist/js/bootstrap.min.js';\n\nimport $ from 'jquery';\n\nimport { scrollUpFunc } from './constants';\n\nimport Header from './components/Header';\nimport NavBar from './components/NavBar';\n\nexport default {\n  name: 'App',\n  components: {\n    Header,\n    NavBar\n  },\n  mounted() {  \n      // This code block was taken from the solution to this Q on stackoverflow: https://stackoverflow.com/questions/21349984/how-to-make-bootstrap-carousel-slider-use-mobile-left-right-swipe\n      $('.carousel').on('touchstart', function(event){\n      const xClick = event.originalEvent.touches[0].pageX;\n      $(this).one('touchmove', function(event) {\n          const xMove = event.originalEvent.touches[0].pageX;\n          const sensitivityInPx = 5;\n\n          if( Math.floor(xClick - xMove) > sensitivityInPx ){\n              $(this).carousel('next');\n          }\n          else if( Math.floor(xClick - xMove) < -sensitivityInPx ){\n              $(this).carousel('prev');\n          }\n          scrollUpFunc();\n      });\n      $(this).on('touchend', function(){\n          $(this).off('touchmove');\n      });\n    });\n  },\n}\n\n</script>\n\n<style>\n\nhtml {\n  height: 100vh;\n  scroll-behavior: smooth;\n}\n\nbody {\n  min-height: 100vh;\n  margin: 0;\n  padding: 0;\n}\n\n#app {\n  font-family: Courier, Helvetica, Arial, sans-serif;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  text-align: center;\n  color: #61DAFB;\n  background-color: #080707;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.tooltip {\n  position: relative;\n  display: inline-block;\n  border-bottom: 1px dotted black;\n}\n\n.tooltip .tooltiptext {\n  visibility: hidden;\n  width: 120px;\n  background-color: black;\n  color: #fff;\n  text-align: center;\n  border-radius: 6px;\n  padding: 5px 0;\n  \n  /* Position the tooltip */\n  position: absolute;\n  z-index: 1;\n  bottom: 100%;\n  left: 50%;\n  margin-left: -60px;\n}\n\n.tooltip:hover .tooltiptext {\n  visibility: visible;\n}\n\n@media (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n\n</style>\n","import mod from \"-!../node_modules/cache-loader/dist/cjs.js??ref--12-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../node_modules/cache-loader/dist/cjs.js??ref--12-0!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js!../node_modules/cache-loader/dist/cjs.js??ref--0-0!../node_modules/vue-loader/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=450e1f00&\"\nimport script from \"./App.vue?vue&type=script&lang=js&\"\nexport * from \"./App.vue?vue&type=script&lang=js&\"\nimport style0 from \"./App.vue?vue&type=style&index=0&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row pt-4 align-items-md-start justify-content-around\"},[_c('div',{staticClass:\"col-6 pt-4 col-md-3 pt-md-5\",attrs:{\"id\":\"about\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"About Me\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToAbout}},[_c('img',{staticClass:\"rounded img-fluid upper-img\",attrs:{\"src\":require(\"../../public/assets/dylan-n-leo.jpg\")}})])])]),_c('div',{staticClass:\"col-6 pt-4 col-md-3 pt-md-5\",attrs:{\"id\":\"resume\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Resume\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToResume}},[_c('img',{staticClass:\"rounded img-fluid upper-img\",attrs:{\"src\":require(\"../../public/assets/resume.png\")}})])])]),_c('div',{staticClass:\"col-md-4\",attrs:{\"id\":\"artwork-and-projs\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Projects\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToProjects}},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../public/assets/project-collage.png\")}})])]),_c('div',{staticClass:\"img-container mt-4\"},[_c('h4',[_vm._v(\"Artwork\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToArtwork}},[_c('img',{staticClass:\"rounded img-fluid lower-img\",attrs:{\"src\":require(\"../../public/assets/portrait.jpg\")}})])])]),_c('div',{staticClass:\"col-5 pt-5 pt-md-0 col-md-2\",attrs:{\"id\":\"artwork\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Artwork\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToArtwork}},[_c('img',{staticClass:\"rounded img-fluid lower-img\",attrs:{\"src\":require(\"../../public/assets/portrait.jpg\")}})])])]),_c('div',{staticClass:\"col-6 pt-5 pt-md-0 col-md-4\",attrs:{\"id\":\"projects\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Projects\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToProjects}},[_c('img',{staticClass:\"rounded img-fluid lower-img\",attrs:{\"src\":require(\"../../public/assets/project-collage.png\")}})])])])])}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"row pt-4 align-items-md-start justify-content-around\">\n    <div id=\"about\" class=\"col-6 pt-4 col-md-3 pt-md-5\">\n      <div class=\"img-container\">\n        <h4>About Me</h4>\n        <div v-on:click=\"goToAbout\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid upper-img\"\n            src=\"../../public/assets/dylan-n-leo.jpg\"\n          />\n        </div>\n      </div>\n    </div>\n\n    <div id=\"resume\" class=\"col-6 pt-4 col-md-3 pt-md-5\">\n      <div class=\"img-container\">\n        <h4>Resume</h4>\n        <div v-on:click=\"goToResume\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid upper-img\"\n            src=\"../../public/assets/resume.png\"\n          />\n        </div>\n      </div>\n    </div>\n    <div id=\"artwork-and-projs\" class=\"col-md-4\">\n        <div class=\"img-container\">\n          <h4>Projects</h4>\n          <div v-on:click=\"goToProjects\" class=\"img-holder\">\n            <img\n              class=\"rounded img-fluid\"\n              src=\"../../public/assets/project-collage.png\"\n            />\n          </div>\n        </div>\n        <div class=\"img-container mt-4\">\n          <h4>Artwork</h4>\n          <div v-on:click=\"goToArtwork\" class=\"img-holder\">\n            <img\n              class=\"rounded img-fluid lower-img\"\n              src=\"../../public/assets/portrait.jpg\"\n            />\n          </div>\n        </div>\n    </div>\n    <div id=\"artwork\" class=\"col-5 pt-5 pt-md-0 col-md-2\">\n      <div class=\"img-container\">\n        <h4>Artwork</h4>\n        <div v-on:click=\"goToArtwork\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid lower-img\"\n            src=\"../../public/assets/portrait.jpg\"\n          />\n        </div>\n      </div>\n    </div>\n    <div id=\"projects\" class=\"col-6 pt-5 pt-md-0 col-md-4\">\n      <div class=\"img-container\">\n        <h4>Projects</h4>\n        <div v-on:click=\"goToProjects\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid lower-img\"\n            src=\"../../public/assets/project-collage.png\"\n          />\n        </div>\n      </div>\n    </div>\n  </div>\n</template>\n\n<script>\nexport default {\n  name: \"Home\",\n  data() {\n    return {\n      windowWidth: window.innerWidth,\n    };\n  },\n  methods: {\n    goToAbout() {\n      this.$router.push(\"/about\");\n    },\n    goToProjects() {\n      this.$router.push(\"/projects\");\n    },\n    goToArtwork() {\n      this.$router.push(\"/artwork\");\n    },\n    goToResume() {\n      this.$router.push(\"/resume\");\n    },\n  },\n};\n</script>\n\n<style scoped>\n\nh4 {\n  font-size: 4vw;\n}\n\n#artwork-and-projs {\n    display: none;\n}\n\n.img-container:hover {\n  border: 1px solid #61dafb;\n  border-radius: 2px;\n  cursor: pointer;\n  color: white;\n}\n\n@media (max-width: 750px) {\n  .upper-img {\n    height: 40vw;\n    width: auto;\n  }\n\n  .lower-img {\n    height: 30vw;\n    width: auto;\n  }\n}\n\n@media (min-width: 768px) {\n  h4 {\n  font-size: 2vw;\n  }\n\n  #about {\n    order: 1;\n  }\n\n  #resume {\n    order: 3;\n  }\n\n  #artwork-and-projs {\n      display: inline;\n      order: 2;\n  }\n\n  #projects {\n    display: none;\n  }\n\n  #artwork {\n    display: none;\n  }\n\n    @media (min-width: 1150px) { \n        .upper-img {\n          height: 30vw;\n          width: auto;\n      }\n        .lower-img {\n            height: 15vw;\n        }\n    }\n}\n</style>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Home.vue?vue&type=template&id=6eb776f2&scoped=true&\"\nimport script from \"./Home.vue?vue&type=script&lang=js&\"\nexport * from \"./Home.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Home.vue?vue&type=style&index=0&id=6eb776f2&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"6eb776f2\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row pt-3 pt-lg-5 justify-content-center align-items-center\",attrs:{\"id\":\"home-container\"}},[_vm._m(0),_c('div',{staticClass:\"col-11 col-md-6 col-lg-5 col-xl-5\",attrs:{\"id\":\"about-description\"}},[_c('p',{attrs:{\"id\":\"intro-paragraph\"}},[_vm._v(\" Hey there! I'm a Masters grad from MIT where I studied Computer Science, specifically Artificial Intelligence. My research focused on machine learning tools for assisting crisis managers during climate crises using crowdsourced climate crisis data. You can learn more about my research\"),_c('a',{attrs:{\"href\":\"#/projects/ml-for-crowdsourced-crisis-data\"},on:{\"click\":_vm.scrollUp}},[_vm._v(\"here!\")]),_vm._v(\"I'm also a proud dad to my son, Leo  \")]),_vm._m(1),_c('p',[_vm._v(\" In my freetime I love to explore nature, play video games (Zelda/DMC/Soulsborne/Skyrim/Kingdom Hearts/Fallout), cook/bake something new, make and experiment with different forms of art, develop new skills, and go on adventures with my pup! \")])])])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-6 col-md-5 col-lg-5 col-xl-3 mr-xl-5\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"id\":\"leo-and-dylan-pic\",\"src\":require(\"../../public/assets/leo_n_me.jpg\"),\"alt\":\"Leo and Dylan\"}})])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('p',[_vm._v(\" I'm interested in Software Engineering, Data Science & Machine Learning, and Web Development! You can contact me through\"),_c('a',{attrs:{\"href\":\"mailto: dylanrl97@gmail.com\"}},[_vm._v(\"email,\")]),_vm._v(\"add me on\"),_c('a',{attrs:{\"href\":\"https://www.linkedin.com/in/dyllew/\",\"target\":\"_blank\"}},[_vm._v(\"LinkedIn,\")]),_vm._v(\"or checkout my\"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/\",\"target\":\"_blank\"}},[_vm._v(\"GitHub!\")]),_vm._v(\" I'm currently looking for remote Data Science and Software Engineering positions! \")])}]\n\nexport { render, staticRenderFns }","<template>\n    <div id=\"home-container\" class=\"row pt-3 pt-lg-5 justify-content-center align-items-center\">\n        <div class=\"col-6 col-md-5 col-lg-5 col-xl-3 mr-xl-5\">\n            <img\n              id=\"leo-and-dylan-pic\"\n              src=\"../../public/assets/leo_n_me.jpg\" \n              class=\"rounded img-fluid\"\n              alt=\"Leo and Dylan\" />\n        </div>\n        <div id=\"about-description\" class=\"col-11 col-md-6 col-lg-5 col-xl-5\">\n            <p id=\"intro-paragraph\">\n              Hey there! I'm a Masters grad from MIT where I studied\n              Computer Science, specifically Artificial Intelligence. My research focused on machine learning tools\n              for assisting crisis managers during climate crises using crowdsourced climate crisis data. You can learn more\n              about my research<a href=\"#/projects/ml-for-crowdsourced-crisis-data\" @click=\"scrollUp\">here!</a>I'm also a proud dad to my son, Leo \n            </p>\n            <p>\n              I'm interested in Software Engineering, Data Science & Machine Learning, and Web Development! \n              You can contact me through<a href=\"mailto: dylanrl97@gmail.com\">email,</a>add me on<a href=\"https://www.linkedin.com/in/dyllew/\" target=\"_blank\">LinkedIn,</a>or checkout my<a href=\"https://github.com/dyllew/\" target=\"_blank\">GitHub!</a>\n              I'm currently looking for remote Data Science and Software Engineering positions!\n            </p>\n            <p>\n              In my freetime I love to explore nature, play video games (Zelda/DMC/Soulsborne/Skyrim/Kingdom Hearts/Fallout), cook/bake something new, make and experiment with different forms of art,\n              develop new skills, and go on adventures with my pup!\n            </p> \n        </div>\n    </div>    \n</template>\n\n<script>\nimport { scrollUpFunc } from '../constants';\n\nexport default {\n  name: 'About',\n  methods: {\n    scrollUp() {\n      scrollUpFunc();\n    }\n  }\n}\n</script>\n\n<style scoped>\na {\n    color: hotpink;\n    margin-left: 1vw;\n    margin-right: 1vw;\n}\n\na:hover {\n    color: white;\n}\n\n#leo-and-dylan-pic  {\n  max-height: 40vw;\n  border: 0.25vw solid #61DAFB;\n}\n\n#about-description {\n  font-size: 1.75vw;\n  text-align: left;\n}\n\n@media (min-width: 0px) and (max-width: 768px) {\n\n    #leo-and-dylan-pic {\n      max-height: 60vw;\n    }\n\n    #about-description p {\n      font-size: 4vw;\n    }\n\n    #intro-paragraph {\n      padding-top: 10vw;\n    }\n\n}\n\n@media (min-width: 768px) and (max-width: 900px) {\n\n    #leo-and-dylan-pic {\n      max-height: 50vw;\n    }\n\n    #about-description p {\n      font-size: 2vw;\n    }\n\n    #intro-paragraph {\n      padding-top: 5vw;\n    }\n\n}\n\n@media (min-width: 900px) and (max-width: 1200px) {\n\n    #leo-and-dylan-pic {\n      max-height: 40vw;\n    }\n\n    #about-description p {\n      font-size: 1.75vw;\n    }\n}\n\n@media (min-width: 1200px) {\n\n    #about-description p {\n      font-size: 1.35vw;\n    }\n}\n\n</style>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./About.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./About.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./About.vue?vue&type=template&id=e91b748a&scoped=true&\"\nimport script from \"./About.vue?vue&type=script&lang=js&\"\nexport * from \"./About.vue?vue&type=script&lang=js&\"\nimport style0 from \"./About.vue?vue&type=style&index=0&id=e91b748a&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"e91b748a\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},_vm._l((_vm.projects),function(project){return _c('div',{key:project.link,staticClass:\"col-12 col-md-4 col-lg-4 pt-3\"},[_c('ProjectCard',{attrs:{\"project\":project}})],1)}),0)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"card border-info bg-transparent align-items-center\"},[_c('h5',{staticClass:\"card-header\"},[_vm._v(_vm._s(_vm.project.title))]),_c('img',{staticClass:\"card-img-top\",class:_vm.project.src.sizeClass ? _vm.project.src.sizeClass : '',attrs:{\"id\":_vm.project.id,\"src\":_vm.getImgURL(_vm.project.src.imgFilename),\"alt\":\"Project Image\"}}),_c('div',{staticClass:\"card-body\"},[_c('p',{staticClass:\"card-text\"},[_vm._v(_vm._s(_vm.project.desc))]),(_vm.project.projectWebsite)?_c('div',{attrs:{\"id\":_vm.project.projectWebsite.id}},[_c('a',{staticClass:\"btn btn-light col-5\",attrs:{\"href\":_vm.project.projectWebsite.url,\"target\":\"_blank\"}},[_vm._v(_vm._s(_vm.project.projectWebsite.btnText))]),_c('span'),_c('a',{staticClass:\"btn btn-primary text-light col-5\",on:{\"click\":function($event){return _vm.goToProjectPage(_vm.project.link)}}},[_vm._v(_vm._s(_vm.projectBtnText))])]):_c('div',{attrs:{\"id\":\"button-holder\"}},[_c('a',{staticClass:\"btn btn-primary text-light\",on:{\"click\":function($event){return _vm.goToProjectPage(_vm.project.link)}}},[_vm._v(\"See Project Details\")])])])])}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"card border-info bg-transparent align-items-center\">\n        <h5 class=\"card-header\">{{project.title}}</h5>\n        <img :id=\"project.id\" \n            class=\"card-img-top\" \n            :class=\"project.src.sizeClass ? project.src.sizeClass : ''\" \n            :src=\"getImgURL(project.src.imgFilename)\" \n            alt=\"Project Image\">\n        <div class=\"card-body\">\n            <p class=\"card-text\">{{project.desc}}</p>\n            <div v-if=\"project.projectWebsite\" :id=\"project.projectWebsite.id\">\n                <a :href=\"project.projectWebsite.url\" target=\"_blank\" class=\"btn btn-light col-5\">{{project.projectWebsite.btnText}}</a>\n                <span></span>\n                <a v-on:click=\"goToProjectPage(project.link)\" class=\"btn btn-primary text-light col-5\">{{projectBtnText}}</a>\n            </div>\n            <div id=\"button-holder\" v-else>\n                <a v-on:click=\"goToProjectPage(project.link)\" class=\"btn btn-primary text-light\">See Project Details</a>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc } from '../constants';\n\nexport default {\n    name: 'ProjectCard',\n    props: ['project'],\n    methods: {\n        goToProjectPage(projectURL) {\n            this.$router.push(projectURL);\n            scrollUpFunc();\n        },\n        getImgURL(imgFilename) {\n            return require('../../public/assets/' + imgFilename)\n        },\n    },\n    computed: {\n        projectBtnText() {\n           return this.project.projectBtnText ? this.project.projectBtnText : \"See Project Details\"\n        }\n    }\n}\n</script>\n\n<style scoped>\np {\n    text-align: left;\n}\n\nh5 {\n    color: white;\n}\n\n#button-holder {\n    display: flex;\n    justify-content: space-around;\n    align-items: center;\n}\n\n#image-module-img {\n    width: 40vh;\n    height: 40vh;\n}\n\n#text-module-img {\n    height: 40vh;\n}\n\n@media (max-width: 500px) {\n    #text-module-img {\n        max-height: 50vw;\n    }\n}\n\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ProjectCard.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ProjectCard.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./ProjectCard.vue?vue&type=template&id=7bfc7dfc&scoped=true&\"\nimport script from \"./ProjectCard.vue?vue&type=script&lang=js&\"\nexport * from \"./ProjectCard.vue?vue&type=script&lang=js&\"\nimport style0 from \"./ProjectCard.vue?vue&type=style&index=0&id=7bfc7dfc&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"7bfc7dfc\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div class=\"row align-items-center justify-content-center\">\n    <div class=\"col-12 col-md-4 col-lg-4 pt-3\" v-for=\"project in projects\" :key=\"project.link\">\n        <ProjectCard :project=\"project\"></ProjectCard>\n    </div>\n  </div>    \n</template>\n\n<script>\nimport ProjectCard from './ProjectCard'\nimport { MAIN_PROJECTS } from '../constants';\n\nexport default {\n  name: 'Projects',\n  components: {\n    ProjectCard,\n  },\n  data() {\n      return { \n          projects: MAIN_PROJECTS\n      }\n  },\n}\n</script>\n\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Projects.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Projects.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Projects.vue?vue&type=template&id=085604dc&\"\nimport script from \"./Projects.vue?vue&type=script&lang=js&\"\nexport * from \"./Projects.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"artwork row align-items-center justify-content-center justify-content-xl-around\"},[_c('div',{staticClass:\"col-8 pt-3 pt-md-0 col-md-4\"},[_c('h4',{staticClass:\"header\"},[_vm._v(\"Ethereality\")]),_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../public/assets/portrait.jpg\")}})]),_c('div',{staticClass:\"col-8 pt-4 pt-md-2 col-md-4\"},[_c('h4',{staticClass:\"header\"},[_vm._v(\"Phantom Dragon\")]),_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../public/assets/reptile.png\")}})])])}]\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"artwork row align-items-center justify-content-center justify-content-xl-around\">\n    <div class=\"col-8 pt-3 pt-md-0 col-md-4\">\n      <h4 class=\"header\">Ethereality</h4>\n      <img\n        class=\"rounded img-fluid\"\n        src=\"../../public/assets/portrait.jpg\"\n      />\n    </div>\n    <div class=\"col-8 pt-4 pt-md-2 col-md-4\">\n        <h4 class=\"header\">Phantom Dragon</h4>\n        <img\n          class=\"rounded img-fluid\"\n          src=\"../../public/assets/reptile.png\"\n        />\n    </div>\n    <!-- <div class=\"col-8 pt-4 pt-md-2 col-md-3\">\n        <h4></h4>\n        <img\n          class=\"rounded img-fluid\"\n          src=\"../../public/assets/feelings.jpg\"\n        />\n    </div> -->\n  </div>    \n</template>\n\n<script>\n\nexport default {\n  name: 'Artwork'\n}\n</script>\n\n<style>\n\n.header {\n  color: white;\n}\n\n</style>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Artwork.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Artwork.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Artwork.vue?vue&type=template&id=5ed9c197&\"\nimport script from \"./Artwork.vue?vue&type=script&lang=js&\"\nexport * from \"./Artwork.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Artwork.vue?vue&type=style&index=0&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"resume row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 mt-4\",attrs:{\"id\":\"sticky-btn\"}},[_c('a',{staticClass:\"btn btn-primary text-light\",attrs:{\"target\":\"_blank\",\"href\":\"./assets/Dylan_Lewis_Resume.pdf\"}},[_vm._v(\"View PDF in Tab\")])]),_c('div',{staticClass:\"col-12 mt-2\"},[_c('embed',{staticClass:\"pdf\",attrs:{\"src\":\"./assets/Dylan_Lewis_Resume.pdf\"}})])])}]\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"resume row align-items-center justify-content-center\">\n    <div id=\"sticky-btn\" class=\"col-12 mt-4\">\n      <a target=\"_blank\" class=\"btn btn-primary text-light\" href=\"./assets/Dylan_Lewis_Resume.pdf\">View PDF in Tab</a>\n    </div>\n    <div class=\"col-12 mt-2\">\n      <embed class=\"pdf\" src=\"./assets/Dylan_Lewis_Resume.pdf\"/>\n    </div>\n  </div>    \n</template>\n\n<script>\n\nexport default {\n  name: 'Resume'\n}\n</script>\n\n<style scoped>\n#sticky-btn {\n  position: -webkit-sticky;\n  position: sticky;\n  top: 0;\n}\n\na {\n  color: #61DAFB;\n  font-size: 30px;\n}\n\na:hover {\n  color: #61DAFB;\n}\n\n@media (max-width: 700px) {\n  .pdf {\n    display: none;\n  }\n}\n\n@media  (min-width: 700px) and (max-width: 1200px) {\n  .pdf {\n    width: 600px;\n    height: 600px;\n  }\n}\n\n@media (min-width: 1200px) {\n  .pdf {\n    width: 800px;\n    height: 800px;\n  }\n}\n\n</style>\n","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Resume.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Resume.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Resume.vue?vue&type=template&id=28d22fa6&scoped=true&\"\nimport script from \"./Resume.vue?vue&type=script&lang=js&\"\nexport * from \"./Resume.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Resume.vue?vue&type=style&index=0&id=28d22fa6&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"28d22fa6\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_vm._m(0),_c('div',{staticClass:\"col-md-10\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"MLForCrowdsourcedCrisisDataCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\",\"data-touch\":\"true\"}},[_vm._m(1),_c('div',{staticClass:\"carousel-inner\"},[_vm._m(2),_vm._m(3),_vm._m(4),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Machine Learning Methodology\")]),_vm._v(\" With crisis management partners in Fukuchiyama (FC), Japan, we present our framework through two ML modules:\"),_c('br'),_vm._v(\" Image Analysis Module & Text Analysis Module\"),_c('br'),_c('br'),_c('p',[_vm._v(\" We note that our findings from the Image Analysis Module influenced the design of the Text Analysis Module in order to meet our aim of developing machine learning systems for crisis management which iteratively incorporate the feedback received from crisis managers. \")]),_c('div',{staticClass:\"row align-items-center justify-content-center pb-5\"},_vm._l((_vm.modules),function(module){return _c('div',{key:module.link,staticClass:\"col-12 col-md-6 col-lg-6 pt-3\"},[_c('ProjectCard',{attrs:{\"project\":module}})],1)}),0),_c('p',[_vm._v(\" In the next slides, we discuss the important implications of this study and summarize our contributions to the field of Crisis Informatics. \")])])]),_vm._m(5),_vm._m(6),_vm._m(7),_vm._m(8),_vm._m(9),_vm._m(10)])])])])}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-12 col-md-8\"},[_c('h3',[_vm._v(\"Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"0\"}},[_c('div',{staticClass:\"tooltip\"},[_c('span',{staticClass:\"tooltiptext\"},[_vm._v(\"Thesis Document & Code\")])])]),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"5\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"6\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"7\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"8\"}}),_c('li',{attrs:{\"data-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-slide-to\":\"9\"}})])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item active cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"id\":\"overview-pic\",\"src\":require(\"../../../../public/assets/masters-thesis-overview.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Thesis Document & Code and Related Open-source Python Packages\")]),_c('p',[_vm._v(\" Master of Engineering in Electrical Engineering and Computer Science thesis I wrote as a research assistant at the Urban Risk Lab (URL) at MIT which investigates the development of machine learning models to assist crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response. This presentation introduces the motivation behind this work, explains how the investigation unfolded, and reports the main findings and implications from the research. \"),_c('br'),_c('br'),_vm._v(\" The published thesis document can be found \"),_c('a',{attrs:{\"href\":\"https://dspace.mit.edu/handle/1721.1/144911\",\"target\":\"_blank\"}},[_vm._v(\"here.\")]),_c('br'),_c('br'),_vm._v(\" The code for this thesis was written in Python using Jupyter Notebooks. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\",\"target\":\"_blank\"}},[_vm._v(\"Here's the GitHub Repo.\")]),_c('br'),_vm._v(\" Relatedly, with this thesis, we produced two open-source Python packages to better enable readability, reusability, and reproducibility of the computational utilities derived for performing various experiments and analysis on crowdsourced crisis image and text data: \"),_c('ul',[_c('li',[_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-image-module/0.27.0/\",\"target\":\"_blank\"}},[_vm._v(\"URL Image Module\")]),_vm._v(\" - Utilities for training, testing, and predicting with pretrained Convolutional Neural Networks for classifying crowdsourced crisis image data and constructing & analyzing annotated datasets\")]),_c('li',[_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-text-module/0.6.1/\",\"target\":\"_blank\"}},[_vm._v(\"URL Text Module\")]),_vm._v(\" - Utilities for featurizing crisis text data, training and testing with a variety of classification machine learning models, and visualizing clusters of featurized text data\")])]),_c('br'),_vm._v(\" This research was supported by a grant from Google.org and the Tides Foundation. \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_c('strong',[_vm._v(\"Abstract\")])]),_c('p',[_vm._v(\" The availability of information during a climate crisis event is critical for crisis managers to assess and respond to crisis impact. During crisis events, affected residents post real-time crisis updates on platforms such as \"),_c('a',{attrs:{\"href\":\"https://riskmap.mit.edu/japan.html\",\"target\":\"_blank\"}},[_vm._v(\"RiskMap\")]),_vm._v(\" and \"),_c('a',{attrs:{\"href\":\"https://twitter.com/?lang=en\",\"target\":\"_blank\"}},[_vm._v(\"Twitter\")]),_vm._v(\". These updates provide localized information, which has the potential to enhance crisis awareness and response. However, with limited resources, crisis managers may endure information overload from the inundation of these updates. Prior work has demonstrated the potential of machine learning (ML) methodologies to mitigate this problem. We have identified limitations in the prior work including the lack of involvement of crisis managers in the development and evaluation of a ML methodology. \")]),_c('p',[_vm._v(\" To address these limitations, we propose a novel framework and ML methodology which investigate the efficacy of various ML methods in enhancing crisis awareness and response beyond model performance metrics. This framework aims to iteratively embed the information needs and priorities of crisis managers during crisis into the design of the ML methodology. We cooperated with crisis managers in Fukuchiyama City (FC), a city in Japan which is susceptible to flood events, and analyzed crowdsourced crisis image and text data from past FC flood events. We devised the Flood Presence image classification task, constructed Train/Dev/Test splits, and annotated images from FC. We report a weighted F1 score of 92.1% on the test split and 82.5% on the FC images. Using the results of our image analysis ML methodology and the insights we gained from crisis managers, we iterated on the design of our text analysis ML methodology. This led to the creation of the Human Risk text classification task which is tailored to a subset of the identified information needs of the crisis managers. To align with the priorities of crisis managers for this task, we determined the model evaluation metric to be the F2 score. We report an F2 score of 92.8% on an FC crisis text test dataset, which is a significant improvement over the baseline score of 43.4%. \")]),_c('h5',[_c('strong',[_vm._v(\"Research Question\")])]),_c('strong',{attrs:{\"id\":\"research-question\"}},[_vm._v(\" In collaborating with crisis managers, how can machine learning methods be utilized and evaluated to effectively reduce the information overload of crowdsourced data on crisis managers during flood events for enhanced crisis awareness and response? \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Main Research Aims\")]),_c('p',{attrs:{\"id\":\"research-aims\"}},[_c('ol',[_c('li',[_c('strong',[_vm._v(\"To reduce information overload during a crisis\")]),_vm._v(\" using accurate, efficient, automated image and text classification of crisis reports by machine learning (ML) models during crisis.\")]),_c('li',[_c('strong',[_vm._v(\"To embed tacit knowledge, information needs, and decision-making priorities of crisis managers\")]),_vm._v(\" into the designed ML methodology.\")]),_c('li',[_c('strong',[_vm._v(\"To evaluate methodology in collaboration with crisis managers\")]),_vm._v(\", e.g. crisis managers in Fukuchiyama, Japan.\")]),_c('li',[_c('strong',[_vm._v(\"To incorporate evaluation results and iterate on the ML methodology\")]),_vm._v(\" in order to better reach aim of using AI to assist crisis managers during crisis.\")])]),_vm._v(\" We investigate various ML techniques that can be applied to mitigate information overload for crisis managers during crisis events while also assessing if those techniques can satisfy the information needs and priorities of crisis managers through qualitative and quantitative evaluation. \")]),_c('h5',[_vm._v(\"Novel Framework\")]),_c('p',[_vm._v(\" To achieve the aims above we develop a novel framework in the crisis informatics community consisting of the following components: \"),_c('ul',[_c('li',[_c('strong',[_vm._v(\"Classification Task Creation\")]),_vm._v(\" - We develop new classification tasks using labels provided to us by crisis managers and labels present in open-source datasets\")]),_c('li',[_c('strong',[_vm._v(\"Data Annotation Procedure\")]),_vm._v(\" - We open-souce the annotation guide we develop for greater transparency into the procurement of human-annotated data that is used to train and evaluate ML models\")]),_c('li',[_c('strong',[_vm._v(\"Interannotator Agreement/Data Reliability Analysis\")]),_vm._v(\" - After performing an annotation effort on data provided by crisis managers, we analyze the quality of the annotations through interannotator agreement analysis to demonstrate the importance of understanding data quality prior to using it for ML purposes\")]),_c('li',[_c('strong',[_vm._v(\"Model Development and Evaluation; Per-Class Performance Analysis\")]),_vm._v(\" - The metrics we used to evaluate models are derived either from metrics reported in the literature or, more notably, \"),_c('strong',[_vm._v(\"metrics determined from insights provided by crisis managers directly\")]),_vm._v(\". Additionally, we consider issues of class imbalance and report per-class performance and confusion matrices to provide more granular insight into model performance. Finally, we establish baselines to compare against the models we develop to assess the degree to which the models we develop outperform the baseline\")]),_c('li',[_c('strong',[_vm._v(\"Qualitative Analysis through Workshops with Crisis Managers\")]),_vm._v(\" - We sought to cooperate with crisis managers with the intent of using this framework to iteratively design ML systems, such as the specific classification tasks performed by the models and their associated performance metric, by iteratively incorporating feedback and insights from crisis managers, so that the system better aligns with their information needs and decision-making priorities\")])]),_vm._v(\" This framework situates \"),_c('i',[_vm._v(\"Model Development and Evaluation\")]),_vm._v(\", which is commonplace in prior work, as part of a larger, contextualized analysis. On that note, we expand on the notion of \"),_c('i',[_vm._v(\"Model Development and Evaluation\")]),_vm._v(\" beyond what is typically seen in the prior work. \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h4',[_vm._v(\"Discussion & Implications of Study\")]),_c('h5',[_vm._v(\"Involvement of Crisis Managers in the development & iteration of ML methodology\")]),_c('p',[_vm._v(\" While crisis classification tasks have been published in the literature, to the best of our knowledge, this work is the first of its kind to both involve & incorporate the insights of crisis managers into an ML methodology to reduce information overload of crisis managers during crisis. This is best seen from the image annotation workshops we held with EOC and the incorporation of those results into our text analysis ML methodology. \")]),_c('h5',[_vm._v(\"Investigation of Non-English Crisis Text \")]),_c('p',[_vm._v(\" While we have developed the infrastructure to develop machine learning text models for any language, we underscore our investigation of Japanese crisis text as prior work has focused on applying NLP techniques to predominantly English crisis text. \")]),_c('h5',[_vm._v(\"Contextualized Framework towards better human-centered design of an AI system\")]),_c('p',[_vm._v(\" Our novel framework contextualizes model performance (e.g. F1, F2, precision, etc. ) as part of a broad approach to assessing the efficacy of using ML in reducing information overload. That is, we consider other important aspects in the design of our system in order to better serve those we aim to assist, crisis managers. We investigate classification task creation, data quality & interannotator agreement analysis, issues of class imbalance, involving crisis managers in the development process, and the determination of a performance metric & corresponding performance baselines \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h4',[_vm._v(\"Summary of Main Contribution\")]),_c('p',[_vm._v(\" With this thesis, we have introduced and exemplified a framework which aims to embed all of the mentioned considerations into the process of designing, developing, and iterating on a ML methodology for enhancing crisis awareness and response. Most notably, this framework situates model development and evaluation, which is commonplace in prior work, as one piece of a broader, contextualized understanding of the efficacy ML methodologies can have for crisis managers in mitigating information overload from crowdsourced crisis reports. Additionally, this framework promotes the iterative development of AI systems and ML methodologies which is informed from the insights gained from engaging with crisis managers, aiming to address this gap in prior work. \")]),_c('p',[_vm._v(\" This framework is only the beginning for similar work in this domain, as the development of AI systems and ML methodologies for mitigating information overload of crisis managers has many complex intricacies for which we only scratch the surface in our attempt to broaden this discussion within the field. We contribute this framework and the full exhibition of the principles and analyses contained within it to work closer towards a goal worth striving for: enhanced crisis awareness and response from automated assessment of crowdsourced crisis reporting. \")]),_c('p',[_vm._v(\" In the next slides, we further detail the other contributions of this thesis. \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h4',[_vm._v(\"Contributions 1/3\")]),_c('h5',[_vm._v(\"Novel Framework for Information Overload Mitigation of Crowdsourced Crisis Data\")]),_c('p',[_vm._v(\" Our main contribution is a novel framework that unifies a variety of machine learning techniques, analysis, and evaluation on images and text present in crowdsourced crisis data, which aims to reduce the information overload of crisis managers, specifically during flood crisis events. This approach leverages classical machine learning models and deep learning models aimed to provide accurate and efficient classifications on individual flood reports to mitigate information overload of crisis managers. This framework addresses the limitations in the prior work such as the lack of engagement of crisis managers in the development of the ML methodology and an evaluation which expands beyond model metrics. In this framework, we embed various utilities for conducting analysis such as computing properties and statistics of annotated datasets and model performance. \")]),_c('h5',[_vm._v(\"Open-source Python Packages/Code\")]),_c('p',[_vm._v(\" To ensure the reuse of the analysis conducted in this work, we release two open-source Python packages: One for the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-image-module/0.27.0/\",\"target\":\"_blank\"}},[_vm._v(\"Image Analysis Module\")]),_vm._v(\" and the other for the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-text-module/0.6.1/\",\"target\":\"_blank\"}},[_vm._v(\"Text Analysis Module\")]),_vm._v(\". These packages include utilities for training, testing, and prediction using the models presented in this work, computing statistics for interannotator agreement, and computing metrics for model performance. For text data, there are also utilities for featurization of text, performing nested cross validation, and clustering. We note that there are utilities for plotting such as methods for producing the plots shown throughout this project. \")]),_c('p',[_vm._v(\" In addition to these packages, we release an \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\",\"target\":\"_blank\"}},[_vm._v(\"open-source repository\")]),_vm._v(\" containing Jupyter notebooks, relevant documents (e.g. the mentioned annotation guide), and other code required to reproduce the experiments and reuse the analysis conducted in this work. \")]),_c('h5',[_vm._v(\"Flood Presence Task Creation, Labeled Image Dataset, and Performance Benchmark\")]),_c('p',[_vm._v(\" Since we focused on flood crises, we defined the image prediction task of Flood Presence classification. The Flood Presence task is the binary classification task of determining whether or not there is presence of flood in an image. We construct a labeled dataset for the task consisting of 23.6 images by combining various opensource datasets which were labeled with labels useful for this task, although they were originally developed for other adjacent tasks. We contribute this dataset for further research in crisis informatics. In addition, we provide Train/Dev/Test splits and an associated benchmark performance on the test split using a state-of-the-art Convolutional Neural Network (CNN) model, EfficientNet-B1. \")]),_c('h5',[_vm._v(\"Data Annotation Procedure and Analysis of Interannotator Agreement\")]),_c('p',[_vm._v(\" We developed a procedure for annotating images in order to label the unlabeled image data provided to us by our partners in Fukuchiyama, Japan. This procedure included creating an annotation guide to assist annotators in their labeling. This guide included the name of each task, the names of the mutually-exclusive classes associated with each task, and an associated description and example image for each class. We then had annotators from the Urban Risk Lab independently annotate the images using this guide. \")]),_c('p',[_vm._v(\" After the annotation effort was completed, we were able analyze the interannotator agreement between the annotators and construct ground-truth labels for these images. We computed interannotator reliability statistics to get a sense of how reproducible the annotation procedure was for each task as well as to have transparency of the data quality prior to using it for ML purposes. Finally, we created ground-truth datasets using these labels for the Fukuchiyama images to use in evaluating the image classification ML models we developed. \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h4',[_vm._v(\"Contributions 2/3\")]),_c('h5',[_vm._v(\"Classification and Clustering of Crowdsourced Japanese Crisis Text\")]),_c('p',[_vm._v(\" Research in crisis informatics on crowdsourced crisis data focuses mostly on English, and thus research on crowdsourced Japanese crisis text is sparse. The Text Analysis Module developed in this work focused exclusively on Japanese text data. We explored various numerical representations of the Japanese crisis text reports provided by our partners and developed a pipeline for preprocessing the raw text and producing the numerical representations. Additionally, our partners provided labels along with the text reports, so we experimented with classifying the text. Lastly, we explored the text data using unsupervised learning, specifically, we employed clustering methods to help inform development of classification tasks in future work \")]),_c('h6',[_vm._v(\"Pipeline for Japanese Crisis Text Preprocessing, Tokenization, and Featurization\")]),_c('p',[_vm._v(\" In order to use the text reports as input to the various ML models employed in this work, we represent the raw text string of each report as a numerical vector, or featurization. Depending on the featurization we choose to use for a text report, we may first preprocess the text. This preprocessing included various steps including tokenization, stopword removal, and lemmatization, which we performed using open-source software (i.e. tokenizer and lemmatizer) and publicly available data (i.e. stopwords list) for the Japanese language. We provide a pipeline for preprocessing and performing the following featurizations on Japanese text: \"),_c('ul',[_c('li',[_vm._v(\"n-gram Bag-of-Words (BOW)\")]),_c('li',[_vm._v(\"n-gram Term Frequency-Inverse Document Frequency (TF-IDF)\")]),_c('li',[_vm._v(\"Pretrained Japanese Masked Language Modeling (MLM) BERT Model with Classification (CLS) Pooling Embedding\")])]),_vm._v(\" The resultant feature vectors representing the text enabled us to use them as input to ML models. Thus, we can then employ classification and clustering techniques on the text data. \")]),_c('h6',[_vm._v(\"Human Risk Task Creation and Performance Metric Determination\")]),_c('p',[_vm._v(\" We devised a new text classification task, Human Risk classification. The human risk text classification task determines whether or not a crisis text report indicates if there are people in need of rescue from a crisis. This includes people being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), and/or being in need of life-saving emergency medical care. This classification task was unique among the classification tasks presented in this work because it was devised using labels that came with the text reports given to us by our crisis management partners. Relatedly, we determine the metric to use in model performance evaluation using the insights we gained from our partners. \")]),_c('h6',[_vm._v(\"Exploratory Analysis of Japanese Crisis Text using Unsupervised Learning\")]),_c('p',[_vm._v(\" With the intention of finding cohesive groupings within the Japanese crisis text corpus, which can inform the development of future text classification tasks, we devise a pipeline for featurizing Japanese crisis text, reducing the high-dimensional text feature vector to 2 dimensions, and clustering the data. We evaluate this pipeline both quantitatively and qualitatively, experimenting with various text featurizations including unigram TF-IDF features and pretrained Japanese MLM BERT with CLS Pooling embeddings mentioned above, t-Distributed Stochastic Neighbor Embedding (t-SNE) and Principle Component Analysis (PCA) for dimensionality reduction, and finally K-means and K-medoids for the algorithm which clusters the data. \")]),_c('p',[_vm._v(\" After we determine the optimal combination of text embedding, dimensionality reduction technique, and clustering algorithm, we create brief summaries of each cluster using the unigrams with highest TF-IDF score for each cluster and the closest reports (by euclidean distance) within each cluster to the cluster center to help in the determination of a label for each cluster. Lastly, a member of the Urban Risk Lab at MIT who is fluent in Japanese used these summaries to determine an interpretable label to accompany each cluster found. We thus provide various labels which can be used for classification experiments and analysis in a future work. \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h4',[_vm._v(\"Contributions 3/3\")]),_c('h6',[_vm._v(\"Quantitative and Qualitative Evaluation in Japanese Flood Crisis Context\")]),_c('p',[_vm._v(\" Prior work has typically evaluated ML methods using quantitative measures, mainly classification performance metrics, e.g. accuracy, precision, recall, F1, and AUROC (Area Under the Receiver Operating Characteristic Curve) and their macro and micro variants. However, with the framework we present in this thesis, we aimed to expand the evaluation of the efficacy ML models have in reducing information overload to not only include similar quantitative measures mentioned above, but also qualitative evaluation derived from engaging with our crisis management partners. Beyond having good performance, we hoped to use the qualitative evaluation used in this study to gain a broader understanding of the efficacy a model can provide crisis managers in mitigating information overload and gaining situational awareness. \")]),_c('p',[_vm._v(\" We held image annotation workshops with various crisis managers and aimed to understand what type of information they seek to gain from a crowdsourced image during a flood crisis event. With their insights, we began to understand how our models can be refined or improved, or how new models can be created in order to better serve the information needs of crisis managers more effectively, such as by tailoring the labels and their associated meanings to the information needs of crisis managers suggested from their annotations. Additionally, we gained more insight into the appropriate metrics to use when evaluating models based on the priorities of crisis managers as it relates to the task. From these workshops, we share key lessons that can influence the design of this framework and AI-augmented crisis information systems of the future. In fact, within this work, we used the lessons learned from the image analysis workshops to assist us in determining the performance metric to use when developing models for the human risk text classification task. This exercise exhibited the principle of iterative development our framework intends to promote. \")])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h4',[_vm._v(\"Adjacent Projects\")]),_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"id\":\"overview-pic\",\"src\":require(\"../../../../public/assets/adjacent-projects.png\"),\"alt\":\"First slide\"}}),_c('p',[_vm._v(\" During my time on this project, I defined and mentored undergraduate research projects that were adjacent to my research. These projects included novel topics in the field of Crisis Informatics such as: \"),_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"Interpretability in Image and Text ML Models:\")])]),_c('p',[_vm._v(\" Investigated interpretability algorithms such as GradCAM (Class Activation Mapping) on classified crisis report images and Local Interpretable Model-Agnostic Explaination (LIME) on classified report text in an effort to increase the interpretability of the models which would be employed during crisis and potentially improve model performance through relabeling & retraining. \")])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Multilabel Image Classification:\")])]),_c('p',[_vm._v(\" Investigated CNNs for image classification tasks which can perform multilabel classification (as opposed to single label classification) in the crisis informatics context as most classification tasks in the literature were single-label. \")])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Towards Establishing Interannotator Agreement Standards & Tools in the Crisis Informatics Community:\")])]),_c('p',[_vm._v(\" Investigated the establishment of standards and analysis tools for understanding interannotator agreement (and disagreement) on human-annotated datasets in the crisis informatics community for both single-label and multilabel classification tasks as such standards and tools did not exist in the crisis informatics community. \")])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Development of a Crisis Management Dashboard and Simulations using ML Models and their Predictions:\")])]),_c('p',[_vm._v(\" Investigated the development of an interactive dashboard for crisis managers to use during a crisis event that visualizes the predictions of various machine learning models on a map to provide situational summarization on individual report and aggregate report levels. A simulation was constructed using past citizen crisis reports and predictions on those reports by trained machine learning models. \")])])])]),_c('div',{staticClass:\"col-12\"})])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12 col-md-8\">  \n            <h3>Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response</h3>\n        </div>\n        <div class=\"col-md-10\">\n            <div id=\"MLForCrowdsourcedCrisisDataCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\" data-touch=\"true\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"0\" class=\"active\"><div class=\"tooltip\"><span class=\"tooltiptext\">Thesis Document & Code</span></div></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"2\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"3\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"4\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"5\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"6\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"7\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"8\"></li>\n                    <li data-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-slide-to=\"9\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active cc-carousel-item\">\n                        <img class=\"rounded img-fluid\" id=\"overview-pic\" src=\"../../../../public/assets/masters-thesis-overview.png\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Thesis Document & Code and Related Open-source Python Packages</h5>\n                            <p>\n                                Master of Engineering in Electrical Engineering and Computer Science thesis I wrote as a research assistant at the Urban Risk Lab (URL) at MIT which investigates \n                                the development of machine learning models to assist crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response.\n                                This presentation introduces the motivation behind this work, explains how the investigation unfolded, and reports the main findings and implications from the research.\n                                <br>\n                                <br>\n                                The published thesis document can be found <a href=\"https://dspace.mit.edu/handle/1721.1/144911\" target=\"_blank\">here.</a>\n                                <br>\n                                <br>\n                                The code for this thesis was written in Python using Jupyter Notebooks. <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\" target=\"_blank\">Here's the GitHub Repo.</a>\n                                <br>\n                                Relatedly, with this thesis, we produced two open-source Python packages to better enable readability, reusability, and reproducibility of the computational utilities derived \n                                for performing various experiments and analysis on crowdsourced crisis image and text data:\n                                <ul>\n                                    <li><a href=\"https://pypi.org/project/url-image-module/0.27.0/\" target=\"_blank\">URL Image Module</a> - Utilities for training, testing, and predicting with pretrained Convolutional Neural Networks for classifying\n                                        crowdsourced crisis image data and constructing & analyzing annotated datasets</li>\n                                    <li><a href=\"https://pypi.org/project/url-text-module/0.6.1/\" target=\"_blank\">URL Text Module</a> - Utilities for featurizing crisis text data, training and testing with a variety of classification machine learning models, \n                                        and visualizing clusters of featurized text data</li>\n                                </ul>\n                                <br>\n                                This research was supported by a grant from Google.org and the Tides Foundation.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5><strong>Abstract</strong></h5>\n                            <p>\n                               The availability of information during a climate crisis event is critical for crisis managers to assess and respond to crisis impact. \n                               During crisis events, affected residents post real-time crisis updates on platforms such as <a href=\"https://riskmap.mit.edu/japan.html\" target=\"_blank\">RiskMap</a> and <a href=\"https://twitter.com/?lang=en\" target=\"_blank\">Twitter</a>. \n                               These updates provide localized information, which has the potential to enhance crisis awareness and response. \n                               However, with limited resources, crisis managers may endure information overload from the inundation of these updates. \n                               Prior work has demonstrated the potential of machine learning (ML) methodologies to mitigate this problem. \n                               We have identified limitations in the prior work including the lack of involvement of crisis managers in the development and evaluation of a ML methodology.\n                            </p>\n                            <p>\n                                To address these limitations, we propose a novel framework and ML methodology which investigate the efficacy of various ML methods in enhancing crisis awareness \n                                and response beyond model performance metrics. This framework aims to iteratively embed the information needs and priorities of crisis managers during crisis \n                                into the design of the ML methodology. We cooperated with crisis managers in Fukuchiyama City (FC), a city in Japan which is susceptible to flood events, and \n                                analyzed crowdsourced crisis image and text data from past FC flood events. We devised the Flood Presence image classification task, constructed Train/Dev/Test splits, \n                                and annotated images from FC. We report a weighted F1 score of 92.1% on the test split and 82.5% on the FC images. Using the results of our image analysis \n                                ML methodology and the insights we gained from crisis managers, we iterated on the design of our text analysis ML methodology. This led to the creation of the \n                                Human Risk text classification task which is tailored to a subset of the identified information needs of the crisis managers. \n                                To align with the priorities of crisis managers for this task, we determined the model evaluation metric to be the F2 score. \n                                We report an F2 score of 92.8% on an FC crisis text test dataset, which is a significant improvement over the baseline score of 43.4%. \n                            </p>\n                            <h5><strong>Research Question</strong></h5>\n                            <strong id=\"research-question\">\n                                In collaborating with crisis managers, how can machine learning methods be utilized and evaluated to effectively reduce the information overload of crowdsourced data on\n                                crisis managers during flood events for enhanced crisis awareness and response?\n                            </strong>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Main Research Aims</h5>\n                            <p id=\"research-aims\">\n                                <ol>\n                                    <li><strong>To reduce information overload during a crisis</strong> using accurate, efficient, automated image and text classification of crisis reports by machine learning (ML) models during crisis.</li>\n                                    <li><strong>To embed tacit knowledge, information needs, and decision-making priorities of crisis managers</strong> into the designed ML methodology.</li>\n                                    <li><strong>To evaluate methodology in collaboration with crisis managers</strong>, e.g. crisis managers in Fukuchiyama, Japan.</li>\n                                    <li><strong>To incorporate evaluation results and iterate on the ML methodology</strong> in order to better reach aim of using AI to assist crisis managers during crisis.</li>\n                                </ol>\n                                We investigate various ML techniques\n                                that can be applied to mitigate information overload for crisis managers during crisis\n                                events while also assessing if those techniques can satisfy the information needs and\n                                priorities of crisis managers through qualitative and quantitative evaluation.\n                            </p>\n                            <h5>Novel Framework</h5>\n                            <p>\n                                To achieve the aims above we develop a novel framework in the crisis informatics community consisting of the following components:\n                                <ul>\n                                    <li><strong>Classification Task Creation</strong> - We develop new classification tasks using labels provided to us by crisis managers\n                                        and labels present in open-source datasets</li>\n                                    <li><strong>Data Annotation Procedure</strong> - We open-souce the annotation guide we develop for greater transparency into the procurement of human-annotated data\n                                        that is used to train and evaluate ML models</li>\n                                    <li><strong>Interannotator Agreement/Data Reliability Analysis</strong> - After performing an annotation effort on data provided by crisis managers, we analyze the quality\n                                        of the annotations through interannotator agreement analysis to demonstrate the importance of understanding data quality prior to using it for ML purposes</li>\n                                    <li><strong>Model Development and Evaluation; Per-Class Performance Analysis</strong> - The metrics we used to evaluate models are derived either from metrics reported in the literature or, more notably, <strong>metrics determined from insights \n                                        provided by crisis managers directly</strong>. Additionally, we consider issues of class imbalance and report per-class performance and confusion matrices to provide more granular insight\n                                        into model performance. Finally, we establish baselines to compare against the models we develop to assess the degree to which the models we develop outperform the baseline</li>\n                                    <li><strong>Qualitative Analysis through Workshops with Crisis Managers</strong> - We sought to cooperate with crisis managers with the intent of using this framework to\n                                        iteratively design ML systems, such as the specific classification tasks performed by the models and their associated performance metric, by iteratively incorporating feedback and insights from\n                                        crisis managers, so that the system better aligns with their information needs and decision-making priorities</li>\n                                </ul>\n                                This framework situates <i>Model Development and Evaluation</i>, which is commonplace in prior work, as part of a larger, contextualized analysis. On that note, we\n                                expand on the notion of <i>Model Development and Evaluation</i> beyond what is typically seen in the prior work.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Machine Learning Methodology</h5>\n                                With crisis management partners in Fukuchiyama (FC), Japan, we present our framework through two ML modules:<br> Image Analysis Module & Text Analysis Module<br>\n                            <br>\n                            <p>\n                                We note that our findings from the Image Analysis Module influenced the design of the Text Analysis Module in order to meet our aim of\n                                developing machine learning systems for crisis management which iteratively incorporate the feedback received from crisis managers.\n                            </p>\n                            <div class=\"row align-items-center justify-content-center pb-5\">\n                                <div class=\"col-12 col-md-6 col-lg-6 pt-3\" v-for=\"module in modules\" :key=\"module.link\">\n                                    <ProjectCard :project=\"module\"></ProjectCard>\n                                </div>\n                            </div>\n                            <p>\n                                In the next slides, we discuss the important implications of this study and summarize our contributions to the field of Crisis Informatics.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h4>Discussion & Implications of Study</h4>\n                            <h5>Involvement of Crisis Managers in the development & iteration of ML methodology</h5>\n                            <p>\n                                While crisis classification tasks have been published in the literature, to the best of our knowledge, this work is the first of its kind \n                                to both involve & incorporate the insights of crisis managers into an ML methodology to reduce information overload of crisis managers during crisis. \n                                This is best seen from the image annotation workshops we held with EOC and the incorporation of those results into our text analysis ML methodology.\n                            </p>\n                            <h5>Investigation of Non-English Crisis Text </h5>\n                            <p>\n                                While we have developed the infrastructure to develop machine learning text models for any language, \n                                we underscore our investigation of Japanese crisis text as prior work has focused on applying NLP techniques to predominantly English crisis text.\n                            </p>\n                            <h5>Contextualized Framework towards better human-centered design of an AI system</h5>\n                            <p>\n                                Our novel framework contextualizes model performance (e.g. F1, F2, precision, etc. ) as part of a broad approach to assessing the efficacy of using ML in \n                                reducing information overload. That is, we consider other important aspects in the design of our system in order to better serve those \n                                we aim to assist, crisis managers. We investigate classification task creation, data quality & interannotator agreement analysis,\n                                issues of class imbalance, involving crisis managers in the development process, and the determination of a performance metric \n                                & corresponding performance baselines\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h4>Summary of Main Contribution</h4>\n                            <p>\n                                With this thesis, we have introduced and exemplified a framework which aims to\n                                embed all of the mentioned considerations into the process of designing, developing,\n                                and iterating on a ML methodology for enhancing crisis awareness and\n                                response. Most notably, this framework situates model development and evaluation,\n                                which is commonplace in prior work, as one piece of a broader, contextualized understanding\n                                of the efficacy ML methodologies can have for crisis managers in mitigating\n                                information overload from crowdsourced crisis reports. Additionally, this framework\n                                promotes the iterative development of AI systems and ML methodologies which is\n                                informed from the insights gained from engaging with crisis managers, aiming to\n                                address this gap in prior work.\n                            </p>\n                            <p>\n                                This framework is only the beginning for similar work in this domain, as the development\n                                of AI systems and ML methodologies for mitigating information overload\n                                of crisis managers has many complex intricacies for which we only scratch the surface\n                                in our attempt to broaden this discussion within the field. We contribute this\n                                framework and the full exhibition of the principles and analyses contained within it to\n                                work closer towards a goal worth striving for: enhanced crisis awareness and response\n                                from automated assessment of crowdsourced crisis reporting.\n                            </p>\n                            <p>\n                                In the next slides, we further detail the other contributions of this thesis. \n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h4>Contributions 1/3</h4>\n                            <h5>Novel Framework for Information Overload Mitigation of Crowdsourced Crisis Data</h5>\n                            <p>\n                                Our main contribution is a novel framework that unifies a variety of machine learning\n                                techniques, analysis, and evaluation on images and text present in crowdsourced crisis\n                                data, which aims to reduce the information overload of crisis managers, specifically\n                                during flood crisis events. This approach leverages classical machine learning models\n                                and deep learning models aimed to provide accurate and efficient classifications on\n                                individual flood reports to mitigate information overload of crisis managers. This\n                                framework addresses the limitations in the prior work such as the lack of engagement \n                                of crisis managers in the development of the ML methodology and an evaluation which expands beyond model metrics. \n                                In this framework, we embed various utilities for conducting analysis such as computing properties\n                                and statistics of annotated datasets and model performance.\n                            </p>\n                            <h5>Open-source Python Packages/Code</h5>\n                            <p>\n                                To ensure the reuse of the analysis conducted in this work, we release two open-source\n                                Python packages: One for the <a href=\"https://pypi.org/project/url-image-module/0.27.0/\" target=\"_blank\">Image Analysis Module</a> and the other for the <a href=\"https://pypi.org/project/url-text-module/0.6.1/\" target=\"_blank\">Text\n                                Analysis Module</a>. These packages include utilities for training, testing, and prediction using the models presented in this work, computing statistics for interannotator\n                                agreement, and computing metrics for model performance. For text data, there are also utilities for featurization of text, performing nested cross validation, and clustering.\n                                We note that there are utilities for plotting such as methods for producing the plots shown throughout this project.\n                            </p>\n                            <p>\n                                In addition to these packages, we release an <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\" target=\"_blank\">open-source repository</a> containing\n                                Jupyter notebooks, relevant documents (e.g. the mentioned annotation guide), and other code required to reproduce the experiments and reuse the analysis\n                                conducted in this work.\n                            </p>\n                            <h5>Flood Presence Task Creation, Labeled Image Dataset, and Performance Benchmark</h5>\n                            <p>\n                                Since we focused on flood crises, we defined the image prediction task of Flood Presence classification. \n                                The Flood Presence task is the binary classification task of determining whether or not there is presence of flood in an image. We construct a\n                                labeled dataset for the task consisting of 23.6 images by combining various opensource datasets which were labeled with labels useful for this task, although they\n                                were originally developed for other adjacent tasks. We contribute this dataset for\n                                further research in crisis informatics. In addition, we provide Train/Dev/Test splits\n                                and an associated benchmark performance on the test split using a state-of-the-art\n                                Convolutional Neural Network (CNN) model, EfficientNet-B1.\n                            </p>\n                            <h5>Data Annotation Procedure and Analysis of Interannotator Agreement</h5>\n                            <p>\n                                We developed a procedure for annotating images in order to label the unlabeled image\n                                data provided to us by our partners in Fukuchiyama, Japan. This procedure included creating\n                                an annotation guide to assist annotators in their labeling. This guide included the\n                                name of each task, the names of the mutually-exclusive classes associated with each\n                                task, and an associated description and example image for each class. We then had\n                                annotators from the Urban Risk Lab independently annotate the images using this\n                                guide.\n                            </p>\n                            <p>\n                                After the annotation effort was completed, we were able analyze the interannotator\n                                agreement between the annotators and construct ground-truth labels for these images.\n                                We computed interannotator reliability statistics to get a sense of how reproducible\n                                the annotation procedure was for each task as well as to have transparency of the\n                                data quality prior to using it for ML purposes. Finally, we created ground-truth\n                                datasets using these labels for the Fukuchiyama images to use in evaluating the image\n                                classification ML models we developed.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h4>Contributions 2/3</h4>\n                            <h5>Classification and Clustering of Crowdsourced Japanese Crisis Text</h5>\n                            <p>\n                                Research in crisis informatics on crowdsourced crisis data focuses mostly on English, \n                                and thus research on crowdsourced Japanese crisis text is sparse. The Text Analysis \n                                Module developed in this work focused exclusively on Japanese text data. We explored \n                                various numerical representations of the Japanese crisis text reports provided by our \n                                partners and developed a pipeline for preprocessing the raw text and producing the numerical representations. \n                                Additionally, our partners provided labels along with the text reports, so we experimented with classifying the\n                                text. Lastly, we explored the text data using unsupervised learning, specifically, we\n                                employed clustering methods to help inform development of classification tasks in\n                                future work\n                            </p>\n                            <h6>Pipeline for Japanese Crisis Text Preprocessing, Tokenization, and Featurization</h6>\n                            <p>\n                                In order to use the text reports as input to the various ML models employed in\n                                this work, we represent the raw text string of each report as a numerical vector, or featurization. \n                                Depending on the featurization we choose to use for a text\n                                report, we may first preprocess the text. This preprocessing included various steps\n                                including tokenization, stopword removal, and lemmatization, which we performed\n                                using open-source software (i.e. tokenizer and lemmatizer) and publicly available\n                                data (i.e. stopwords list) for the Japanese language.\n                                We provide a pipeline for preprocessing and performing the following featurizations\n                                on Japanese text:\n                                <ul>\n                                    <li>n-gram Bag-of-Words (BOW)</li>\n                                    <li>n-gram Term Frequency-Inverse Document Frequency (TF-IDF)</li>\n                                    <li>Pretrained Japanese Masked Language Modeling (MLM) BERT Model with Classification (CLS) Pooling Embedding</li>\n                                </ul>\n                                The resultant feature vectors representing the text enabled us to use them as input\n                                to ML models. Thus, we can then employ classification and clustering techniques on\n                                the text data.\n                            </p>\n                            <h6>Human Risk Task Creation and Performance Metric Determination</h6>\n                            <p>\n                                We devised a new text classification task, Human Risk classification. The human\n                                risk text classification task determines whether or not a crisis text report indicates if\n                                there are people in need of rescue from a crisis. This includes people being unable\n                                to evacuate due to physical disability (such as unable to use stairs), surrounding\n                                conditions (such as being trapped in a submerged car), and/or being in need of\n                                life-saving emergency medical care. This classification task was unique among the\n                                classification tasks presented in this work because it was devised using labels that\n                                came with the text reports given to us by our crisis management partners. Relatedly,\n                                we determine the metric to use in model performance evaluation using the insights\n                                we gained from our partners.\n                            </p>\n                            <h6>Exploratory Analysis of Japanese Crisis Text using Unsupervised Learning</h6>\n                            <p>\n                                With the intention of finding cohesive groupings within the Japanese crisis text corpus, which can inform the development of future text classification tasks, we devise\n                                a pipeline for featurizing Japanese crisis text, reducing the high-dimensional text feature vector to 2 dimensions, and clustering the data. We evaluate this pipeline\n                                both quantitatively and qualitatively, experimenting with various text featurizations\n                                including unigram TF-IDF features and pretrained Japanese MLM BERT with CLS\n                                Pooling embeddings mentioned above, t-Distributed Stochastic Neighbor Embedding\n                                (t-SNE) and Principle Component Analysis (PCA) for dimensionality reduction, and finally K-means and K-medoids for the algorithm which clusters\n                                the data.\n                            </p>\n                            <p>\n                                After we determine the optimal combination of text embedding, dimensionality\n                                reduction technique, and clustering algorithm, we create brief summaries of each\n                                cluster using the unigrams with highest TF-IDF score for each cluster and the closest\n                                reports (by euclidean distance) within each cluster to the cluster center to help in the\n                                determination of a label for each cluster. Lastly, a member of the Urban Risk Lab at\n                                MIT who is fluent in Japanese used these summaries to determine an interpretable\n                                label to accompany each cluster found. We thus provide various labels which can be\n                                used for classification experiments and analysis in a future work.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h4>Contributions 3/3</h4>\n                            <h6>Quantitative and Qualitative Evaluation in Japanese Flood Crisis Context</h6>\n                            <p>\n                                Prior work has typically evaluated ML methods using quantitative measures, mainly\n                                classification performance metrics, e.g. accuracy, precision, recall, F1, and AUROC\n                                (Area Under the Receiver Operating Characteristic Curve) and their macro and micro\n                                variants. However, with the framework we present in this thesis, we aimed to expand\n                                the evaluation of the efficacy ML models have in reducing information overload to\n                                not only include similar quantitative measures mentioned above, but also qualitative\n                                evaluation derived from engaging with our crisis management partners. Beyond having good performance, we hoped to use the qualitative evaluation used in this study\n                                to gain a broader understanding of the efficacy a model can provide crisis managers\n                                in mitigating information overload and gaining situational awareness.\n                            </p>\n                            <p>\n                                We held image annotation workshops with various crisis managers and aimed to\n                                understand what type of information they seek to gain from a crowdsourced image\n                                during a flood crisis event. With their insights, we began to understand how our\n                                models can be refined or improved, or how new models can be created in order to\n                                better serve the information needs of crisis managers more effectively, such as by\n                                tailoring the labels and their associated meanings to the information needs of crisis\n                                managers suggested from their annotations. Additionally, we gained more insight\n                                into the appropriate metrics to use when evaluating models based on the priorities of\n                                crisis managers as it relates to the task. From these workshops, we share key lessons\n                                that can influence the design of this framework and AI-augmented crisis information\n                                systems of the future. In fact, within this work, we used the lessons learned from\n                                the image analysis workshops to assist us in determining the performance metric to\n                                use when developing models for the human risk text classification task. This exercise\n                                exhibited the principle of iterative development our framework intends to promote.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h4>Adjacent Projects</h4>\n                            <img class=\"rounded img-fluid\" id=\"overview-pic\" src=\"../../../../public/assets/adjacent-projects.png\" alt=\"First slide\">\n                            <p>\n                                During my time on this project, I defined and mentored undergraduate research projects that were adjacent to my research.\n\n                                These projects included novel topics in the field of Crisis Informatics such as: \n                                <ul>\n                                    <li>\n                                        <strong><u>Interpretability in Image and Text ML Models:</u></strong>\n                                        <p>\n                                            Investigated interpretability algorithms such as GradCAM (Class Activation Mapping) on classified crisis report images and Local Interpretable Model-Agnostic Explaination\n                                            (LIME) on classified report text in an effort to increase the interpretability of the models which would be employed during crisis and potentially \n                                            improve model performance through relabeling & retraining.\n                                        </p>\n                                    </li>\n                                    <li>\n                                        <strong><u>Multilabel Image Classification:</u></strong>\n                                        <p>\n                                            Investigated CNNs for image classification tasks which can perform multilabel classification (as opposed to single label classification) in the crisis informatics context as\n                                            most classification tasks in the literature were single-label.\n                                        </p>\n                                    </li>\n                                    <li>\n                                        <strong><u>Towards Establishing Interannotator Agreement Standards & Tools in the Crisis Informatics Community:</u></strong>\n                                        <p>\n                                            Investigated the establishment of standards and analysis tools for understanding interannotator agreement (and disagreement) on human-annotated datasets in the crisis informatics community for\n                                            both single-label and multilabel classification tasks as such standards and tools did not exist in the crisis informatics community.\n                                        </p>\n                                    </li>\n                                    <li>\n                                        <strong><u>Development of a Crisis Management Dashboard and Simulations using ML Models and their Predictions:</u></strong>\n                                        <p>\n                                            Investigated the development of an interactive dashboard for crisis managers to use during a crisis event that visualizes the predictions of\n                                            various machine learning models on a map to provide situational summarization on individual report and aggregate report levels. \n                                            A simulation was constructed using past citizen crisis reports and predictions on those reports by trained machine learning models.\n                                        </p>\n                                    </li>\n                                </ul>\n                            </p>\n                            <div class=\"col-12\">\n\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport ProjectCard from '../../ProjectCard.vue';\nimport { ML_MODULES, enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../../constants.js';\n\nexport default {\n  name: 'MLForCrowdsourcedCrisisData',\n  components: {\n    ProjectCard\n  },\n  data() {\n    return {\n        modules: ML_MODULES\n    }\n  },\n  mounted() {\n    enableScrollUpOnCarousel('#MLForCrowdsourcedCrisisDataCarousel');\n    enableSwipeOnCarousel('#MLForCrowdsourcedCrisisDataCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h4, h5 {\n    color: white;\n}\n\n#overview-pic {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 90vh;\n    height: 40vh;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n@media (max-width: 500px) {\n\n    #overview-pic {\n        max-height: 50vw;\n    }\n\n    h3 {\n        font-size: 4.5vw;\n    }\n\n}\n\n</style>","import mod from \"-!../../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./MLForCrowdsourcedCrisisData.vue?vue&type=template&id=79893eb2&scoped=true&\"\nimport script from \"./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"\nexport * from \"./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"\nimport style0 from \"./MLForCrowdsourcedCrisisData.vue?vue&type=style&index=0&id=79893eb2&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"79893eb2\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-8\"},[_c('h3',[_vm._v(\"Image Analysis Module\")])]),_c('div',{staticClass:\"col-12 col-md-10\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"ImageAnalysisCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"5\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"6\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"7\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"8\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"9\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"10\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"11\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"12\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"13\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"14\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"15\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"16\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"17\"}}),_c('li',{attrs:{\"data-target\":\"#ImageAnalysisCarousel\",\"data-slide-to\":\"18\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item cc-carousel-item active\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"image-analysis-module\",\"src\":require(\"../../../../public/assets/image-analysis-module.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"carousel-text col-12 col-md-6\"},[_c('h5',[_vm._v(\"Image Analysis Methodology Overview\")]),_c('p',[_vm._v(\" The goal of the Image Analysis Module is to utilize pretrained Convolutional Neural Network models to yield efficient and accurate predictions from image data in crowdsourced crisis reports such as those on RiskMap & Twitter. The classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP) form a diverse suite of labels for assisting crisis managers in automatically classifying the data. In a fraction of a second, the model predictions made for these tasks provide a series of categorizations for an individual report. We leverage state-of-the-art CNNs, which strike a necessary balance between model complexity, memory and storage constraints, and model performance to provide these predictions. \"),_c('br'),_c('br'),_vm._v(\" To achieve this aim, we use large, labeled, open-source datasets for training and evaluating the models. We formulate a new crisis image classification task for detecting flood presence in an image and construct a new dataset altogether using flood images from various open-source datasets, which contained flood-adjacent labels. We were given unlabeled flood crisis images from crisis managers in Fukuchiyama (FC), Japan. We devise an annotation procedure to label this data and analyze the results of human-annotations on the images. Since this data was not used to train or develop the models, we aimed to see if our trained models could accurately classify unseen images from the FC context. We held various image annotation workshops with crisis managers to identify the limitations in our approach and understand how we could iterate on the design of our ML methodology. This evaluation procedure thus enabled us to evaluate the use of image classification models in assisting crisis managers using quantitative metrics as well as qualitatively through the feedback we got through the image annotation workshops. This evaluation directly influenced our approach for devising the Text Analysis Module. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Datasets & Train/Dev/Test Splits for Model Development & Evaluation\")]),_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"carousel-text col-12 col-md-9\"},[_c('p',[_vm._v(\" Since we used CNN models, specifically the EfficientNet-B1 architecture pretrained on ImageNet, we wanted to make use of large open-source Twitter crisis image datasets for finetuning the models to perform the classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP). \")]),_c('h6',[_vm._v(\"Open-source Consolidated Crisis Image Datasets\")]),_c('p',[_vm._v(\" For the DS, HC, and IN tasks, we made use of the open-source train/dev/test splits made available at \"),_c('a',{attrs:{\"href\":\"https://crisisnlp.qcri.org/crisis-image-datasets-asonam20\",\"target\":\"_blank\"}},[_vm._v(\"CrisisNLP\")]),_vm._v(\". We train the models which perform these tasks using the train and dev splits. For evaluation, we make use of the respective test splits for each task. \")]),_c('h6',[_vm._v(\"Flood Presence Task Creation and Dataset Formation\")]),_c('p',[_vm._v(\" In this work we focus on flood-crisis events, thus we used various open-source image datasets which have flood-adjacent labels and map them to the binary labels of \\\"Flood\\\"/\\\"Not Flood\\\". Using the resulting dataset, we create randomized, non-overlapping Train/Dev/Test splits. Similar to the above, we use the train & dev splits to develop the FP model, and the test split to evaluate the model. \")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-8\"},[_c('h5',[_vm._v(\"Flood Presence Dataset Composition\")]),_vm._v(\" Composition of the Flood Presence dataset from the original datasets and the number of images for each label. \"),_c('table',{attrs:{\"id\":\"fp-table\"}},[_c('tr',[_c('th',[_c('strong',[_vm._v(\"Dataset\")])]),_c('th',[_c('strong',[_vm._v(\"Flood\")])]),_c('th',[_c('strong',[_vm._v(\"Not Flood\")])]),_c('th',[_c('strong',[_vm._v(\"Total\")])])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Consolidated Disaster Types\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.08916\",\"target\":\"_blank\"}},[_vm._v(\"Alam et al. 2020\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"3201\")]),_c('td',[_vm._v(\"14310\")]),_c('td',[_vm._v(\"17511\")])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Central European Floods 2013\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/1908.03361\",\"target\":\"_blank\"}},[_vm._v(\"Barz et al. 2018\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"3151\")]),_c('td',[_vm._v(\"559\")]),_c('td',[_vm._v(\"3710\")])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Harz Region Floods 2017\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.05756\",\"target\":\"_blank\"}},[_vm._v(\"Barz et al. 2020\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"264\")]),_c('td',[_vm._v(\"405\")]),_c('td',[_vm._v(\"669\")])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Rhine River Floods 2018\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.05756\",\"target\":\"_blank\"}},[_vm._v(\"Barz et al. 2020\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"730\")]),_c('td',[_vm._v(\"1007\")]),_c('td',[_vm._v(\"1737\")])]),_c('tr',[_c('td',[_vm._v(\"Total\")]),_c('td',[_vm._v(\"7346\")]),_c('td',[_vm._v(\"16281\")]),_c('td',[_vm._v(\"23627\")])])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Training Sets\")]),_c('div',{staticClass:\"row align-items-center justify-content-between\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/ds-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/hc-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/in-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/fp-train.png\")}})])]),_c('br'),_c('p',[_vm._v(\" As part of the framework we developed, we investigated the class imbalance for each of the image classification training sets. We observed significant imbalance in the label distributions for the Damage Severity and the Humanitarian Categories tasks. We note that this imbalance could be problematic for the performance of the models on the minority classes for those tasks, e.g. the \\\"Mild\\\", \\\"Rescue, Volunteering, or Donation Effort\\\", and \\\"Affected, Injured, or Dead People\\\" classes. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Model Evaluation on Test Splits & Flood Presence Benchmark Performance\")])]),_c('div',{staticClass:\"col-12 col-md-8\"},[_c('p',[_vm._v(\" Performance of image classification models on their respective test splits. [1] as referred to in the table can be found in the footnote below.\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn1\",\"id\":\"ref1\"}},[_vm._v(\"1\")])])]),_c('br'),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"test-set-eval\",\"src\":require(\"../../../../public/assets/test-set-eval.png\")}})])]),_c('br'),_c('p',[_vm._v(\" Similar to the authors in [1]\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn1\",\"id\":\"ref1\"}},[_vm._v(\"1\")])]),_vm._v(\", we report overall model performance as weighted metrics in order to take into account class imbalance present in the test splits. From the table above, we observe that the models we finetuned achieve a weighted F1 score within a 1% difference compared to the model performances reported in [1]. \"),_c('strong',[_vm._v(\"We report a benchmark performance of 92.1% weighted F1 for the Flood Presence (FP) task.\")]),_vm._v(\" We postulate the comparatively higher performance of FP task to be due to the task being binary as well as being the most clear and objective task, thus being a comparatively simipler task for the model to learn. We explore this more through interannotator agreement analysis discussed in the next slides. \")]),_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fn1\"}},[_vm._v(\"1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \"),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2011.08916.pdf\",\"target\":\"_blank\"}},[_vm._v(\"Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response\")]),_vm._v(\", In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.\"),_c('a',{attrs:{\"href\":\"#ref1\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Annotating Fukuchiyama Crisis Images\")]),_c('p',[_vm._v(\" To evaluate our developed models and framework in a context which is susceptible to flood events, we cooperated with crisis managers in Fukuchiyama, Japan to attain \"),_c('strong',[_vm._v(\"658 images\")]),_vm._v(\" from previous flood events as well as non-crisis normal days in Fukuchiyama, which were collected on the ground, similar to RiskMap & Twitter crisis images. This evaluation enabled us to understand how well training the models on the large, consolidated crisis datasets, which cover a diverse set geographical locations and a multitude of crisis events, would perform on the unseen data from flood events in Fukuchiyama. To form evaluation/test sets from this data, we needed to label the images for each of the four image classification tasks we've discussed. \")]),_c('p',[_c('ul',[_c('li',[_vm._v(\"The images were labeled independently by Urban Risk Lab researchers for each task. The researchers used \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/blob/main/Image%20Analysis%20Module/Annotation/Image%20Labeling%20Guide.docx\",\"target\":\"_blank\"}},[_vm._v(\"this guide\")]),_vm._v(\" to inform their decisions.\")]),_c('li',[_vm._v(\"Each image was independently provided a \"),_c('strong',[_vm._v(\"single label for each task\")]),_vm._v(\" by 3 annotators.\")]),_c('li',[_vm._v(\"We enforced independent labeling by hiding the labels given by the other labelers while someone was labeling.\")])])]),_c('p',[_vm._v(\" Since each image was given three labels for a task, we use the plurality, or most frequent label, given to the image as the ground-truth label for that image. We chose this method of ground-truthing to minimize any specific person's contributed bias towards the ground-truth label. If there was not a plurality label, we do not label the image and thus do not put that image in the test set for that task, but did make note of the disagreement for later analysis. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images\")]),_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-2\"},[_c('h2',[_vm._v(\"... a computer cannot generally agree with annotators at a rate that is higher than the rate at which the annotators agree with each other.\\\"\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn2\",\"id\":\"ref2\"}},[_vm._v(\"1\")])])])]),_c('div',{staticClass:\"carousel-text col-12 col-md-6\"},[_c('p',[_vm._v(\" Since the data we have is \"),_c('strong',[_vm._v(\"human-annotated\")]),_vm._v(\" and thus permits subjectivity, we aimed to assess and make transparent \"),_c('strong',[_vm._v(\"the quality of the annotated datasets\")]),_vm._v(\", thus we computed measures of inter-annotator agreement (IAA). \"),_c('br'),_c('br'),_vm._v(\" This IAA analysis enabled us to determine, for each task: \"),_c('ul',[_c('li',[_vm._v(\"How reproducible labeling for the task is\")]),_c('li',[_vm._v(\"If our annotation procedure can be improved:\")]),_c('ul',[_c('li',[_vm._v(\"Refinement of task label definitions\")]),_c('li',[_vm._v(\"Clarifying data points of disagreement between annotators\")]),_c('li',[_vm._v(\"Adding more examples for each class\")])])]),_vm._v(\" This analysis has the advantage of happening before any model development, focusing on \"),_c('strong',[_vm._v(\"improvement of classification task formulation itself rather than building a model which will likely perform poorly on an ill-formed task.\")])])])]),_c('p',[_c('sup',{attrs:{\"id\":\"fn2\"}},[_vm._v(\"1. D. T. Nguyen, K. Al-Mannai, S. R. Joty, H. Sajjad, M. Imran, and P. Mitra, \"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/1608.03902\",\"target\":\"_blank\"}},[_vm._v(\"Rapid classification of crisis-related data on social networks using convolutional neural networks,\")]),_vm._v(\" CoRR, vol. abs/1608.03902, 2016.\"),_c('a',{attrs:{\"href\":\"#ref2\",\"title\":\"Jump back to footnote 2 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images\")])]),_c('div',{staticClass:\"col-12 col-md-6\"},[_vm._v(\" Agreement Measures by Task for Labeled Fukuchiyama Images \"),_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/iaa.png\")}})])]),_c('br'),_c('p',[_c('strong',[_vm._v(\"Fleiss' Kappa\")]),_vm._v(\" [-1, +1] score incorporates the level of agreement attained if the annotators did not look at the data when labeling, i.e. \"),_c('strong',[_vm._v(\"random chance agreement\")]),_vm._v(\", which is an advantage over the complete agreement percentage (\\\"Unanimous Agreement Percentage\\\" pictured above), in which all annotators agree on the same label for an image. \"),_c('ul',[_c('li',[_vm._v(\"By Fleiss' Kappa, we observe smaller improvements over random chance agreement for Damage Severity (0.413), Humanitarian Categories (0.304), and Informativeness (0.313) as compared to Flood Presence (0.829)\")]),_c('ul',[_c('li',[_vm._v(\"This suggests that further investigation should be conducted in refining the label definitions and the annotation guide for clarity by understanding potential causes for disagreement on the tasks with lower Fleiss' Kappa scores, in order to improve agreement and thus the quality of the dataset.\")])]),_c('li',[_vm._v(\"We use the plurality labels found for each task to form the ground-truth Fukuchiyama datasets for each of the tasks which we evaluate the trained CNN models on.\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Annotated Fukuchiyama Image Test Sets\")]),_c('div',{staticClass:\"row align-items-center justify-content-between\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/ds-fc.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/hc-fc.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/in-fc.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/fp-fc.png\")}})])]),_c('br'),_c('p',[_vm._v(\" The ground-truth datasets are formed from the plurality labels found from the annotations given to the Fukuchiyama images. We again observe imbalance in the resulting datasets, albeit to varying degrees. Therefore, we again make use of weighted aggregate metrics for model evaluation, however, for a more granular insight into model performance, we also investigate the per-class performance of each model by precision, recall, and F1 score for each class and visualize the confusion matrix. Lastly, we establish a comparison to a baseline classifier using the Cohen's Kappa score. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Damage Severity\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/damage_severity_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/damage_severity_per_class_metric.png\")}})])]),_c('br'),_c('p',[_vm._v(\" For the damage severity model, we notice that when the model mispredicts the \\\"Little or None\\\" class it predicts \\\"Mild\\\" far more than \\\"Severe\\\". Relatedly, when the damage severity model mispredicts the \\\"Mild\\\" class, it far more often predicts \\\"Little or None\\\" than \\\"Severe\\\". Finally, when the model mispredicts the \\\"Severe\\\" class, it predicts \\\"Mild\\\" more than either \\\"Little or None\\\" or \\\"Severe\\\". \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Humanitarian Categories\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/humanitarian_categories_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/humanitarian_categories_per_class_metric.png\")}})])]),_c('br'),_c('p',[_vm._v(\" When analyzing the per-class performance for each of the models on the Fukuchiyama datasets, we observe that the humanitarian categories model performance varies greatly between the classes for the task. Namely, we observed that the AIDP class has scores of 0 across all metrics. From the training set distributions discussed earlier, we observe that the AIDP class is only 6.12% of the entire training set for the humanitarian categories task. This is the smallest training set class proportion for any of the image classification tasks examined in this work, with the next lowest training set class proportions being those for the RVDE class in the humanitarian categories task and the \\\"Mild\\\" class of damage severity at 14.0% and 14.4%, respectively. This suggests that the imbalance of the humanitarian categories training set impacts the performance of the humanitarian categories severely on the minority classes, especially the AIDP class, the class with the lowest proportion. \")]),_c('br')])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Informativeness\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/informativeness_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/informativeness_per_class_metric.png\")}})])]),_c('br'),_c('p',[_vm._v(\" By nature of the labeled Fukuchiyama crisis image data being almost exclusively related to crisis events or \\\"normal day\\\" photos, the \\\"Not Informative\\\" class is only 69 images as opposed to the 589 \\\"Informative\\\" photos. We note that in the original conception of the task in [1]\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn3\",\"id\":\"ref3\"}},[_vm._v(\"1\")])]),_vm._v(\", the informativeness classifier is intended to be used for filtering noisy tweets which are completely unrelated to crisis events from relevant tweets, however as we report, it is not an adequate classifier for filtering images indicative of crisis impact and those of \\\"normal-day\\\" scenes, because, as we have learned, that is a different task altogether. We observe that the model correctly classifies most of the images labeled \\\"Informative\\\" with a recall score of 0.781. When classifying the images labeled as \\\"Not Informative\\\", the classifier classifies 53.6% of these images incorrectly as \\\"Informative\\\" and 46.4% of these images correctly as \\\"Not Informative\\\". Across all metrics, the model performs reasonably well on the \\\"Informative\\\" class, but significantly worse on the \\\"Not Informative\\\". \")]),_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fn3\"}},[_vm._v(\"1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \"),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2011.08916.pdf\",\"target\":\"_blank\"}},[_vm._v(\"Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response\")]),_vm._v(\", In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.\"),_c('a',{attrs:{\"href\":\"#ref3\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Flood Presence\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/flood_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/flood_presence_per_class_metric.png\")}})])]),_c('br'),_c('p',[_vm._v(\" Unlike the models for damage severity, humanitarian categories, and informativeness, the flood presence model performs consistently well (less variation and higher values) by all metrics across all classes in the task, attaining metric scores at or above 0.793 across all metrics for both classes. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Aggregate Metrics\")])]),_c('div',{staticClass:\"col-12 col-md-8\"},[_vm._v(\" Performance of Image Classification models on task-respective Fukuchiyama Data \"),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"agg-metrics\",\"src\":require(\"../../../../public/assets/agg-metrics-fc.png\")}})])]),_c('br'),_c('p',[_vm._v(\" By Cohen's Kappa score, we see that the damage severity and informativeness tasks provide a relatively small improvement over the random classifier for their corresponding datasets as compared to the humanitarian categories model and far more so for the flood presence model, which provides the most improvement over the random classifier for its dataset. \")]),_c('p',[_vm._v(\" We observe across all of the weighted metrics for all tasks, the performances of the models on the Fukuchiyama data are lower than the performances achieved on the consolidated crisis image test splits and flood presence test set reported earlier. This is most apparent for the damage severity model, which achieves a weighted F1 score of 43.2% on the Fukuchiyama data. Similar to the performance of the flood presence model on flood presence test split, the flood presence model performs relatively well on the Fukuchiyama flood presence task images achieving a weighted F1 of 82.5%. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Discussion\")]),_c('br'),_c('h6',[_vm._v(\"Performance on Fukuchiyama Data\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Investigating the \"),_c('strong',[_vm._v(\"per-class performance\")]),_vm._v(\" allowed us to see which classes may be suffering from \"),_c('strong',[_vm._v(\"class imbalance issues\")]),_vm._v(\" (e.g. Affect, Injured, or Dead People) as well as the common mistakes a model makes when predicting, such as when the damage severity model commonly mispredicts \\\"mild\\\" for actual \\\"little-or-none\\\" images. \")]),_c('li',[_vm._v(\"There are likely multiple reasons why the model performance is comparatively lower for the Fukuchiyama data as compared to the test splits for the consolidated crisis image datasets and the flood presence dataset: \"),_c('ul',[_c('li',[_vm._v(\"May be in part due to \"),_c('strong',[_vm._v(\"concept drift\")]),_vm._v(\" between the data the models were \"),_c('strong',[_vm._v(\"trained on\")]),_vm._v(\" and the Fukuchiyama data which the models were \"),_c('strong',[_vm._v(\"evaluated on\")])]),_c('li',[_vm._v(\"Labeled Fukuchiyama data may have been of \"),_c('strong',[_vm._v(\"poorer data quality\")]),_vm._v(\" as suggested from the relatively \"),_c('strong',[_vm._v(\"low Fleiss Kappa coefficients\")]),_vm._v(\" for the damage severity, humanitarian categories, and informativeness tasks\")])])]),_c('li',[_vm._v(\" The \"),_c('strong',[_vm._v(\"low Fleiss' Kappa scores\")]),_vm._v(\" for the damage severity, humanitarian categories, and informativeness tasks can \"),_c('strong',[_vm._v(\"potentially be improved\")]),_vm._v(\" by converting the abstract \"),_c('strong',[_vm._v(\"definitions\")]),_vm._v(\" of different classes \"),_c('strong',[_vm._v(\"into checklists\")]),_vm._v(\", understanding common \"),_c('strong',[_vm._v(\"annotator disagreements\")]),_vm._v(\", and adding \"),_c('strong',[_vm._v(\"more clarification/examples\")]),_vm._v(\" where necessary in the annotation procedure. \")]),_c('li',[_vm._v(\" We consider the performance of the Flood Presence model across the datasets to be robust, which we attribute to the task being binary (as opposed to multiclass) and to the task having classes which yield higher agreement between annotators. \")])])]),_c('h6',[_vm._v(\"Importance of Selecting a Metric based on Crisis Management Priorities and the Nature of the Data\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\"While the Flood Presence model may be performant, we aimed to understand if this task as well as the other tasks have informative utility to crisis managers during flood crisis.\")]),_c('li',[_vm._v(\"Although \"),_c('strong',[_vm._v(\"weighted F1\")]),_vm._v(\" is a popular metric reported in the literature, it is \"),_c('strong',[_vm._v(\"biased towards the model's performance on the majority classes.\")])]),_c('li',[_vm._v(\"Cohen's Kappa provides the level of accuracy achieved that is above the \"),_c('strong',[_vm._v(\"random classifier baseline.\")])]),_c('li',[_vm._v(\"Cohens Kappa can be a more appropriate metric to use in the case of multiclass classification tasks when the dataset is imbalanced, but there is likely an even more appropriate metric depending on the task and the priorities of crisis managers.\")]),_c('li',[_vm._v(\"We have determined that selecting a model performance metric should be a process which both considers the \"),_c('strong',[_vm._v(\"nature of the data\")]),_vm._v(\" (i.e. class imbalance) & uses \"),_c('strong',[_vm._v(\"insights from crisis managers\")]),_vm._v(\".\")])])]),_c('h6',[_vm._v(\"Understanding the Informative Utility of the Image Analysis Module\")]),_c('p',[_vm._v(\" To broaden the discussion of the efficacy the image analysis module has in mitigating information overload and enhancing situational awareness, we also qualitatively examine the informative utility the image models have in assisting crisis managers during a flood crisis event. We held image annotation workshops with various crisis managers and aimed to understand what type of information they seek to gain from a crowdsourced image during a flood crisis event as well as what their priorities are. We iterate on our methodology by using insights we gained from the crisis managers into the Text Analysis Module, exhibiting the principle of iterative development ou framework intends to promote. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\" Image Annotation Workshops with EOC & Methodology Iteration \")]),_c('p',[_vm._v(\" The Urban Risk Lab\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn4\",\"id\":\"ref4\"}},[_vm._v(\"1\")])]),_vm._v(\" held image annotation workshops in December 2021 with the following crisis experts in Fukuchiyama: \"),_c('ul',[_c('li',[_vm._v(\"Director of the Regional Disaster Management Research Center, Fukuchiyama Public University and Former Crisis Management Supervisor of Fukuchiyama City\")]),_c('li',[_vm._v(\"3 Crisis Managers at an EOC in Fukuchiyama\")]),_c('li',[_vm._v(\"5 Associates (including Fire Department Director & 1 Firefighter) of the Fire Department in Fukuchiyama\")])]),_vm._v(\" In January 2022, the same workshop was held with a former Deputy Administrator of the Federal Emergency Management Agency (FEMA) in the US \")]),_c('br'),_c('p',[_vm._v(\" Crisis experts were \"),_c('strong',[_vm._v(\"presented 25 images\")]),_vm._v(\" from past FC flood crises. The images represented various types of crisis impact.\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn5\",\"id\":\"ref5\"}},[_vm._v(\"2\")])]),_vm._v(\" A subset of the images were selected because they had \"),_c('strong',[_vm._v(\"disagreement between annotators\")]),_vm._v(\" & were given \"),_c('strong',[_vm._v(\"a wrong prediction by the CNN model\")]),_vm._v(\" developed for a task. \")]),_c('br'),_c('p',[_vm._v(\" The \"),_c('strong',[_vm._v(\"crisis experts were tasked with labeling images with a variety of labels\")]),_vm._v(\" & \"),_c('strong',[_vm._v(\"identifying insights\")]),_vm._v(\" from an image that are \"),_c('strong',[_vm._v(\"useful for decision making and response\")]),_vm._v(\" during crisis events. \")]),_c('br'),_c('p',[_c('sup',{attrs:{\"id\":\"fn4\"}},[_vm._v(\" 1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who conducted the qualitative focus-group research in the form of interviews, workshops, and general interfacing with our partners in Fukuchiyama and in the US, provided the translations of the results from Japanese to English to enable the analysis of those results as it relates to the work presented in this thesis, and assisted in the synthesis of the main findings from the observations and discourse that occurred during the image annotation workshops. We note that Saeko Baird has formal training in Human-Computer Interaction (HCI) research.\"),_c('a',{attrs:{\"href\":\"#ref4\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])]),_c('br'),_c('sup',{attrs:{\"id\":\"fn5\"}},[_vm._v(\" 2. \"),_c('strong',[_vm._v(\"Crisis Impact Types:\")]),_vm._v(\" river flood, rock-fall, landslide, residential housing damage, blocked roads, agricultural land damage, submerged residential areas, and damaged infrastructure.\"),_c('a',{attrs:{\"href\":\"#ref5\",\"title\":\"Jump back to footnote 2 in the text.\"}},[_vm._v(\"\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Annotation Workshop & Methodology Iteration - Aims\")]),_c('p',[_vm._v(\" With these workshops, we aimed to: \"),_c('ul',[_c('li',[_vm._v(\"Understand \"),_c('strong',[_vm._v(\"cross-contextual insights\")]),_vm._v(\", i.e. information needs and decision-making priorities common to both Fukuchiyama & US crisis management\")]),_c('li',[_c('strong',[_vm._v(\"Compare our devised image analysis ML methodology\")]),_vm._v(\" for automatic insights to the insights gained from \"),_c('strong',[_vm._v(\"manual assessment\")]),_vm._v(\" of crisis images \"),_c('strong',[_vm._v(\"by crisis experts.\")])]),_c('li',[_vm._v(\"Use results to \"),_c('strong',[_vm._v(\"iterate on design of ML methodology\")]),_vm._v(\" to better embed information needs and priorities of crisis managers.\")])])]),_c('p',[_vm._v(\" The following questions were posed for each of the images to actively engage crisis managers in the annotation process. We note, however, that the conversations often expanded beyond these questions, getting to the core of what their main insights would be through manual assessment of the image data and what their main decision priorities would be during a flood crisis event as it related to analyzing the image data: \")]),_c('br'),_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-8\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/workshop-preface-questions.png\")}})])]),_c('br')])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Annotation Workshops with Crisis Experts - Results\")]),_c('p',[_vm._v(\" We report qualitative summaries describing the insights derived from manual assessment by domain experts of the crisis report images and their expressed information needs. We used these summaries to compare how the insights the Image Analysis Module aims to automatically provide to crisis managers compares to the insights derived from manual assessment by crisis experts and their expressed information needs. This comparison allowed us to qualitatively assess the utility of the Image Analysis Module in attaining situational awareness as it was devised in this work, and helped us in determining ways to iterate on the module in the future to better accommodate the information needs of the crisis managers during a crisis event through new prediction tasks which align with their information needs. \")]),_c('h6',[_vm._v(\"Cross-contextual Insights\")]),_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('p',[_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"Potential of Human Casualties in Crisis Imagery\")])]),_c('ul',[_c('li',[_vm._v(\"Possibility of Human Casualty is \"),_c('strong',[_vm._v(\"Top Priority\")])]),_c('li',[_c('strong',[_vm._v(\"Identified physical markers\")]),_vm._v(\" suggesting \"),_c('strong',[_vm._v(\"potential for human casualty:\")]),_c('ul',[_c('li',[_vm._v(\"Submerged Vehicles\")]),_c('li',[_vm._v(\"Collapsed Buildings\")]),_c('li',[_vm._v(\"Housing in Close Proximity to Rockfall or Landslide\")])])]),_c('li',[_vm._v(\" Not investigating when there is actually human casualty (False Negative) \"),_c('strong',[_vm._v(\"is more costly\")]),_vm._v(\" than investigating when there is not actually human casualty (False Positive) \")])])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Presence of People in Crisis Imagery\")])]),_c('ul',[_c('li',[_c('strong',[_vm._v(\"People in crisis imagery\")]),_vm._v(\" is important and should be \"),_c('strong',[_vm._v(\"assessed with high priority\")])]),_c('li',[_c('strong',[_vm._v(\"Insights should be specific,\")]),_vm._v(\" e.g. people laying down vs. people standing casually/walking about - can indicate crisis impact severity to personnel \")])])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Insights of Broader Impact derived from Physical Markers in Images:\")])]),_c('ul',[_c('li',[_vm._v(\" Experts \"),_c('strong',[_vm._v(\"identified physical markers\")]),_vm._v(\" which suggest \"),_c('strong',[_vm._v(\"potential broader impact to area\")]),_vm._v(\", including: \"),_c('ul',[_c('li',[_vm._v(\"Muddy Water  potential nearby landslide\")]),_c('li',[_vm._v(\"Fallen Power Pole  potential power outage\")]),_c('li',[_vm._v(\"Road Passability  possibility of emergency vehicle use & isolated residential areas\")])])])])])])])])]),_c('h6',[_vm._v(\"Contextual Insights\")]),_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('p',[_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"National Standards in Japan for Assessing Impact Severity\")])]),_c('ul',[_c('li',[_vm._v(\" Standards for Flood Impact Severity on Housing used in Fukuchiyama: \"),_c('ul',[_c('li',[_vm._v(\"Water reaches up to the first-floor ceiling  Severe Flooding/Destruction\")]),_c('li',[_vm._v(\"Water reaches 1m above first-floor level  Partial Flooding/Destruction\")]),_c('li',[_vm._v(\"Water reaches below flood level  Minor Flooding/Destruction\")])])])])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Insights Derived from both the Image and Contextual-Knowledge:\")])]),_c('ul',[_c('li',[_c('strong',[_vm._v(\"FC crisis experts used contextual knowledge\")]),_vm._v(\" of the area where the image was taken in gaining insights \"),_c('ul',[_c('li',[_vm._v(\"E.g. Image showing flooding in an area that doesnt typically flood causes more concern for that area\")])])])])])])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Annotation Workshop & Methodology Iteration - Discussion\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" From crisis expert insights and feedback, we have determined that the tasks as presented in this work have classes with interpretations that are either \"),_c('strong',[_vm._v(\"too vague and subjective (damage severity, humanitarian categories, and informativeness)\")]),_vm._v(\" or \"),_c('strong',[_vm._v(\"too simplistic (flood presence)\")]),_vm._v(\" to be useful for them in gaining situational awareness about an unfolding crisis event. \")]),_c('li',[_vm._v(\" The humanitarian categories task has the \\\"Rescue, Volunteering, or Donation Effort\\\" class, which has insights for the \"),_c('strong',[_vm._v(\"recovery phase of a crisis event rather than the emergency phase. Since our ML methodology aims to assist crisis managers during the emergency phase of a crisis event, such classes should be revised or replaced with classes which have insights directly for the emergency phase.\")])]),_c('li',[_vm._v(\" Although the \"),_c('strong',[_vm._v(\"flood presence task\")]),_vm._v(\" has classes with interpretations which are too simple for attaining situational awareness, we note that the \"),_c('strong',[_vm._v(\"relatively high performance, high consistency between independent annotators, and clarity\")]),_vm._v(\" in the interpretation of the classes associated with the flood presence task \"),_c('strong',[_vm._v(\"sets precedent for task creation and model performance for the future tasks\")]),_vm._v(\" developed from the insights and feedback received from the workshops discussed in this work and future workshops. \")])])]),_c('p',[_vm._v(\" The insights and feedback provided by the domain experts enabled us to determine how the Image Analysis Module we have developed in this work is limited in helping to gain insights about the unfolding crisis event. \"),_c('strong',[_vm._v(\"Where our ML methodology falls short in meeting their information needs, their feedback will assist in developing new classification tasks which would be informative enough to assist them during a crisis event and clear enough to yield more consistent labels between annotators, ensuring better quality data to train and evaluate models.\")]),_vm._v(\" The development of new image prediction tasks and associated models will be conducted in a future work. However, we were able to apply some of these insights to inform the ML methodology of the \"),_c('a',{attrs:{\"href\":\"#/projects/ml-for-crowdsourced-crisis-data/text-analysis-module\"}},[_vm._v(\"Text Analysis Module\")]),_vm._v(\". \")])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-8\">  \n            <h3>Image Analysis Module</h3>\n        </div>\n        <div class=\"col-12 col-md-10\">\n            <div id=\"ImageAnalysisCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"2\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"3\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"4\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"5\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"6\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"7\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"8\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"9\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"10\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"11\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"12\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"13\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"14\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"15\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"16\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"17\"></li>\n                    <li data-target=\"#ImageAnalysisCarousel\" data-slide-to=\"18\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item cc-carousel-item active\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12 col-md-6 pt-3 pb-5\">\n                                <img id=\"image-analysis-module\" class=\"img-fluid\" src=\"../../../../public/assets/image-analysis-module.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"carousel-text col-12 col-md-6\">\n                                <h5>Image Analysis Methodology Overview</h5>\n                                <p> \n                                    The goal of the Image Analysis Module is to utilize pretrained Convolutional Neural Network models to yield\n                                    efficient and accurate predictions from image data in crowdsourced crisis reports such as those on RiskMap & Twitter. The\n                                    classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and\n                                    Flood Presence (FP) form a diverse suite of labels for assisting crisis managers in automatically classifying the data. In a fraction of a second, the model\n                                    predictions made for these tasks provide a series of categorizations for an individual\n                                    report. We leverage state-of-the-art CNNs, which strike a necessary balance between\n                                    model complexity, memory and storage constraints, and model performance to provide\n                                    these predictions.\n                                    <br>\n                                    <br>\n                                    To achieve this aim, we use large, labeled, open-source datasets for training and\n                                    evaluating the models. We formulate a new crisis image classification task for detecting flood presence in an image and construct\n                                    a new dataset altogether using flood images from various open-source datasets, which contained\n                                    flood-adjacent labels. We were given unlabeled flood crisis images from crisis managers in Fukuchiyama (FC), Japan. We devise an annotation procedure to label this data and analyze the results of human-annotations on the images. \n                                    Since this data was not used to train or develop the models, we aimed to see if our trained models could accurately classify unseen images from the FC context.\n                                    We held various image annotation workshops with crisis managers to identify the limitations in our\n                                    approach and understand how we could iterate on the design of our ML methodology. This evaluation procedure thus enabled us to evaluate the use of image classification models in assisting crisis managers\n                                    using quantitative metrics as well as qualitatively through the feedback we got through the image annotation workshops. \n                                    This evaluation directly influenced our approach for devising the Text Analysis Module.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Datasets & Train/Dev/Test Splits for Model Development & Evaluation</h5>\n                            <div class=\"row justify-content-around\">\n                                <div class=\"carousel-text col-12 col-md-9\">\n                                    <p>\n                                        Since we used CNN models, specifically the EfficientNet-B1 architecture pretrained on ImageNet, we wanted to make use of large open-source Twitter crisis image datasets\n                                        for finetuning the models to perform the classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP).\n                                    </p>\n                                    <h6>Open-source Consolidated Crisis Image Datasets</h6>\n                                    <p>\n                                        For the DS, HC, and IN tasks, we made use of the open-source train/dev/test splits made available at \n                                        <a href=\"https://crisisnlp.qcri.org/crisis-image-datasets-asonam20\" target=\"_blank\">CrisisNLP</a>. We train the models which perform these tasks using\n                                        the train and dev splits. For evaluation, we make use of the respective test splits for each task.\n                                    </p>\n                                    <h6>Flood Presence Task Creation and Dataset Formation</h6>\n                                    <p>\n                                        In this work we focus on flood-crisis events, thus we used various open-source image datasets which have flood-adjacent labels\n                                        and map them to the binary labels of \"Flood\"/\"Not Flood\". Using the resulting dataset, we create randomized, non-overlapping Train/Dev/Test splits.\n                                        Similar to the above, we use the train & dev splits to develop the FP model, and the test split to evaluate the model.\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12 col-md-8\">\n                                    <h5>Flood Presence Dataset Composition</h5>\n                                    Composition of the Flood Presence dataset from the original datasets and the number of images for each label.\n                                    <table id=\"fp-table\">\n                                        <tr>\n                                            <th><strong>Dataset</strong></th>\n                                            <th><strong>Flood</strong></th>\n                                            <th><strong>Not Flood</strong></th>\n                                            <th><strong>Total</strong></th>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Consolidated Disaster Types</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/2011.08916\" target=\"_blank\">Alam et al. 2020</a>)\n                                                </i>\n                                            </td>\n                                            <td>3201</td>\n                                            <td>14310</td>\n                                            <td>17511</td>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Central European Floods 2013</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/1908.03361\" target=\"_blank\">Barz et al. 2018</a>)\n                                                </i>\n                                            </td>\n                                            <td>3151</td>\n                                            <td>559</td>\n                                            <td>3710</td>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Harz Region Floods 2017</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/2011.05756\" target=\"_blank\">Barz et al. 2020</a>)\n                                                </i>\n                                            </td>\n                                            <td>264</td>\n                                            <td>405</td>\n                                            <td>669</td>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Rhine River Floods 2018</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/2011.05756\" target=\"_blank\">Barz et al. 2020</a>)\n                                                </i>\n                                            </td>\n                                            <td>730</td>\n                                            <td>1007</td>\n                                            <td>1737</td>\n                                        </tr>\n                                        <tr>\n                                            <td>Total</td>\n                                            <td>7346</td>\n                                            <td>16281</td>\n                                            <td>23627</td>\n                                        </tr>\n                                    </table>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Training Sets</h5>\n                            <div class=\"row align-items-center justify-content-between\">\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/ds-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/hc-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/in-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/fp-train.png\" class=\"img-fluid\"/>\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                As part of the framework we developed, we investigated the class imbalance for each of the\n                                image classification training sets. We observed significant imbalance in the label distributions for the Damage Severity \n                                and the Humanitarian Categories tasks. We note that this imbalance could be problematic for the performance of the models on\n                                the minority classes for those tasks, e.g. the \"Mild\", \"Rescue, Volunteering, or Donation Effort\", and \"Affected, Injured, or Dead People\" classes.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12\">\n                                    <h5>Model Evaluation on Test Splits & Flood Presence Benchmark Performance</h5>\n                                </div>\n                                <div class=\"col-12 col-md-8\">\n                                    <p>\n                                        Performance of image classification models on their respective test splits. [1] as referred to in the table can be found in the footnote below.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>\n                                    </p>\n                                    <br>\n                                    <img id=\"test-set-eval\" src=\"../../../../public/assets/test-set-eval.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                    Similar to the authors in [1]<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>, we report overall model performance as\n                                    weighted metrics in order to take into account class imbalance present in the test splits.\n                                    From the table above, we observe that the models we finetuned achieve a weighted F1 score within a 1% difference\n                                    compared to the model performances reported in [1]. <strong>We report a benchmark performance of 92.1% weighted F1 for the Flood Presence (FP) task.</strong>\n                                    We postulate the comparatively higher performance of FP task to be due to the task being binary as well as being the most clear and objective task, thus being a comparatively\n                                    simipler task for the model to learn. We explore this more through interannotator agreement analysis discussed in the next slides.\n                            </p>\n                            <hr>\n                            <p>\n                                <sup id=\"fn1\">1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \n                                    <a href=\"https://arxiv.org/pdf/2011.08916.pdf\" target=\"_blank\">Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response</a>, \n                                    In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                </sup>\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Annotating Fukuchiyama Crisis Images</h5>\n                            <p>\n                                To evaluate our developed models and framework in a context which is susceptible to flood events, we cooperated with crisis managers in \n                                Fukuchiyama, Japan to attain <strong>658 images</strong> from previous flood events as well as non-crisis normal days in Fukuchiyama, which were collected on the ground,\n                                similar to RiskMap & Twitter crisis images. This evaluation enabled us to understand how well training the models on the large, consolidated crisis datasets, which cover a diverse set\n                                geographical locations and a multitude of crisis events, would perform on the unseen\n                                data from flood events in Fukuchiyama. To form evaluation/test sets from this data, we needed to label the images for each of the four image classification tasks we've discussed.\n                            </p>\n                            <p>\n                                <ul>\n                                    <li>The images were labeled independently by Urban Risk Lab researchers for each task. The researchers used <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/blob/main/Image%20Analysis%20Module/Annotation/Image%20Labeling%20Guide.docx\" target=\"_blank\">this guide</a> to inform their decisions.</li>\n                                    <li>Each image was independently provided a <strong>single label for each task</strong> by 3 annotators.</li>\n                                    <li>We enforced independent labeling by hiding the labels given by the other labelers while someone was labeling.</li>\n                                </ul>\n                            </p>\n                            <p>\n                                Since each image was given three labels for a task, we use the plurality, or most frequent label, given to the image as the ground-truth label for that image. We chose this method of ground-truthing to minimize any specific person's contributed bias towards the ground-truth label.\n                                If there was not a plurality label, we do not label the image and thus do not put that image in the test set for that task, but did make note of the disagreement for later analysis.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images</h5>\n                        <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12 col-md-6 pt-3 pb-2\">\n                                    <h2>... a computer cannot generally agree with annotators at a rate that is higher than the rate at which the annotators agree with each other.\"<sup><a href=\"#fn2\" id=\"ref2\">1</a></sup></h2>\n                                </div>\n                                <div class=\"carousel-text col-12 col-md-6\">\n                                    <p> \n                                        Since the data we have is <strong>human-annotated</strong> and thus permits subjectivity, we aimed to assess and make transparent <strong>the quality of the annotated datasets</strong>, thus we computed measures of inter-annotator agreement (IAA).\n                                        <br>\n                                        <br>\n                                        This IAA analysis enabled us to determine, for each task:\n                                        <ul>\n                                            <li>How reproducible labeling for the task is</li>\n                                            <li>If our annotation procedure can be improved:</li>\n                                                <ul>\n                                                    <li>Refinement of task label definitions</li>\n                                                    <li>Clarifying data points of disagreement between annotators</li>\n                                                    <li>Adding more examples for each class</li>\n                                                </ul>\n                                        </ul>\n                                        This analysis has the advantage of happening before any model development, focusing on <strong>improvement of classification task formulation itself rather than building a model which will likely perform poorly on an ill-formed task.</strong>\n                                    </p>\n                                </div>\n                        </div>\n                        <p>\n                            <sup id=\"fn2\">1. D. T. Nguyen, K. Al-Mannai, S. R. Joty, H. Sajjad, M. Imran, and P. Mitra, <a href=\"https://arxiv.org/abs/1608.03902\" target=\"_blank\">Rapid classification of crisis-related data on social networks using convolutional neural networks,</a> CoRR, vol. abs/1608.03902, 2016.<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12\">\n                                    <h5>Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images</h5>\n                                </div>\n                                <div class=\"col-12 col-md-6\">\n                                    Agreement Measures by Task for Labeled Fukuchiyama Images\n                                    <img src=\"../../../../public/assets/iaa.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                <strong>Fleiss' Kappa</strong> [-1, +1] score incorporates the level of agreement attained if the annotators did not look at the data when labeling, i.e. \n                                <strong>random chance agreement</strong>, which is an advantage over the complete agreement percentage (\"Unanimous Agreement Percentage\" pictured above), in which all annotators\n                                agree on the same label for an image.\n                                <ul>\n                                    <li>By Fleiss' Kappa, we observe smaller improvements over random chance agreement for Damage Severity (0.413), Humanitarian Categories (0.304), and Informativeness (0.313) as compared to Flood Presence (0.829)</li>\n                                        <ul>\n                                            <li>This suggests that further investigation should be conducted in refining the label definitions and the annotation guide for clarity by understanding potential causes for disagreement on the tasks with lower Fleiss' Kappa scores,\n                                                in order to improve agreement and thus the quality of the dataset.</li>\n                                        </ul>\n                                    <li>We use the plurality labels found for each task to form the ground-truth Fukuchiyama datasets for each of the tasks which we evaluate the trained CNN models on.</li>\n                                </ul>\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Annotated Fukuchiyama Image Test Sets</h5>\n                            <div class=\"row align-items-center justify-content-between\">\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/ds-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/hc-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/in-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/fp-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                The ground-truth datasets are formed from the plurality labels found from the annotations given to the Fukuchiyama images.\n\n                                We again observe imbalance in the resulting datasets, albeit to varying degrees. Therefore, we again make use of weighted aggregate metrics for model evaluation,\n                                however, for a more granular insight into model performance, we also investigate the per-class performance of each model by precision, recall, \n                                and F1 score for each class and visualize the confusion matrix. Lastly, we establish a comparison to a baseline classifier using the Cohen's Kappa score.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5>Damage Severity</h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                    <img src=\"../../../../public/assets/damage_severity_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                    <img src=\"../../../../public/assets/damage_severity_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                For the damage severity model, we notice that when the model\n                                mispredicts the \"Little or None\" class it predicts \"Mild\" far more than \"Severe\".\n                                Relatedly, when the damage severity model mispredicts the \"Mild\" class, it far more\n                                often predicts \"Little or None\" than \"Severe\". Finally, when the model mispredicts\n                                the \"Severe\" class, it predicts \"Mild\" more than either \"Little or None\" or \"Severe\".\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5>Humanitarian Categories</h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                    <img src=\"../../../../public/assets/humanitarian_categories_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                    <img src=\"../../../../public/assets/humanitarian_categories_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p> \n                                When analyzing the per-class performance for each of the models on the Fukuchiyama datasets, \n                                we observe that the humanitarian categories model performance varies greatly between the classes for the task. \n                                Namely, we observed that the AIDP class has scores of 0 across all metrics. From the training set distributions discussed earlier, \n                                we observe that the AIDP class is only 6.12% of the entire training set for the humanitarian categories task. \n                                This is the smallest training set class proportion for any of the image classification tasks examined in this work, \n                                with the next lowest training set class proportions being those for the RVDE class in the humanitarian categories task \n                                and the \"Mild\" class of damage severity at 14.0% and 14.4%, respectively. \n                                This suggests that the imbalance of the humanitarian categories training set impacts the performance of the humanitarian \n                                categories severely on the minority classes, especially the AIDP class, the class with the lowest proportion.\n                            </p>\n                            <br>\n                        </div>\n                    </div>                 \n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5>Informativeness</h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                    <img src=\"../../../../public/assets/informativeness_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                    <img src=\"../../../../public/assets/informativeness_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                By nature of the labeled Fukuchiyama crisis image data being almost\n                                exclusively related to crisis events or \"normal day\" photos, the \"Not Informative\"\n                                class is only 69 images as opposed to the 589 \"Informative\" photos. We note that\n                                in the original conception of the task in [1]<sup><a href=\"#fn3\" id=\"ref3\">1</a></sup>, the informativeness classifier is intended\n                                to be used for filtering noisy tweets which are completely unrelated to crisis events\n                                from relevant tweets, however as we report, it is not an adequate classifier for filtering\n                                images indicative of crisis impact and those of \"normal-day\" scenes, because, as we\n                                have learned, that is a different task altogether. We observe that the model correctly\n                                classifies most of the images labeled \"Informative\" with a recall score of 0.781. When\n                                classifying the images labeled as \"Not Informative\", the classifier classifies 53.6% of\n                                these images incorrectly as \"Informative\" and 46.4% of these images correctly as\n                                \"Not Informative\". Across all metrics, the model performs reasonably well on the\n                                \"Informative\" class, but significantly worse on the \"Not Informative\".\n                            </p>\n                            <hr>\n                            <p>\n                                <sup id=\"fn3\">1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \n                                    <a href=\"https://arxiv.org/pdf/2011.08916.pdf\" target=\"_blank\">Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response</a>, \n                                    In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.<a href=\"#ref3\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                </sup>\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5>Flood Presence</h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                    <img src=\"../../../../public/assets/flood_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                    <img src=\"../../../../public/assets/flood_presence_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p>\n                                Unlike the models for damage severity, humanitarian categories, and\n                                informativeness, the flood presence model performs consistently well (less variation\n                                and higher values) by all metrics across all classes in the task, attaining metric scores at or\n                                above 0.793 across all metrics for both classes.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12\">\n                                    <h5>Model Evaluation on Fukuchiyama Data - Aggregate Metrics</h5>\n                                </div>\n                                <div class=\"col-12 col-md-8\">\n                                    Performance of Image Classification models on task-respective Fukuchiyama Data\n                                    <img id=\"agg-metrics\" src=\"../../../../public/assets/agg-metrics-fc.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                            <p> \n                                By Cohen's Kappa score, we see that the damage severity and informativeness tasks provide \n                                a relatively small improvement over the random classifier for their corresponding datasets \n                                as compared to the humanitarian categories model and far more so for the flood presence model, \n                                which provides the most improvement over the random classifier for its dataset.\n                            </p>\n                            <p>\n                                We observe across all of the weighted metrics for all tasks, the performances of the models \n                                on the Fukuchiyama data are lower than the performances achieved on the consolidated crisis image \n                                test splits and flood presence test set reported earlier. This is most apparent for the damage severity model, \n                                which achieves a weighted F1 score of 43.2% on the Fukuchiyama data. Similar to the\n                                performance of the flood presence model on flood presence test split, the flood presence model performs \n                                relatively well on the Fukuchiyama flood presence task images achieving a weighted F1 of 82.5%.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Model Evaluation on Fukuchiyama Data - Discussion</h5>\n                            <br>\n                            <h6>Performance on Fukuchiyama Data</h6>\n                            <p>\n                                <ul>\n                                    <li>\n                                        Investigating the <strong>per-class performance</strong> allowed us to see which classes may be suffering from <strong>class imbalance issues</strong> \n                                        (e.g. Affect, Injured, or Dead People) as well as the common mistakes a model makes when predicting, such as when the damage severity model \n                                        commonly mispredicts \"mild\" for actual \"little-or-none\" images.\n                                    </li>\n                                    <li>There are likely multiple reasons why the model performance is comparatively lower\n                                        for the Fukuchiyama data as compared to the test splits for the consolidated crisis image datasets and the flood presence dataset:\n                                        <ul>\n                                            <li>May be in part due to <strong>concept drift</strong> between the data the models were <strong>trained on</strong> and the Fukuchiyama data which the models were <strong>evaluated on</strong></li>\n                                            <li>Labeled Fukuchiyama data may have been of <strong>poorer data quality</strong> as suggested from the relatively <strong>low Fleiss Kappa coefficients</strong> for the damage\n                                        severity, humanitarian categories, and informativeness tasks</li>\n                                        </ul>\n                                    </li>\n                                    <li>\n                                        The <strong>low Fleiss' Kappa scores</strong> for the damage severity, humanitarian categories, \n                                        and informativeness tasks can <strong>potentially be improved</strong> by converting the abstract <strong>definitions</strong> \n                                        of different classes <strong>into checklists</strong>, understanding common <strong>annotator disagreements</strong>, \n                                        and adding <strong>more clarification/examples</strong> where necessary in the annotation procedure.\n                                    </li>\n                                    <li>\n                                        We consider the performance of the Flood Presence model across the datasets to be robust, \n                                        which we attribute to the task being binary (as opposed to multiclass) and to the task having classes which yield higher agreement between annotators.\n                                    </li>\n                                </ul>\n                            </p>\n                            <h6>Importance of Selecting a Metric based on Crisis Management Priorities and the Nature of the Data</h6>\n                            <p>\n                                <ul>\n                                    \n                                    <li>While the Flood Presence model may be performant, we aimed to understand if this task as well as the other tasks have informative utility to crisis managers during flood crisis.</li>\n                                    <li>Although <strong>weighted F1</strong> is a popular metric reported in the literature, it is <strong>biased towards the model's performance on the majority classes.</strong></li>\n                                    <li>Cohen's Kappa provides the level of accuracy achieved that is above the <strong>random classifier baseline.</strong></li>\n                                    <li>Cohens Kappa can be a more appropriate metric to use in the case of multiclass classification tasks when the dataset is imbalanced, \n                                    but there is likely an even more appropriate metric depending on the task and the priorities of crisis managers.</li>\n                                    <li>We have determined that selecting a model performance metric should be a process which both considers the <strong>nature of the data</strong> (i.e. class imbalance) & uses <strong>insights from crisis managers</strong>.</li>\n                                </ul>\n                            </p>\n                            <h6>Understanding the Informative Utility of the Image Analysis Module</h6>\n                            <p>\n                                To broaden the discussion of the efficacy the image analysis module has in mitigating information overload and enhancing situational awareness, we also qualitatively\n                                examine the informative utility the image models have in assisting crisis managers during a flood crisis event. We held image annotation workshops with various crisis managers and aimed to\n                                understand what type of information they seek to gain from a crowdsourced image during a flood crisis event as well as what their priorities are. We iterate on our methodology\n                                by using insights we gained from the crisis managers into the Text Analysis Module, exhibiting the principle of iterative development ou framework intends to promote.\n                            </p>\n                            \n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5> Image Annotation Workshops with EOC & Methodology Iteration </h5>\n                            <p>\n                                The Urban Risk Lab<sup><a href=\"#fn4\" id=\"ref4\">1</a></sup> held image annotation workshops in December 2021 with the following crisis experts in Fukuchiyama:\n                                <ul>\n                                    <li>Director of the Regional Disaster Management Research Center, Fukuchiyama Public University  and Former Crisis Management Supervisor of Fukuchiyama City</li>\n                                    <li>3 Crisis Managers at an EOC in Fukuchiyama</li>\n                                    <li>5 Associates (including Fire Department Director & 1 Firefighter) of the Fire Department in Fukuchiyama</li>\n                                </ul>\n                                In January 2022, the same workshop was held with a former Deputy Administrator of the Federal Emergency Management Agency (FEMA) in the US\n                            </p>\n                            <br>\n                            <p>\n                                Crisis experts were <strong>presented 25 images</strong> from past FC flood crises. The images represented various types of crisis impact.<sup><a href=\"#fn5\" id=\"ref5\">2</a></sup> A subset of the images were selected because they had <strong>disagreement between annotators</strong> & \n                                were given <strong>a wrong prediction by the CNN model</strong> developed for a task.\n                            </p>\n                            <br>\n                            <p>\n                                The <strong>crisis experts were tasked with labeling images with a variety of labels</strong> & <strong>identifying insights</strong> from an image that are <strong>useful for decision making and response</strong> during crisis events.\n                            </p>\n                            <br>\n                            <p>\n                                <sup id=\"fn4\">\n                                        1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who conducted the qualitative focus-group research in the form of interviews, workshops, and general interfacing with our partners\n                                        in Fukuchiyama and in the US, provided the translations of the results from Japanese to\n                                        English to enable the analysis of those results as it relates to the work presented in this thesis, and assisted in the synthesis of the main\n                                        findings from the observations and discourse that occurred during the image annotation workshops. We\n                                        note that Saeko Baird has formal training in Human-Computer Interaction (HCI) research.<a href=\"#ref4\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                </sup>\n                                <br>\n                                <sup id=\"fn5\">\n                                        2. <strong>Crisis Impact Types:</strong> river flood, rock-fall, landslide, residential housing damage, blocked roads, agricultural land damage, submerged residential areas, and damaged infrastructure.<a href=\"#ref5\" title=\"Jump back to footnote 2 in the text.\"></a>\n                                </sup>\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Annotation Workshop & Methodology Iteration - Aims</h5>\n                            <p>\n                                With these workshops, we aimed to:\n                                <ul>\n                                    <li>Understand <strong>cross-contextual insights</strong>, i.e. information needs and decision-making priorities common to both Fukuchiyama & US crisis management</li>\n                                    <li><strong>Compare our devised image analysis ML methodology</strong> for automatic insights to the insights gained from <strong>manual assessment</strong> of crisis images <strong>by crisis experts.</strong></li>\n                                    <li>Use results to <strong>iterate on design of ML methodology</strong> to better embed information needs and priorities of crisis managers.</li>\n                                </ul>\n                            </p>\n                            <p>\n                                The following questions were posed for each of the images to actively engage crisis managers in the annotation process. We note, however, that the conversations\n                                often expanded beyond these questions, getting to the core of what their main insights would be through manual assessment of the image data and what their main decision priorities would be during a flood crisis event as it related to analyzing the image data:\n                            </p>\n                            <br>\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12 col-md-8\">\n                                        <img src=\"../../../../public/assets/workshop-preface-questions.png\" class=\"img-fluid\">\n                                </div>\n                            </div>\n                            <br>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Annotation Workshops with Crisis Experts - Results</h5>\n                            <p>\n                                We report qualitative summaries describing the insights derived\n                                from manual assessment by domain experts of the crisis report images and their\n                                expressed information needs. We used these summaries to compare how the\n                                insights the Image Analysis Module aims to automatically provide to crisis managers\n                                compares to the insights derived from manual assessment by crisis experts and their\n                                expressed information needs. This comparison allowed us to qualitatively assess the utility of the Image Analysis Module in attaining situational awareness as it was\n                                devised in this work, and helped us in determining ways to iterate on the module in the future to better accommodate the information needs of the crisis managers during\n                                a crisis event through new prediction tasks which align with their information needs.\n                            </p>\n                            <h6>Cross-contextual Insights</h6>\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12\">\n                                    <p>\n                                        <ul>\n                                            <li>\n                                                <strong><u>Potential of Human Casualties in Crisis Imagery</u></strong>\n                                                <ul>\n                                                    <li>Possibility of Human Casualty is <strong>Top Priority</strong></li>\n                                                    <li>\n                                                        <strong>Identified physical markers</strong> suggesting <strong>potential for human casualty:</strong>\n                                                        <ul>\n                                                            <li>Submerged Vehicles</li>\n                                                            <li>Collapsed Buildings</li>\n                                                            <li>Housing in Close Proximity to Rockfall or Landslide</li>\n                                                        </ul>\n                                                    </li>\n                                                    <li>\n                                                        Not investigating when there is actually human casualty (False Negative) <strong>is more costly</strong> \n                                                        than investigating when there is not actually human casualty (False Positive)\n                                                    </li>\n                                                </ul>\n                                            </li>\n                                            <li>\n                                                <strong><u>Presence of People in Crisis Imagery</u></strong>\n                                                <ul>\n                                                    <li><strong>People in crisis imagery</strong> is important and should be <strong>assessed with high priority</strong></li>\n                                                    <li><strong>Insights should be specific,</strong> e.g. people laying down vs. people standing casually/walking about - can indicate crisis impact\n                                                        severity to personnel\n                                                    </li>\n                                                </ul>\n                                            </li>\n                                            <li>\n                                                <strong><u>Insights of Broader Impact derived from Physical Markers in Images:</u></strong>\n                                                <ul>\n                                                    <li>\n                                                        Experts <strong>identified physical markers</strong> which suggest <strong>potential broader impact to area</strong>, including:\n                                                        <ul>\n                                                            <li>Muddy Water  potential nearby landslide</li>\n                                                            <li>Fallen Power Pole  potential power outage</li>\n                                                            <li>Road Passability  possibility of emergency vehicle use & isolated residential areas</li>\n                                                        </ul>\n                                                    </li>\n                                                    \n                                                </ul>\n                                            </li>\n                                        </ul>\n                                    </p>\n                                </div>\n                            </div>\n                            <h6>Contextual Insights</h6>\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12\">\n                                    <p>\n                                        <ul>\n                                            <li>\n                                                <strong><u>National Standards in Japan for Assessing Impact Severity</u></strong>\n                                                <ul>\n                                                    <li>\n                                                        Standards for Flood Impact Severity on Housing used in Fukuchiyama:\n                                                        <ul>\n                                                            <li>Water reaches up to the first-floor ceiling  Severe Flooding/Destruction</li>\n                                                            <li>Water reaches 1m above first-floor level   Partial Flooding/Destruction</li>\n                                                            <li>Water reaches below flood level  Minor Flooding/Destruction</li>\n                                                        </ul>\n                                                    </li>\n                                                </ul>\n                                            </li>\n                                            <li>\n                                                <strong><u>Insights Derived from both the Image and Contextual-Knowledge:</u></strong>\n                                                <ul>\n                                                    <li>\n                                                        <strong>FC crisis experts used contextual knowledge</strong> of the area where the image was taken in gaining insights\n                                                        <ul>\n                                                            <li>E.g. Image showing flooding in an area that doesnt typically flood causes more concern for that area</li>\n                                                        </ul>\n                                                    </li>\n                                                </ul>\n                                            </li>\n                                        </ul>\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Annotation Workshop & Methodology Iteration - Discussion</h5>\n                            <p>\n                                <ul>\n                                    <li>\n                                        From crisis expert insights and feedback, we have determined that the tasks as\n                                        presented in this work have classes with interpretations that are either <strong>too\n                                        vague and subjective (damage severity, humanitarian categories, and informativeness)</strong>\n                                        or <strong>too simplistic (flood presence)</strong> to be useful for them in gaining situational awareness\n                                        about an unfolding crisis event.\n                                    </li>\n                                    <li>\n                                        The humanitarian categories task has the \"Rescue, Volunteering,\n                                        or Donation Effort\" class, which has insights for the <strong>recovery phase of a crisis\n                                        event rather than the emergency phase. Since our ML methodology aims to assist\n                                        crisis managers during the emergency phase of a crisis event, such classes should be\n                                        revised or replaced with classes which have insights directly for the emergency phase.</strong>\n                                    </li>\n                                    <li>\n                                        Although the <strong>flood presence task</strong> has classes with interpretations which are too simple for attaining situational awareness, we note that the <strong>relatively high performance,\n                                        high consistency between independent annotators, and clarity</strong> in the interpretation\n                                        of the classes associated with the flood presence task <strong>sets precedent for task creation and model performance for the future tasks</strong> developed from the insights and\n                                        feedback received from the workshops discussed in this work and future workshops.\n                                    </li>\n                                </ul>\n                            </p>\n                            <p> \n                                The insights and feedback provided by the domain experts enabled us to determine\n                                how the Image Analysis Module we have developed in this work is limited in helping\n                                to gain insights about the unfolding crisis event. <strong>Where our ML methodology falls\n                                short in meeting their information needs, their feedback will assist in developing new\n                                classification tasks which would be informative enough to assist them during a crisis\n                                event and clear enough to yield more consistent labels between annotators, ensuring\n                                better quality data to train and evaluate models.</strong> The development of new image\n                                prediction tasks and associated models will be conducted in a future work. However,\n                                we were able to apply some of these insights to inform the ML methodology of the\n                                <a href=\"#/projects/ml-for-crowdsourced-crisis-data/text-analysis-module\">Text Analysis Module</a>.\n                            </p>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../../constants';\n\nexport default {\n  name: 'ImageAnalysisCarousel',\n  mounted() {\n    enableScrollUpOnCarousel('#ImageAnalysisCarousel');\n    enableSwipeOnCarousel('#ImageAnalysisCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h5 {\n    color: white;\n}\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n#overview-pic {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 90vh;\n    height: 40vh;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n#image-analysis-module {\n    width: 70vh;\n    height: 80vh;\n}\n\n#pika-gif {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 60vh;\n    height: 40vh;\n}\n\n@media (min-width: 501px) and (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n@media (max-width: 500px) {\n\n    #test-set-eval {\n        height: 42vw;\n    }\n\n    #agg-metrics {\n        height: 42vw;\n    }\n}\n\n#fp-table {\n  font-family: Arial, Helvetica, sans-serif;\n  border-collapse: collapse;\n  color: black;\n  width: 100%;\n}\n\n#fp-table td, #fp-table th {\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n\n#fp-table tr:nth-child(even){background-color: #f2f2f2;}\n#fp-table tr:hover{background-color: #ddd;}\n#fp-table tr:nth-child(odd) {background-color: #ddd;}\n\n#fp-table th {\n  padding-top: 12px;\n  padding-bottom: 12px;\n  text-align: center;\n  background-color: darkturquoise;\n  color: white\n}\n\n</style>","import mod from \"-!../../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./ImageAnalysisCarousel.vue?vue&type=template&id=a1aa68b2&scoped=true&\"\nimport script from \"./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"\nexport * from \"./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"\nimport style0 from \"./ImageAnalysisCarousel.vue?vue&type=style&index=0&id=a1aa68b2&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"a1aa68b2\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-8\"},[_c('h3',[_vm._v(\"Text Analysis Module\")])]),_c('div',{staticClass:\"col-md-10\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"TextAnalysisCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"5\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"6\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"7\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"8\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"9\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"10\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"11\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"12\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"13\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"14\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"15\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"16\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"17\"}}),_c('li',{attrs:{\"data-target\":\"#TextAnalysisCarousel\",\"data-slide-to\":\"18\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item cc-carousel-item active\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-7 pt-3 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"text-analysis-module\",\"src\":require(\"../../../../public/assets/text-analysis-module.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"carousel-text col-12\"},[_c('h5',[_vm._v(\"Text Analysis Methodology Overview\")]),_c('p',[_vm._v(\" The Text Analysis Module aims to provide accurate and efficient classifications of crisis reports using the text modality that is often present in the reports. Another aim was to incorporate the insights we gained from the results of our qualitative analysis on the Image Analysis Module that were transferable between the data modalities, i.e. the importance of identifying potential for human casualty or risk to humans. We did this to exemplify our frameworks intention of producing iteratively developed ML methodologies and AI systems to enhance crisis awareness and response using insights gained from crisis managers. \")]),_c('h6',[_vm._v(\"Incorporating Crisis Expert Insights into Model Development & Performance Metric Selection\")]),_c('p',[_vm._v(\" To incorporate the insights of the crisis managers into the design and development of a new text classification model, we first created a classification task and associated classes that align with the expressed information needs of crisis managers during a crisis event. Then, we selected a performance evaluation metric that aligns with priorities of the crisis managers for that task, finally developing a model that is evaluated using the selected performance metric. \")]),_c('h6',[_vm._v(\"Human Risk Text Classification Experiments\")]),_c('p',[_vm._v(\" In the process of conducting this exercise, we performed various classification experiments, experimenting with various text featurizations, classical machine learning algorithms, and importantly, we deliberated on the selection of a performance evaluation metric based on our findings from the qualitative analysis of the image annotation workshops. \")]),_c('h6',[_vm._v(\"Preprocessing & Featurization of Japanese Text\")]),_c('p',[_vm._v(\" We note that since this study focused exclusively on Japanese crisis text, we constructed a preprocessing pipeline that uses open-source Japanese tokenizers, stop-words, and a lemmatizer to preprocess the Japanese text. Additionally, we investigated the use of text embeddings of the Japanese crisis text that are created by applying CLS pooling, a process which creates a contextualized numerical embedding of inputted text, using a pretrained Japanese Masked Language Modeling (MLM) BERT model in both our supervised and unsupervised learning experiments. \")]),_c('h6',[_vm._v(\"Clustering Data to Uncover Semantically-similar Groupings\")]),_c('p',[_vm._v(\" Finally, we conclude the development of this ML module on an exploratory note, devising a pipeline that evaluates a combination of text featurizations, dimensionality reduction techniques, and clustering algorithms to provide intuitive groupings of text to help inform the development of text classification tasks in future work. \")]),_c('h6',[_vm._v(\"Evaluation\")]),_c('p',[_vm._v(\" In our evaluation of our text classification experiments, we perform quantitative evaluation, assessing the performance of the model for the task based on the determined evaluation metric mentioned above in addition to other metrics, e.g. per-class performance metrics. For our unsupervised experiments, we include both quantitative and qualitative evaluation. Using the Within-Cluster Sum of Squares (WCSS) metric, we determine a set of optimal clustering pipeline configurations and their corresponding optimal number of clusters to use for further investigation. We assess qualitatively by investigating the resulting clusters and determining for each cluster, whether or not the representative documents within that cluster have a cohesive, interpretable label, and if they do, what that label is. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"carousel-text col-12\"},[_c('h5',[_vm._v(\"Summary of Results\")]),_c('h5',[_vm._v(\"Iterating on ML Methodology based on Crisis Managers Insights\")]),_c('p',[_vm._v(\" Our framework was developed to both highlight the importance of involving crisis managers in the process of developing a ML methodology and contextualize model performance among other measures of efficacy for the ML methodology. Since our framework seeks to be used in the development and iteration of an ML methodology based on the insights gained from crisis managers, we iterated on the Text Analysis Module using insights we had gained from our results on the Image Analysis Module. \")]),_c('h5',[_vm._v(\"Developing the Human Risk Task\")]),_c('p',[_vm._v(\" Using labels provided directly to us by crisis managers, we created a new text classification task in an effort to better fulfill their information needs during a crisis event. Using insights gained from the results of image annotation workshops of the Image Analysis Module, we determined F2 score to be an appropriate performance metric for model performance evaluation as false negatives are more costly than false positives for assessing human risk from text reports. \"),_c('strong',[_vm._v(\"To the best of our knowledge, the exercises of creating a classification task from labels provided directly by EOC and formulating an appropriate model performance metric informed from crisis expert insights are novel contributions of this work.\")]),_vm._v(\" These exercises follow directly inline with our framework, \"),_c('strong',[_vm._v(\"using the results from the Image Analysis Module to iteratively design and develop ML models for the Text Analysis Module.\")])]),_c('h5',[_vm._v(\"Human Risk Task Model Evaluation\")]),_c('p',[_vm._v(\" Using F2 as the metric to optimize for during 5 x 5 Nested CV, we were able to identify the SVM algorithm and its corresponding hyperparameter grid as achieving a relatively high mean F2 performance with low variance. To assess the models ability to perform the human risk classification task, we found the tuned SVM model to achieve an F2 score of 92.8%, which is a substantial improvement over the baseline models F2 score of 43.4%. Having a baseline is an important aspect of our framework as it enables us the ability to determine if a developed model is performing the task well, i.e. if it does not perform the task better than the baseline, it is not a useful model for the task. This suggests the tuned SVM model is a useful classifier for the task and performs the task reasonably well. This is further evidenced from the Precision-Recall curve, where the tuned SVM model achieved an AUCPR score of 0.919, which is a significant improvement over the typical baseline classifier used for that metric which achieves an AUCPR of 0.133. We note that recall for the \\\"Human Risk\\\" class is higher than precision likely being a result of using F2 as the performance metric to optimize in the classification experiments. Lastly, when looking at the per- class performance metrics for each class, we see that the model performs reasonably well on both classes achieving scores at or above 0.857 for the \\\"Human Risk\\\" class and at or above 0.976 for the \\\"Not Human Risk\\\" class. \")]),_c('h5',[_vm._v(\"Clustering of Firefighter Crisis Text Reports\")]),_c('p',[_vm._v(\" From our preliminary clustering assessments, we observed that clustering using K-medoids rather than K-means with all else equal (i.e. text featurization and dimensionality reduction technique), typically yielded lower WCSS scores across all  values between 2-20. This is likely due to the K-medoids algorithms robustness to outliers and noise, suggesting that there may exist some reports in the corpus which are quite different from the rest. \")]),_c('p',[_vm._v(\" We note that since the corpus we studied was specific to flood and typhoon crisis events, it is no surprise that many of the identified cluster labels are geared towards flood-related information such as \\\"Areas with Flood Risk\\\", \\\"River Water Level and Corresponding Warning for EOC/FD\\\", \\\"Residential Areas/Buildings in Flood (Risk)\\\", and \\\"Landslide/Fallen Tree\\\". Although some of the categories are quite general such as \\\"Rescue (Activities/Requests)\\\", \\\"Closed Roads by the City\\\", and \\\"Impassable Roads (due to Flood/Obstacles/Damage)\\\", we also see that some of the cluster labels are specific to the fire department such as \\\"Areas where FD is active\\\" and \\\"FD Activities/Weather Warning/Flood Control Alert\\\". \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Fukuchiyama Flood Text Reports Data Collection\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-4 pt-1 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"text-data-collection\",\"src\":require(\"../../../../public/assets/txt-data-collection.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 pt-2 pb-4 col-md-6\"},[_c('p',[_vm._v(\" Our crisis management partners in Fukuchiyama City (FC) compiled \"),_c('strong',[_vm._v(\"716 Japanese (JA) text transcripts\")]),_vm._v(\" of radio communications from \"),_c('strong',[_vm._v(\"on-the-ground firefighters\")]),_vm._v(\" which occurred during the following past FC flood events: \"),_c('ul',[_c('li',[_vm._v(\"Typhoon Manyi in 2013\")]),_c('li',[_vm._v(\"Heavy Rain Event in August 2014\")]),_c('li',[_vm._v(\"Typhoon Lan in 2017\")]),_c('li',[_vm._v(\"Heavy Rain Event in July 2018\")])]),_vm._v(\" The data collection process for during these events is depicted in the neighbouring figure. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Fukuchiyama Flood Text Reports Dataset Characteristics\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-7 pt-3 pb-2\"},[_c('table',{attrs:{\"id\":\"text-characteristics-table\"}},[_c('tr',[_c('th',[_c('strong',[_vm._v(\"Total Reports\")])]),_c('th',[_c('strong',[_vm._v(\"Reports Labeled for Human Risk\")])]),_c('th',[_c('strong',[_vm._v(\"Reports Labeled for Emergency Operation Center (EOC) Humanitarian Categories\")])]),_c('th',[_c('strong',[_vm._v(\"Unique EOC Humanitarian Categories\")])])]),_c('tr',[_c('td',[_vm._v(\"716\")]),_c('td',[_vm._v(\"715\")]),_c('td',[_vm._v(\"584\")]),_c('td',[_vm._v(\"108\")])])])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" We use all 716 reports in our clustering experiments. Since 715 out of the total 716 reports are labeled for Human Risk, we use those labeled reports in our classification experiments. \"),_c('br'),_c('br'),_vm._v(\" To understand how these firefighter crisis text reports compare to other Japanese crisis reports, we compare the FC firefighter reports character length distribution against another Japanese crisis text report corpus, the text of Tokyo crisis reports received by RiskMap (RM) during Typhoon Hagibis in 2019. We note that there are 68 reports in total for the Typhoon Hagibis RM reports dataset. \")])]),_c('div',{staticClass:\"col-10\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"dataset-comparison\",\"src\":require(\"../../../../public/assets/character_box_and_whisk_fc_rm.png\"),\"alt\":\"First slide\"}})]),_c('div',{staticClass:\"col-6 col-lg-4 pt-3\"},[_c('h6',[_vm._v(\"FC Firefighter Reports Characteristics\")]),_c('div',[_c('p',[_c('ul',[_c('li',[_vm._v(\"N = 716 Reports\")]),_c('li',[_vm._v(\"Median = 22 Characters\")])])])])]),_c('div',{staticClass:\"col-6 col-lg-4 pt-3\"},[_c('h6',[_vm._v(\"Typhoon Hagibis RM Reports Characteristics\")]),_c('p',{attrs:{\"id\":\"hagibis-details\"}},[_c('ul',[_c('li',[_vm._v(\"N = 68 Reports\")]),_c('li',[_vm._v(\"Median = 14 Characters\")])])])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" Most Japanese (JA) crisis text reports are between only a \"),_c('strong',[_vm._v(\"few characters\")]),_vm._v(\" to about \"),_c('strong',[_vm._v(\"50 characters\")]),_vm._v(\". We also observe that each distribution is right-skewed. This is further seen by Twitter research, which finds that \"),_c('strong',[_vm._v(\"JA tweets\")]),_vm._v(\" have a mode of \"),_c('strong',[_vm._v(\"15 characters\")]),_vm._v(\", with a character distribution exhibiting right-skew. It is noted that \"),_c('strong',[_vm._v(\"English (EN) tweets\")]),_vm._v(\" have a mode of \"),_c('strong',[_vm._v(\"34 characters\")]),_vm._v(\", which as the authors state, \")])]),_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\" This is because in languages like Japanese, Korean, and Chinese you can convey about double the amount of information in one character as you can in many other languages, like English, Spanish, Portuguese, or French\\\"\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnA\",\"id\":\"refA\"}},[_vm._v(\"1\")])])])]),_c('div',{staticClass:\"carousel-text col-12\"},[_c('p',[_c('strong',[_vm._v(\"Comparing these various dataset distributions suggests that the FC firefighter text reports are of similar character length to RM JA reports and JA tweets.\")])])])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnA\"}},[_vm._v(\"1. A. Rosen and I. Ihara, Giving you more characters to express yourself. \"),_c('a',{attrs:{\"href\":\"https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself\",\"target\":\"_blank\"}},[_vm._v(\"https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself\")]),_vm._v(\", Sept. 2017.\"),_c('a',{attrs:{\"href\":\"#refA\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Text Preprocessing & Featurization Pipeline\")]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-9 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"text-pipeline\",\"src\":require(\"../../../../public/assets/text-preprocessing.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"carousel-text col-12\"},[_c('p',[_vm._v(\" In order to use the FC firefighter report text data in our classification and clustering experiments, we needed to featurize, or construct numerical representations (feature vectors) of the text to use as input to a ML model. In the pipeline we developed for the featurization of Japanese text, \"),_c('strong',[_vm._v(\"we investigate 4 different featurizations of the text\")]),_vm._v(\", namely, \"),_c('strong',[_vm._v(\"Bag-of-Words (BOW)\")]),_vm._v(\" based on unigram & bigram representations, \"),_c('strong',[_vm._v(\"Term-Frequency Inverse-Document-Frequency (TF-IDF)\")]),_vm._v(\" based on unigram, and finally \"),_c('strong',[_vm._v(\"pretrained BERT embeddings using CLS Pooling\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Depending on the featurization, we integrate various preprocessing steps in order to perform commonplace Natural Language Processing (NLP) preprocessing steps. Due to our limited knowledge of the Japanese language, we make use of popular tokenizers & a lemmatizer pretrained on Japanese text as well as an open-source Japanese stopwords list for preprocessing the JA text. \")]),_c('p',[_vm._v(\" In the following slide we describe the preprocessing and featurization steps of our pipeline as applied to the input data in order to yield the featurizations we have mentioned. Additionally, we note the associated pros and cons of the featurizations. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Text Preprocessing & Featurization Pipeline\")]),_c('h5',[_c('u',[_vm._v(\"BOW & TF-IDF Preprocessing & Featurization\")])]),_c('div',{staticClass:\"col-12 pb-2 pt-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"n-gram-preprocessing\",\"src\":require(\"../../../../public/assets/n-gram-preprocessing.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\"Preprocessing\")])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" For the BOW based on unigram, BOW based on bigram, and TF-IDF (based on unigram) featurizations, we leverage a popular, open-source JA tokenizer, stopwords list, and lemmatizer to \"),_c('strong',[_vm._v(\"preprocess the raw input text\")]),_c('sup',[_c('a',{attrs:{\"href\":\"#fnB\",\"id\":\"refB\"}},[_vm._v(\"1\")])]),_vm._v(\", these steps can be seen in the figure above: \"),_c('ol',[_c('li',[_vm._v(\" Break up raw report text into \"),_c('strong',[_vm._v(\"word tokens\")]),_vm._v(\", i.e. tokenize: \"),_c('ul',[_c('li',[_vm._v(\"E.g. the road is submerged.  [the, road, is, submerged, .]\")])])]),_c('li',[_vm._v(\" We remove stopwords (e.g. the, as, it, is, .)  otherwise could add noise to the input: \"),_c('ul',[_c('li',[_vm._v(\"E.g. [\"),_c('strong',[_vm._v(\"the\")]),_vm._v(\", road, \"),_c('strong',[_vm._v(\"is\")]),_vm._v(\", submerged, .]  [road, submerged]\")])])]),_c('li',[_vm._v(\" We \"),_c('strong',[_vm._v(\"lemmatize\")]),_vm._v(\" word tokens, i.e. convert word to its lemma, or dictionary form: \"),_c('ul',[_c('li',[_vm._v(\"E.g. [road, \"),_c('strong',[_vm._v(\"submerged\")]),_vm._v(\"]  [road, \"),_c('strong',[_vm._v(\"submerge\")]),_vm._v(\"] \")])])])])])]),_c('h6',[_vm._v(\"Featurization\")]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" After preprocessing the raw text, we then use the preprocessed input to form an n-gram representation, which in our work was limited to unigram and bigram representations, but we note our pipeline generalizes to produce n-gram representations. For example, the unigram representation of the processed example used above [\\\"road\\\", \\\"submerge\\\"] would be as [\\\"road\\\", \\\"submerge\\\"] and the bigram representation would be [\\\"road submerge\\\"]. Once the n-gram representation is computed from the preprocessed input, we convert the preprocessed word tokens into the BOW or TF-IDF feature vector representations, or featurizations, which can be used as inputs to ML models. The values of the \"),_c('strong',[_vm._v(\"BOW n-gram features\")]),_vm._v(\" are simply their associated \"),_c('strong',[_vm._v(\"frequency\")]),_vm._v(\" in a text report. We show the resulting feature vector for BOW based on unigram for the [\\\"road\\\", \\\"submerge\\\"] example below: \")])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"bow-unigram-ex\",\"src\":require(\"../../../../public/assets/bag-of-word-features.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" For the TF-IDF featurizations based on unigrams, the feature values are computed by considering both the \"),_c('strong',[_vm._v(\"frequency\")]),_vm._v(\" of the unigrams in the report as well as the \"),_c('strong',[_vm._v(\"occurence of the unigram across all reports\")]),_vm._v(\". This value gives a relative importance to a unigram that considers the unigram in a specific report and across all reports. We show the resulting feature vector for TF-IDF based on unigram for the [\\\"road\\\", \\\"submerge\\\"] example below as if it were part of a collection of reports, or a text corpus: \")])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"tfidf-unigram-ex\",\"src\":require(\"../../../../public/assets/tfidf-features.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-7 col-md-3 pt-3 pl-md-5\"},[_c('h6',[_c('u',[_vm._v(\"Pros:\")])]),_c('div',[_c('p',[_c('ul',[_c('li',[_vm._v(\"Interpretable\")]),_c('li',[_c('strong',[_vm._v(\"Language-agnostic\")])])])])])]),_c('div',{staticClass:\"col-12 col-lg-4 pt-3\"},[_c('h6',[_c('u',[_vm._v(\"Cons:\")])]),_c('p',[_c('ul',[_c('li',[_vm._v(\"Sparse (many 0's) & High-Dimensional\")]),_c('li',[_vm._v(\"Doesn't do well for \"),_c('strong',[_vm._v(\"Out-of-Vocab (OOV)\")]),_vm._v(\" word tokens\")]),_c('li',[_c('strong',[_vm._v(\"Language-agnostic\")]),_vm._v(\" (i.e. inability to capture specificity to a particular language)\")]),_c('li',[_c('strong',[_vm._v(\"Severely limited ability\")]),_vm._v(\" to capture \"),_c('strong',[_vm._v(\"token similarity, long-range dependencies, and understanding of a language\")])])])])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" Although these n-gram-based featurizations have the benefit of being language-agnostic, we note that they have the limitations of being high-dimensional and sparse in which most entries of the feature vector are zero, an inability to model long-range dependencies between tokens in the context of a document, and a severely limited ability to capture token similarity and understanding of a language. Therefore, we investigate a featurization strategy that yields dense, contextualized document representations specific to Japanese text documents. This strategy uses a pretrained Japanese Masked Language Modeling (MLM) Bidirectional Encoder Representations from Transformers (BERT) model and the CLS pooling technique. \")])])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnB\"}},[_vm._v(\"1. We note that in this work we make use of the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/fugashi/1.1.2/\",\"target\":\"_blank\"}},[_vm._v(\"fugashi\")]),_vm._v(\" (version: 1.1.2) open-source morphological tool for \"),_c('strong',[_vm._v(\"tokenizing and lemmatizing\")]),_vm._v(\" Japanese text. Since fugashi requires a dictionary to operate, we use the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/unidic-lite/1.0.8/\",\"target\":\"_blank\"}},[_vm._v(\"Unidic Lite\")]),_vm._v(\" dictionary (version: 1.0.8). Finally, for the stopwords list, we use a versioned, open-source list available \"),_c('a',{attrs:{\"href\":\"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/5a000f6a62f9e3a12f436f36d168e2fcd2fb1878/stopwords-ja.json\",\"target\":\"_blank\"}},[_vm._v(\"here\")]),_vm._v(\".\"),_c('a',{attrs:{\"href\":\"#refB\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Text Preprocessing & Featurization Pipeline\")]),_c('h5',[_c('u',[_vm._v(\"Pretrained Japanese MLM BERT Model Embeddings Preprocessing & Featurization\")])]),_c('div',{staticClass:\"col-12 pb-2 pt-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"bert-features-preprocessing\",\"src\":require(\"../../../../public/assets/bert-preprocessing.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\"Preprocessing\")])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" The BERT MLM Deep learning (NN) model is optimized to \"),_c('strong',[_vm._v(\"predict\")]),_vm._v(\" a randomly \"),_c('strong',[_vm._v(\"masked words\")]),_vm._v(\" by using the \"),_c('strong',[_vm._v(\"context\")]),_vm._v(\", or the words that surround them. \")]),_c('p',[_vm._v(\" Tohoku University Researchers \"),_c('strong',[_vm._v(\"pretrained a MLM BERT\")]),_vm._v(\" model using a dataset of approx. \"),_c('strong',[_vm._v(\"30M sentences\")]),_vm._v(\" of the \"),_c('strong',[_vm._v(\"Japanese version of Wikipedia\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Raw text data is word tokenized using Fugashi & Unidic Lite library. Word tokens are further split into subwords using the \"),_c('strong',[_vm._v(\"WordPiece algorithm\")]),_vm._v(\", yielding a token vocabulary of \"),_c('strong',[_vm._v(\"32768 unique tokens.\")]),_c('sup',[_c('a',{attrs:{\"href\":\"#fnC\",\"id\":\"refC\"}},[_vm._v(\"1\")])])])]),_c('h6',[_vm._v(\"Featurization\")]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" The text input tokens from resulting from the preprocessing steps are prepended with a \"),_c('strong',[_vm._v(\"[CLS]\")]),_vm._v(\" token, the Classification token. Deep in the BERT model, we extract a \"),_c('strong',[_vm._v(\"contextualized numerical representation\")]),_vm._v(\" of the report for classification tasks by grabbing the \"),_c('strong',[_vm._v(\"final hidden state\")]),_vm._v(\" corresponding to this token. This is \"),_c('strong',[_c('i',[_vm._v(\"CLS Pooling\")])]),_vm._v(\", yielding a dense, contextualized feature vector of 768 dimensions. We refer to these feature vectors as BERT embeddings. We show an example of a featurized text report below: \")])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"bert-features-ex\",\"src\":require(\"../../../../public/assets/bert-features.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-5 pt-3 pl-md-5\"},[_c('h6',[_c('u',[_vm._v(\"Pros:\")])]),_c('div',[_c('p',[_c('ul',[_c('li',[_c('strong',[_vm._v(\"Optimized for the Japanese language\")])]),_c('li',[_c('strong',[_vm._v(\"Dense features \")]),_vm._v(\" (i.e. lower dimensions than previous n-gram based features & not sparse)\")]),_c('li',[_c('strong',[_vm._v(\"Contextualized representations\")]),_vm._v(\", which can better capture long-range dependencies between words in a report & State-of-the-art \"),_c('strong',[_vm._v(\"language understanding\")])]),_c('li',[_vm._v(\" Better-equipped to handle \"),_c('strong',[_vm._v(\"OOV tokens\")]),_vm._v(\" due to the use of the WordPiece algorithm \")])])])])]),_c('div',{staticClass:\"col-12 col-lg-4 pt-3\"},[_c('h6',[_c('u',[_vm._v(\"Cons:\")])]),_c('p',[_c('ul',[_c('li',[_vm._v(\"Features are \"),_c('strong',[_vm._v(\"NOT interpretable\")])]),_c('li',[_c('strong',[_vm._v(\"Specific to JA\")])]),_c('li',[_c('strong',[_vm._v(\"Not finetuned to crisis text corpus\")]),_vm._v(\", although we note this could be done in a future work\")])])])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" Having preprocessed the text and produced various featurizations with various pros and cons, we utilized all of the featurizations in our classification experiments discussed in the next slides and TF-IDF based on unigram features as well as BERT embeddings in our clustering experiments. \")])])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnC\"}},[_vm._v(\"1. \"),_c('a',{attrs:{\"href\":\"https://huggingface.co/cl-tohoku/bert-base-japanese-v2\",\"target\":\"_blank\"}},[_vm._v(\"Link to Pretrained Japanese BERT MLM Model\")]),_c('a',{attrs:{\"href\":\"#refC\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Human Risk Text Classification\")]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" Since 715 out of the 716 firefighter text reports were labeled for the binary \"),_c('strong',[_vm._v(\"Human Risk/No Human Risk classes\")]),_vm._v(\", we chose to focus our text classification experiments on this classification task. Additional with the text analysis module, we aimed to develop a task that \"),_c('strong',[_vm._v(\"better met the information needs\")]),_vm._v(\" of crisis managers during a crisis event. We did this by using the labels our crisis management partners in Fukuchiyama provided to us directly. \")])]),_c('h6',[_vm._v(\"Task Description\")]),_c('div',{staticClass:\"col-12\"},[_c('p',[_c('i',[_vm._v(\" The Human Risk text classification task \"),_c('strong',[_vm._v(\"determines whether or not a crisis text report indicates if there are people in need of rescue from a crisis.\")]),_vm._v(\" This includes people being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), and/or being in need of life-saving emergency medical care. \")])]),_c('p',[_vm._v(\" In an effort to transition from abstract class descriptions to something more specific, e.g. a checklist, we choose to both provide a definition of the task and further detail the Human Risk classes as bulleted lists containing the specific traits/descriptors contained in a text report which are characteristic of each class with the aim of enhancing the clarity of each class and the task overall: \")])]),_c('div',{staticClass:\"col-12 col-md-7 pt-3 pl-md-5\"},[_c('h6',[_c('strong',[_vm._v(\"Human Risk\")]),_vm._v(\" Class Descriptors\")]),_c('div',[_c('p',[_c('ul',[_c('li',[_vm._v(\"Rescue being requested (to the Fire Department (FD))\")]),_c('li',[_vm._v(\"Evacuation support being requested (to the FD)\")]),_c('li',[_vm._v(\"Human missed the chance to evacuate from their own house, at work, shopping center, etc.\")]),_c('li',[_vm._v(\"Vulnerable population (elderly, disabled, small children) being left in the house in the flooding area\")]),_c('li',[_vm._v(\"Water rising inside the house above the floor (human inside)\")]),_c('li',[_vm._v(\"Water current is fast inside the house and hard to move upstairs (human inside) \")]),_c('li',[_vm._v(\"Sediment flowing into the house (human inside)\")]),_c('li',[_vm._v(\"Human being trapped in elevator, submerged car, or a car which is not submerged yet\")]),_c('li',[_vm._v(\"Human being washed away in a river\")]),_c('li',[_vm._v(\"Rescue team dispatched\")]),_c('li',[_vm._v(\"Rescue team in activity (such as helping evacuation, rescuing, etc.)\")]),_c('li',[_vm._v(\"Rescue activity completed\")]),_c('li',[_vm._v(\"Landslide occurrence on the highway - possible vehicle being involved\")])])])])]),_c('div',{staticClass:\"col-12 col-md-4 pt-3\"},[_c('h6',[_c('strong',[_vm._v(\"No Human Risk\")]),_vm._v(\" Class Descriptors\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\"Dam Discharge\")]),_c('li',[_vm._v(\"Meteorological Information\")]),_c('li',[_vm._v(\"River Water Level Information\")]),_c('li',[_vm._v(\"Weather Alert\")]),_c('li',[_vm._v(\"Road Closure\")]),_c('li',[_vm._v(\"Road Flood Risk (not flooded yet)\")]),_c('li',[_vm._v(\"Area Flood Risk (not flooded yet)\")])])])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-risk-classification\",\"src\":require(\"../../../../public/assets/human-risk-diagram.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" In addition to using labels/classes which better meet the information needs of crisis managers during crisis, we aimed to develop the classification model for this task \"),_c('strong',[_vm._v(\"using a performance metric which better aligns with the priorities of the crisis managers\")]),_vm._v(\" as it pertains to this task and also \"),_c('strong',[_vm._v(\"considers the nature of the data (i.e. class imbalance)\")]),_vm._v(\". We discuss these considerations and how we \"),_c('strong',[_vm._v(\"determined the performance metric\")]),_vm._v(\" for this task in the next slides. We note that \"),_c('strong',[_vm._v(\"this process\")]),_vm._v(\" of constructing a task from \"),_c('strong',[_vm._v(\"crisis manager's labels\")]),_vm._v(\" to determining the appropriate performance metric to utilize for developing a model which considers both the \"),_c('strong',[_vm._v(\"properties of the data\")]),_vm._v(\" and the \"),_c('strong',[_vm._v(\"insights we gained from crisis managers\")]),_vm._v(\" is a \"),_c('strong',[_vm._v(\"novel contribution of this research.\")])])])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnD\"}},[_vm._v(\"1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who determined these class definitions from examining the original Japanese reports.\"),_c('a',{attrs:{\"href\":\"#refD\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('h5',[_vm._v(\"Human Risk Classification - Determination of the Performance Evaluation Metric\")]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" In our determination of the performance metric to use for assessing the performance of the Human Risk classifier we consider both the class imbalance of the task and insights we gained from the workshops conducted in the image analysis module. \")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-riks-label-distribution\",\"src\":require(\"../../../../public/assets/human-risk-label-distribution.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 pt-3 col-md-6\"},[_c('h5',[_vm._v(\"Substantial Class Imbalance\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Across the 715 reports labeled for Human Risk, we observe that there are disporportionately more \\\"No Human Risk\\\" data points than \\\"Human Risk\\\" data points, thus the \"),_c('strong',[_vm._v(\"\\\"Human Risk\\\" class is the minority class\")]),_vm._v(\" for this task. \")]),_c('li',[_c('strong',[_vm._v(\"Accuracy\")]),_vm._v(\" of the classifier which always predicts \\\"No Human Risk\\\": \"),_c('strong',[_vm._v(\"86.7%\")]),_vm._v(\", i.e. the percentage of \\\"No Human Risk\\\" labels in the dataset \")]),_c('li',[_vm._v(\" Need to account for this imbalance in the metric, otherwise conclusions about the model's ability to perform the task can be misleading, as seen with using accuracy as the performance metric \")])])])]),_c('div',{staticClass:\"col-12 col-md-6 pt-md-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"insights-from-workshops\",\"src\":require(\"../../../../public/assets/workshop-insights.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"carousel-text col-12 col-md-6\"},[_c('h5',[_vm._v(\"Insights from Image Annotation Workshops\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" From the workshops, we understand that in assessing the potential for human casualities, the cost associated with \"),_c('strong',[_vm._v(\"NOT investigating the potential for human casualities when there ARE human casualities (False Negative (FN))\")]),_vm._v(\" is considered \"),_c('strong',[_vm._v(\"higher\")]),_vm._v(\" than the cost of \"),_c('strong',[_vm._v(\"investigating potential human casualities when there ARE NONE (False Positive (FP))\")]),_vm._v(\"  \"),_c('strong',[_vm._v(\"Performance on \\\"Human Risk\\\" class is paramount\")]),_vm._v(\". \")]),_c('li',[_c('strong',[_vm._v(\"Accuracy\")]),_vm._v(\" does NOT tell us how well the model performs on the \"),_c('strong',[_vm._v(\"\\\"Human Risk\\\"\")]),_vm._v(\" class \"),_c('strong',[_vm._v(\" Although Recall (Minimizing FNs)\")]),_vm._v(\" & \"),_c('strong',[_vm._v(\"Precision (Minimizing FPs)\")]),_vm._v(\" have different priorities, both focus on the performance of the \\\"Human Risk\\\" class \")]),_c('li',[_vm._v(\" Ideally, we'd like to minimize both FN & FP, which is captured in the \"),_c('strong',[_vm._v(\"F1 score\")]),_vm._v(\", however F1 score treats recall as equally as important as precision \")]),_c('li',[_c('strong',[_vm._v(\"F2 score\")]),_vm._v(\" treats recall as 2x as important as precision, i.e. \"),_c('strong',[_vm._v(\"the relative cost of FN is twice as much as the cost of FP\")])])])])]),_c('div',{staticClass:\"col-12 pb-5\"},[_c('p',[_vm._v(\"  We thus used the \"),_c('strong',[_vm._v(\"properties of the data\")]),_vm._v(\" (i.e. class imbalance) & \"),_c('strong',[_vm._v(\"the insights we gained from crisis experts\")]),_vm._v(\" to determine the performance metric to evaluate the model we develop for the Human Risk task  the \"),_c('strong',[_vm._v(\"F2 score\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('h5',[_vm._v(\"Human Risk Classification - Data Splits & Algorithm Selection\")]),_c('div',{staticClass:\"col-12 col-md-6 pt-3\"},[_c('h5',[_vm._v(\"Train/Test Splits\")]),_c('p',[_vm._v(\" We split the full dataset of 715 reports into non-overlapping train and test splits in percentages of 80%/20%, respectively. Additionally, we preserve the class imbalance using stratified splitting. \")])]),_c('div',{staticClass:\"col-12 col-md-4 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"data-splitting\",\"src\":require(\"../../../../public/assets/human-risk-data-splits.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Nested Cross Validation for Algorithm Selection\")]),_c('p',[_vm._v(\" We were interested in investigating multiple ML algorithms for the Human Risk classification task, each with their own set of tunable hyperparameters. We aimed to determine which algorithm paired with a corresponding hyperparameter grid search procedure (e.g. Grid Search), i.e. fitting a model to each unique hyperparameter combination in the grid, had the best estimated generalization performance and use that algorithm for the final model evaluation. We note that since we had insufficient data to use train/dev/test splits, we used a variation of K-fold Cross Validation (CV). \")]),_c('p',[_vm._v(\" Since using the same K-fold CV procedure for both performing hyperparameter tuning and estimating generalization performance can yield an estimated generalization performance that is biased and overly-optimistic, we elected to use \"),_c('strong',[_vm._v(\"Nested CV\")]),_vm._v(\". Nested CV is typically inpractical in large data settings as it is substantially more computationally expensive to perform as compared to K-fold CV; for our low-data setting it was feasible to use. Nested CV is a useful variation of CV as it mitigates the bias in the generalization performance estimate of the algorithm and its corresponding search procedure by nesting the hyperparameter optimization within the generalization performance estimation procedure. \")]),_c('p',[_vm._v(\" For the algorithm selection procedure, we investigated the following classification algorithms: \")])]),_c('div',{staticClass:\"col-8 col-md-4\"},[_c('p',[_c('ul',[_c('li',[_vm._v(\"Logistic Regression\")]),_c('li',[_vm._v(\"Decision Tree\")]),_c('li',[_vm._v(\"Random Forest\")]),_c('li',[_vm._v(\"Support Vector Machine\")]),_c('li',[_vm._v(\"Multinomial Naive Bayes\")]),_c('li',[_vm._v(\"K-Nearest Neighbors\")])])])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" We note that we perform 5 x 5 Nested CV on the train split data & treat the various text featurizations offered by our featurization pipeline as hyperparameters in the hyperparameter grid for each algorithm. \")]),_c('p',[_vm._v(\" For the model evaluation on the test set, we select the algorithm (and corresponding search procedure) which had the highest relative mean F2 score with the lowest variance across fold (low standard deviation) from the nested CV procedure. \")])]),_c('div',{staticClass:\"col-12 col-md-8 pt-3 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"nested-cv\",\"src\":require(\"../../../../public/assets/nested-cv.png\"),\"alt\":\"Second slide\"}})])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Human Risk Classification\")]),_c('h5',[_vm._v(\"Algorithm Selection Results\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"algo-selection-table\",\"src\":require(\"../../../../public/assets/nested-cv-table.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-md-5 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"algo-selection-graph\",\"src\":require(\"../../../../public/assets/nested-cv-graph.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" The results presented above are determined from the generalization performance estimation found from the performance (by F2 score) on the outer loop 5-fold CV in Nested CV. We make available the intermediate and final results of Nested CV for each algorithm and corresponding hyperparameter grid.\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnD\",\"id\":\"refD\"}},[_vm._v(\"1\")])])]),_c('p',[_vm._v(\" From the results, we determined that the performance of the \"),_c('strong',[_vm._v(\"Support Vector Machine (SVM)\")]),_vm._v(\" algorithm with its corresponding hyperparameter search procedure yielded the highest mean F2 score, 82.0%, and the lowest standard deviation, 4.22%. We therefore select the Support Vector Machine (SVM) algorithm and its corresponding hyperparameter grid for the final human risk model evaluation on the test set. \")])])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnD\"}},[_vm._v(\"1. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module/Classification/Nested%20CV\",\"target\":\"_blank\"}},[_vm._v(\"Link to Results & Hyperparameters of Nested CV\")]),_c('a',{attrs:{\"href\":\"#refD\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Human Risk Classification\")]),_c('h5',[_vm._v(\"Final Model Evaluation\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"model-evalution-diagram\",\"src\":require(\"../../../../public/assets/human-risk-model-evaluation.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Tuning the SVM Model\")]),_c('p',[_vm._v(\" Prior to performing the final evaluation of the SVM on the test split data, we performed 5-fold CV on the full train split data, applying grid search with the hyperparameter grid associated with the SVM algorithm to find optimal hyperparameter values. The optimal hyperparameter values found for the SVM are shown in the table below: \")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"svm-hyperparameters\",\"src\":require(\"../../../../public/assets/svm-hyperparameters.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 pb-5\"},[_c('p',[_vm._v(\" We report the estimated generalization performance of the tuned SVM found from the 5-fold CV mentioned above, noting that this is \"),_c('strong',[_vm._v(\"likely a biased estimate of generalization performance\")]),_vm._v(\" as the 5-fold CV procedure was also used to tune the model. The \"),_c('strong',[_vm._v(\"tuned SVM model\")]),_vm._v(\" achieves a \"),_c('strong',[_vm._v(\"mean F2 score of 85.0%\")]),_vm._v(\" and has a \"),_c('strong',[_vm._v(\"standard deviation of 7.40%\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" After tuning the SVM model to find the optimal hyperparameters above, we fit the SVM algorithm using those optimal hyperparameters on the \"),_c('strong',[_vm._v(\"entire train split data\")]),_vm._v(\". We then use this \"),_c('strong',[_vm._v(\"fitted SVM model to predict on the unseen test split data\")]),_vm._v(\". The model's predictions on the test split yield the final evaluation of the model's generalization performance. As part of our framework, we report aggregate metrics including the F2 and Area Under the Precision-Recall Curve (AUCPR) score and compare our trained model to baseline scores. Lastly, we report per-class performance metrics and the confusion matrix of the model's predictions. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Human Risk Classification\")]),_c('h5',[_vm._v(\"Final Model Evaluation - Results\")])]),_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Aggregate Metrics & Comparison to Baseline Scores\")])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" We report an \"),_c('strong',[_vm._v(\"F2 score of 92.8% on the test split data.\")]),_vm._v(\" The baseline classifier which always predicts \\\"Human Risk\\\", has an F2 score of 43.4%, so the \"),_c('strong',[_vm._v(\"trained classifier is a significant improvement over the baseline classifier\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" In addition to F2, we plot the Precision-Recall curve, visualizing the tradeoff between precision and recall for different classification thresholds used by the classifier when classifying the data. It is advised to use the PR curve over the Receiver Operating Characteristic (ROC) curve in the case of imbalanced data, as ROC can give an optimistic estimate of the classifiers output quality by considering true negatives in the computation, which in high quantity can dramatically lessen the effect of the false positives, false negatives, and true positives in the performance estimate, giving a misleadingly high estimate of performance. \")])]),_c('div',{staticClass:\"col-12 col-md-4 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"aucpr-curve\",\"src\":require(\"../../../../public/assets/human-risk-aucpr.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-8\"},[_c('p',[_vm._v(\" The better the classifier (higher recall and higher precision), the closer the AUCPR score is to 1. For AUCPR, the performance of a \"),_c('strong',[_vm._v(\"baseline model has a score which is given by the proportion of positive samples to the total number of samples\")]),_vm._v(\" in the test dataset, which in this case is \"),_c('strong',[_vm._v(\"0.133\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" We report an \"),_c('strong',[_vm._v(\"AUCPR of 0.919 for the SVM model\")]),_vm._v(\" on the test split data, a significant improvement over the baseline score. \")])]),_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Confusion Matrix & Per-Class Metrics\")])]),_c('div',{staticClass:\"col-12 col-md-5 pt-3 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-risk-confusion-matrix\",\"src\":require(\"../../../../public/assets/human-risk-cm-svm.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-5 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-risk-per-class\",\"src\":require(\"../../../../public/assets/human-risk-per-class-metric.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 pb-5\"},[_c('p',[_vm._v(\" From the confusion matrix, we see that the model made very few misclassifications on the test split data. Specifically, the model only misclassified one data point which was labeled as \\\"Human Risk\\\" as \\\"No Human Risk\\\" out of all 19 data points labeled as \\\"Human Risk\\\", thus the model had \"),_c('strong',[_vm._v(\"low false negatives\")]),_vm._v(\". The model misclassified 3 \\\"No Human Risk\\\" data points as \\\"Human Risk\\\" out of a total of 124 \\\"No Human Risk\\\" data points, thus the model predicted \"),_c('strong',[_vm._v(\"3 false positives\")]),_vm._v(\". We note that the model had more false positives than false negatives, but \"),_c('strong',[_vm._v(\"few of each\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" The model performs well by all per-class metrics on the \\\"No Human Risk\\\" class achieving scores at and above 0.976. Comparatively lower performance is observed across all metrics for the \\\"Human Risk\\\" class with precision, recall, and F1 scores of 0.857, 0.947, and 0.9, respectively. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"carousel-text col-12\"},[_c('h5',[_vm._v(\"Human Risk Text Classification - Discussion\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" When determing the performance metric for the human risk task, we asked the following technical questions: \"),_c('ol',[_c('li',[_c('strong',[_vm._v(\"Does the metric account for imbalance present in the data distribution?\")])]),_c('li',[_c('strong',[_vm._v(\"Once the metric is determined, what is the performance of the baseline model for the task?\")])])])])])]),_c('p',[_vm._v(\" While these technical questions are no doubt important for assessing model efficacy for the task, we underscore that there were other questions we asked which \"),_c('strong',[_c('u',[_vm._v(\"could only be answered by our engagement with crisis managers.\")])])]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Before we began developing the human risk model, we asked a question about the task itself: \"),_c('strong',[_vm._v(\"Do the classes for the task sufficiently capture the expressed information needs of crisis managers during a crisis?\")]),_c('ul',[_c('li',[_vm._v(\" Since these \"),_c('strong',[_vm._v(\"labels were provided directly by crisis managers\")]),_vm._v(\" and given the \"),_c('strong',[_vm._v(\"crisis manager's insight into the importance of assessing the potential of human casualty\")]),_vm._v(\", we determined that these classes sufficiently capture an important information need of crisis managers during crisis. \")])])]),_c('li',[_vm._v(\" For the determining the performance metric for the task, questions which required crisis manager engagement included: \"),_c('ul',[_c('li',[_c('strong',[_vm._v(\" Does the metric incorporate the priorities of the crisis managers as it relates to the task, e.g. the cost of a false negative is significantly higher than the cost of a false positive for assessing human risk? \")])]),_c('li',[_c('strong',[_vm._v(\" Are there multiple metrics that should considered in assessing model efficacy in performing the classification task, e.g. precision and recall, or F2? \")])])])])])]),_c('p',[_vm._v(\" Asking these questions allowed us to both consider the technical intricacies for the task (i.e. data imbalance and baseline performance) and directly embed the information needs and priorities of crisis managers into our text ML methodology and evaluation, which are important aims of our framework. \")]),_c('p',[_vm._v(\" The \"),_c('strong',[_vm._v(\"significant improvement over the baseline classifier by F2\")]),_vm._v(\" suggests the tuned \"),_c('strong',[_vm._v(\"SVM model is a useful classifier for the human risk task and performs the task reasonably well\")]),_vm._v(\". This is further evidenced from the Precision-Recall curve, where the tuned SVM model achieved an AUCPR score of 0.919, a significant improvement over the typical baseline classifier used for that metric which achieves an AUCPR of 0.133 \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering of Crowdsourced Japanese Crisis Text Data\")]),_c('h5',[_vm._v(\"Overview\")])]),_c('div',{staticClass:\"col-12\"},[_c('p',[_vm._v(\" Beyond investigating the human risk classification task, we aimed to explore the Fukuchiyama firefighter flood crisis report corpus to see if we could uncover other coherent categories which may exist in the data. These uncovered categories could inform the development of classification tasks in future work, in addition to any of the humanitarian categories provided by crisis managers. \")]),_c('p',[_vm._v(\" This exploration is powered by a series of unsupervised learning techniques including dimensionality reduction and clustering. We developed a pipeline that utilizes these unsupervised techniques to perform the clustering experiments we conducted to uncover coherent categories in the data. \")]),_c('p',[_vm._v(\" For our clustering experiments, we note that we use \"),_c('strong',[_vm._v(\"all 716 FC firefighter crisis reports\")]),_vm._v(\". Additionally, we focus on using the \"),_c('strong',[_vm._v(\"TF-IDF based on unigrams embeddings\")]),_vm._v(\" and \"),_c('strong',[_vm._v(\"Pretrained Japanese BERT embeddings\")]),_vm._v(\", which we refer to as BERT embeddings for brevity. Since these featurizations are 1489 and 768 dimensions respectively, this motivated our incorporation of dimensionality reduction techniques into our clustering pipeline. \")]),_c('div',{staticClass:\"col-12 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"clustering-features\",\"src\":require(\"../../../../public/assets/text-features-for-clustering.png\"),\"alt\":\"Second slide\"}})])]),_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\"Evaluation Overview\")])]),_c('div',{staticClass:\"col-12 pb-5\"},[_c('p',[_vm._v(\" The evaluation of our clustering experiments consisted of multiple stages. \"),_c('strong',[_vm._v(\"First, we perform quantitative analysis\")]),_vm._v(\", producing \"),_c('strong',[_vm._v(\"Within-Cluster Sum of Squares (WCSS)\")]),_vm._v(\", or \\\"Elbow\\\" plots for each combination of featurization type, dimensionality reduction technique, and clustering algorithm (12 combos in total). We refer to these as configuration combinations. Using these plots, we identify \"),_c('strong',[_vm._v(\"a query subset\")]),_vm._v(\" of the combinations to further investigate for our \"),_c('strong',[_vm._v(\"qualitative evaluation\")]),_vm._v(\". In the first stage of our qualitative evaluation we used the \"),_c('strong',[_vm._v(\"english translations of the closest documents to each cluster center\")]),_vm._v(\" to select a configuration combination from the query subset for the \"),_c('strong',[_vm._v(\"final stage of qualitative assessment\")]),_vm._v(\". In the final stage, a \"),_c('strong',[_vm._v(\"fluent Japanese speaker\")]),_vm._v(\" investigated the raw Japanese reports in the clusters made by the selected configuration combination and \"),_c('strong',[_vm._v(\"determined a human-interpretable label to describe the cluster overall\")]),_vm._v(\" for each cluster. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering Pipeline & Experiments\")])]),_c('div',{staticClass:\"col-12 col-md-5 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"clustering-pipeline\",\"src\":require(\"../../../../public/assets/clustering-pipeline.png\"),\"alt\":\"Pipeline for Clustering\"}})]),_c('div',{staticClass:\"col-12 col-md-7 pb-5\"},[_c('h6',[_vm._v(\"Featurize, Reduce Dimensions, and Cluster\")]),_c('p',[_vm._v(\" Using the devised clustering pipeline, we can sequentially reduce the dimensions of the input features to \"),_c('strong',[_vm._v(\"2-dimensions\")]),_vm._v(\", using \"),_c('strong',[_vm._v(\"Principle Component Analysis (PCA)\")]),_vm._v(\", \"),_c('strong',[_vm._v(\"t-distributed Stochastic Neighbor Embedding (t-SNE)\")]),_vm._v(\", or we \"),_c('strong',[_vm._v(\"do not apply dimensionality reduction\")]),_vm._v(\" at all. We finally cluster the reduced data using either \"),_c('strong',[_vm._v(\"K-means\")]),_vm._v(\" or \"),_c('strong',[_vm._v(\"K-medoids\")]),_vm._v(\", which is more robust to outliers present in the data. These hyperparameters to the clustering pipeline are summarized in the neighboring figure. \")]),_c('div',{staticClass:\"row justify-content-center pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"clustering-hyperparameters\",\"src\":require(\"../../../../public/assets/clustering-hyperparameters.png\"),\"alt\":\"Hyperparameters for Clustering\"}})]),_c('h6',[_vm._v(\"Outputs of Clustering Pipeline\")]),_c('p',[_c('strong',[_vm._v(\"Having selected a configuration to use and a K-value to use\")]),_vm._v(\", our clustering pipeline produces the clustered data points, the text associated with the \"),_c('strong',[_vm._v(\"20 closest documents to the cluster center (in JA & EN translations)\")]),_vm._v(\" for each cluster, and the top 20 unigrams (in JA) by TF-IDF score for the document formed from concatenating the documents in a cluster, for each cluster, forming a cluster-level document corpus. We note that our pipeline can take any positive values x & y for displaying the top x unigrams or top y documents in a cluster. \")])]),_c('div',{staticClass:\"col-12 pb-5\"},[_c('h5',[_vm._v(\"Clustering Experiments - Identifying the Query Subset\")]),_c('p',[_c('ol',[_c('li',[_c('p',[_vm._v(\" We investigated all 12 featurization, dimensionality reduction, and clustering algorithm combinations by investigating the corresponding \"),_c('strong',[_vm._v(\"WCSS or Elbow plot\")]),_vm._v(\" for \"),_c('strong',[_vm._v(\"K = 2, , 20 clusters\")])]),_c('p',[_c('strong',[_vm._v(\"WCSS\")]),_vm._v(\" captures extent to which data points within a cluster are at a close distance to each other, ideally we want this to be low, but not too low. Minimizing WCSS is the same as maximizing the distance between data points in different clusters. \")])])])]),_c('div',{staticClass:\"pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"elbow-plot\",\"src\":require(\"../../../../public/assets/elbow-plot.png\"),\"alt\":\"Second slide\"}})]),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"2\"}},[_vm._v(\" Select a subset of combinations which have \"),_c('strong',[_vm._v(\"relatively lower WCSS scores\")]),_vm._v(\" across all K values and \"),_c('strong',[_vm._v(\"have an elbow in the elbow plot\")]),_vm._v(\" to qualitatively investigate further. We call this the \"),_c('strong',[_vm._v(\"query subset\")]),_vm._v(\". \")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering Qualitative Evaluation & Results\")]),_c('h5',[_vm._v(\"Preliminary Qualitative Assessment (in EN): Investigating the Query Subset\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-5\",attrs:{\"id\":\"workflow-and-configs\"}},[_c('h6',[_vm._v(\"Preliminary Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-evaluation-workflow\",\"src\":require(\"../../../../public/assets/preliminary-assessment.png\"),\"alt\":\"Second slide\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Query Subset\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"query-subset\",\"src\":require(\"../../../../public/assets/query-subset.png\"),\"alt\":\"Configuration Combinations in the Query Subset\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Qualitative Summaries of Clusters and Possible Labels\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-summaries\",\"src\":require(\"../../../../public/assets/qualitative-summaries.png\"),\"alt\":\"Qualitative Summaries of Resultant Clustering for Each Cluster\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-1\",attrs:{\"id\":\"workflow\"}},[_c('h6',[_vm._v(\"Preliminary Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-evaluation-workflow\",\"src\":require(\"../../../../public/assets/preliminary-assessment.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-2 pb-md-5\"},[_c('p',[_c('ol',[_c('li',[_c('p',[_c('strong',[_vm._v(\"For each combination in the subset, we determine an elbow\")]),_vm._v(\" from the corresponding elbow plot as the K value to use in our qualitative analysis. \")])])])]),_c('img',{staticClass:\"img-fluid pb-3\",attrs:{\"id\":\"identified-elbow\",\"src\":require(\"../../../../public/assets/identified-elbow.png\"),\"alt\":\"Second slide\"}}),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"2\"}},[_c('p',[_vm._v(\" For each cluster, we investigate the \"),_c('strong',[_vm._v(\"top 20 reports within a cluster which were closest to the cluster center\")]),_vm._v(\". We note that for the \"),_c('strong',[_vm._v(\"preliminary assessment\")]),_vm._v(\", we used \"),_c('strong',[_vm._v(\"English translations of the reports given by DeepL neural translation.\")]),_c('sup',[_c('a',{attrs:{\"href\":\"#fnE\",\"id\":\"refE\"}},[_vm._v(\"1\")])])]),_c('p',[_vm._v(\" When investigating the representative reports in each cluster, we answered the question: \")])])])]),_c('p',{attrs:{\"id\":\"research-question\"}},[_c('strong',[_vm._v(\"When looked at together, does the content of the representative reports in a cluster elicit an interpretable label? If so, what is it?\")])]),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"3\"}},[_c('p',[_vm._v(\" We then identified the configuration combination in the query subset which yielded \"),_c('strong',[_vm._v(\"the most interpretable labels across combinations\")]),_vm._v(\", i.e. selected the combination which had the highest number of clusters that had representative documents which elicited an interpretable label. \")]),_c('p',[_vm._v(\" In the neighboring graphics, we showcase the configuration combinations in the query subset and we report qualitative summaries for each of the cluster configurations. We note that the configuration combination which gave the highest number of clusters which had a coherent, interpretable label (9 in total) was the \"),_c('strong',[_vm._v(\"BERT embedding, t-SNE (2 components), and K-medoids clustering\")]),_vm._v(\" combination, which is \"),_c('strong',[_vm._v(\"bolded\")]),_vm._v(\". \")])])])])]),_c('div',{staticClass:\"col-12 pt-3 pb-5\",attrs:{\"id\":\"configs\"}},[_c('h6',[_vm._v(\"Query Subset\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"query-subset\",\"src\":require(\"../../../../public/assets/query-subset.png\"),\"alt\":\"Configuration Combinations in the Query Subset\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Qualitative Summaries of Clusters and Possible Labels\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-summaries\",\"src\":require(\"../../../../public/assets/qualitative-summaries.png\"),\"alt\":\"Qualitative Summaries of Resultant Clustering for Each Cluster\"}})])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnE\"}},[_vm._v(\"1. \"),_c('a',{attrs:{\"href\":\"https://www.deepl.com/en/translator\",\"target\":\"_blank\"}},[_vm._v(\"Link to DeepL.\")]),_vm._v(\" We acknowledge Saeko Baird of the Urban Risk Lab at MIT who cleaned these translations of their inaccuracies.\"),_c('a',{attrs:{\"href\":\"#refE\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering Qualitative Evaluation & Results\")]),_c('h5',[_vm._v(\"Final Qualitative Assessment (in JA): Investigating the Optimal Configuration Combination determined from the Preliminary Assessment\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-0\",attrs:{\"id\":\"workflow-and-clusters\"}},[_c('h6',[_vm._v(\"Final Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-final-evaluation-workflow\",\"src\":require(\"../../../../public/assets/final-assessment.png\"),\"alt\":\"Final Qualitative Assessment Workflow\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Unlabeled Clusters\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"unlabled-clusters-img\",\"src\":require(\"../../../../public/assets/unlabeled-clusters.png\"),\"alt\":\"Unlabeled Clusters\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-1\",attrs:{\"id\":\"final-assessment-workflow\"}},[_c('h6',[_vm._v(\"Final Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-final-evaluation-workflow\",\"src\":require(\"../../../../public/assets/final-assessment.png\"),\"alt\":\"Final Qualitative Assessment Workflow\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-2\",attrs:{\"id\":\"steps-with-unlabeled-clusters\"}},[_c('p',[_c('ol',[_c('li',[_c('p',[_vm._v(\" Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \"),_c('strong',[_vm._v(\"top 20 closest reports\")]),_vm._v(\" within the cluster to the cluster center & the \"),_c('strong',[_vm._v(\"top 20 unigrams in by TF-IDF score for cluster-level document corpus\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters. \")])])])]),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('h6',[_vm._v(\"Unlabeled Clusters\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"unlabeled-clusters-img\",\"src\":require(\"../../../../public/assets/unlabeled-clusters.png\"),\"alt\":\"Unlabeled Clusters\"}})]),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"2\"}},[_c('p',[_vm._v(\" Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnF\",\"id\":\"refF\"}},[_vm._v(\"1\")])]),_vm._v(\" investigated each cluster and assigned an interpretable label to it. \")]),_c('p',[_vm._v(\" The interpretable label given for each cluster is depicted in the figure below: \")])])])]),_c('div',{staticClass:\"col-12\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"cluster-labels\",\"src\":require(\"../../../../public/assets/cluster-labels.png\"),\"alt\":\"Cluster Labels\"}})])]),_c('div',{staticClass:\"col-12 col-md-6\",attrs:{\"id\":\"steps-without-unlabeled-clusters\"}},[_c('p',[_c('ol',[_c('li',[_c('p',[_vm._v(\" Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \"),_c('strong',[_vm._v(\"top 20 closest reports\")]),_vm._v(\" within the cluster to the cluster center & the \"),_c('strong',[_vm._v(\"top 20 unigrams in by TF-IDF score for cluster-level document corpus\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters. \")])]),_c('li',[_c('p',[_vm._v(\" Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnF\",\"id\":\"refF\"}},[_vm._v(\"1\")])]),_vm._v(\" investigated each cluster and assigned an interpretable label to it. \")]),_c('p',[_vm._v(\" The interpretable label given for each cluster is depicted in the figure below: \")])])])]),_c('div',{staticClass:\"col-12\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"cluster-labels\",\"src\":require(\"../../../../public/assets/cluster-labels.png\"),\"alt\":\"Cluster Labels\"}})])]),_c('div',{staticClass:\"col-12 col-md-7 pb-3\",attrs:{\"id\":\"labeled-clusters\"}},[_c('p',{attrs:{\"id\":\"process-arrow\"}},[_vm._v(\"\")]),_c('h6',[_vm._v(\"Labeled Clusters\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"labeled-clusters-img\",\"src\":require(\"../../../../public/assets/labeled-clusters.png\"),\"alt\":\"Labeled Clusters\"}})])]),_c('p',[_c('sup',{attrs:{\"id\":\"fnF\"}},[_vm._v(\"1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who assigned an interpretable label to each cluster.\"),_c('a',{attrs:{\"href\":\"#refF\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"carousel-text col-12 col-md-7\"},[_c('h5',[_vm._v(\"Clustering Firefighter Flood Crisis Reports - Discussion\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Suggests what is deemed important to report during flood crisis by FC firefighters on-the-ground. \")]),_c('li',[_vm._v(\" Since we applied the clustering on-the-ground firefighter reports, these results can be used to devise \"),_c('strong',[_vm._v(\"classification tasks with labels which better embed the information needs of EOC\")]),_vm._v(\" & can be cross-referenced with EOC. \")]),_c('li',[_vm._v(\" Experiment can also be applied on Japanese RiskMap reports or crisis tweets to see if similar cluster labels are unveiled by resident reporting. \")]),_c('li',[_vm._v(\" This method has the \"),_c('strong',[_vm._v(\"drawback of permitting data points to be in only one cluster (hard clustering)\")]),_vm._v(\", and we observed that some of the data points have content which is indicative of multiple of the unveiled interpretable labels. \")])])])])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-8\">  \n            <h3>Text Analysis Module</h3>\n        </div>\n        <div class=\"col-md-10\">\n            <div id=\"TextAnalysisCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"2\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"3\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"4\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"5\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"6\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"7\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"8\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"9\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"10\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"11\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"12\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"13\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"14\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"15\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"16\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"17\"></li>\n                    <li data-target=\"#TextAnalysisCarousel\" data-slide-to=\"18\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item cc-carousel-item active\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12 col-md-7 pt-3 pb-2\">\n                                <img id=\"text-analysis-module\" class=\"img-fluid\" src=\"../../../../public/assets/text-analysis-module.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"carousel-text col-12\">\n                                <h5>Text Analysis Methodology Overview</h5>\n                                <p> \n                                    The Text Analysis Module aims to provide accurate and efficient classifications of\n                                    crisis reports using the text modality that is often present in the reports. Another aim\n                                    was to incorporate the insights we gained from the results of our qualitative analysis\n                                    on the Image Analysis Module that were transferable\n                                    between the data modalities, i.e. the importance of identifying potential for human\n                                    casualty or risk to humans. We did this to exemplify our frameworks intention of\n                                    producing iteratively developed ML methodologies and AI systems to enhance crisis\n                                    awareness and response using insights gained from crisis managers.\n                                </p>\n                                <h6>Incorporating Crisis Expert Insights into Model Development & Performance Metric Selection</h6>\n                                <p>\n                                    To incorporate the insights of the crisis managers into the design and development\n                                    of a new text classification model, we first created a classification task and associated\n                                    classes that align with the expressed information needs of crisis managers during a\n                                    crisis event. Then, we selected a performance evaluation metric that aligns with\n                                    priorities of the crisis managers for that task, finally developing a model that is\n                                    evaluated using the selected performance metric.\n                                </p>\n                                <h6>Human Risk Text Classification Experiments</h6>\n                                <p>\n                                    In the process of conducting this exercise, we performed various classification \n                                    experiments, experimenting with various text featurizations, classical machine learning\n                                    algorithms, and importantly, we deliberated on the selection of a performance \n                                    evaluation metric based on our findings from the qualitative analysis of the image annotation\n                                    workshops.\n                                </p>\n                                <h6>Preprocessing & Featurization of Japanese Text</h6>\n                                <p>\n                                    We note that since this study focused exclusively on Japanese crisis text, we\n                                    constructed a preprocessing pipeline that uses open-source Japanese tokenizers, \n                                    stop-words, and a lemmatizer to preprocess the Japanese text. Additionally, we \n                                    investigated the use of text embeddings of the Japanese crisis text that are created by\n                                    applying CLS pooling, a process which creates a contextualized numerical embedding\n                                    of inputted text, using a pretrained Japanese Masked Language Modeling (MLM)\n                                    BERT model in both our supervised and unsupervised learning experiments.\n                                </p>\n                                <h6>Clustering Data to Uncover Semantically-similar Groupings</h6>\n                                <p>\n                                    Finally, we conclude the development of this ML module on an exploratory note,\n                                    devising a pipeline that evaluates a combination of text featurizations, dimensionality\n                                    reduction techniques, and clustering algorithms to provide intuitive groupings of text\n                                    to help inform the development of text classification tasks in future work.\n                                </p>\n                                <h6>Evaluation</h6>\n                                <p>\n                                    In our evaluation of our text classification experiments, we perform quantitative\n                                    evaluation, assessing the performance of the model for the task based on the \n                                    determined evaluation metric mentioned above in addition to other metrics, e.g. per-class\n                                    performance metrics. For our unsupervised experiments, we include both \n                                    quantitative and qualitative evaluation. Using the Within-Cluster Sum of Squares (WCSS)\n                                    metric, we determine a set of optimal clustering pipeline configurations and their\n                                    corresponding optimal number of clusters to use for further investigation. We assess\n                                    qualitatively by investigating the resulting clusters and determining for each cluster,\n                                    whether or not the representative documents within that cluster have a cohesive,\n                                    interpretable label, and if they do, what that label is.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"carousel-text col-12\">\n                                <h5>Summary of Results</h5>\n                                <h5>Iterating on ML Methodology based on Crisis Managers Insights</h5>\n                                <p>\n                                    Our framework was developed to both highlight the importance of involving crisis\n                                    managers in the process of developing a ML methodology and contextualize model\n                                    performance among other measures of efficacy for the ML methodology. Since our\n                                    framework seeks to be used in the development and iteration of an ML methodology\n                                    based on the insights gained from crisis managers, we iterated on the Text Analysis\n                                    Module using insights we had gained from our results on the Image Analysis Module.\n                                </p>\n                                <h5>Developing the Human Risk Task</h5>\n                                <p>\n                                    Using labels provided directly to us by crisis managers, we created a new text\n                                    classification task in an effort to better fulfill their information needs during a crisis\n                                    event. Using insights gained from the results of image annotation workshops of the\n                                    Image Analysis Module, we determined F2 score to be an appropriate\n                                    performance metric for model performance evaluation as false negatives are more\n                                    costly than false positives for assessing human risk from text reports. <strong>To the best\n                                    of our knowledge, the exercises of creating a classification task from labels provided\n                                    directly by EOC and formulating an appropriate model performance metric informed\n                                    from crisis expert insights are novel contributions of this work.</strong> These exercises follow\n                                    directly inline with our framework, <strong>using the results\n                                    from the Image Analysis Module to iteratively design and develop ML models for the\n                                    Text Analysis Module.</strong>\n                                </p>\n                                <h5>Human Risk Task Model Evaluation</h5>\n                                <p>\n                                    Using F2 as the metric to optimize for during 5 x 5 Nested CV, we were able to\n                                    identify the SVM algorithm and its corresponding hyperparameter grid as achieving a\n                                    relatively high mean F2 performance with low variance. To assess the models ability\n                                    to perform the human risk classification task, we found the tuned SVM model to\n                                    achieve an F2 score of 92.8%, which is a substantial improvement over the baseline\n                                    models F2 score of 43.4%. Having a baseline is an important aspect of our framework\n                                    as it enables us the ability to determine if a developed model is performing the task\n                                    well, i.e. if it does not perform the task better than the baseline, it is not a useful\n                                    model for the task. This suggests the tuned SVM model is a useful classifier for\n                                    the task and performs the task reasonably well. This is further evidenced from the\n                                    Precision-Recall curve, where the tuned SVM model achieved an AUCPR score of\n                                    0.919, which is a significant improvement over the typical baseline classifier used for\n                                    that metric which achieves an AUCPR of 0.133. We note that recall for the \"Human\n                                    Risk\" class is higher than precision likely being a result of using F2 as the performance\n                                    metric to optimize in the classification experiments. Lastly, when looking at the per-\n                                    class performance metrics for each class, we see that the model performs reasonably\n                                    well on both classes achieving scores at or above 0.857 for the \"Human Risk\" class\n                                    and at or above 0.976 for the \"Not Human Risk\" class.\n                                </p>\n                                <h5>Clustering of Firefighter Crisis Text Reports</h5>\n                                <p>\n                                    From our preliminary clustering assessments, we observed that clustering using\n                                    K-medoids rather than K-means with all else equal (i.e. text featurization and dimensionality reduction technique), \n                                    typically yielded lower WCSS scores across all  values between 2-20. This is likely due to the K-medoids algorithms robustness to\n                                    outliers and noise, suggesting that there may exist some reports in the corpus which\n                                    are quite different from the rest.\n                                </p>\n                                <p>\n                                    We note that since the corpus we studied was specific to flood and typhoon crisis events, \n                                    it is no surprise that many of the identified cluster labels are geared\n                                    towards flood-related information such as \"Areas with Flood Risk\", \"River Water\n                                    Level and Corresponding Warning for EOC/FD\", \"Residential Areas/Buildings in\n                                    Flood (Risk)\", and \"Landslide/Fallen Tree\". Although some of the categories are\n                                    quite general such as \"Rescue (Activities/Requests)\", \"Closed Roads by the City\",\n                                    and \"Impassable Roads (due to Flood/Obstacles/Damage)\", we also see that some\n                                    of the cluster labels are specific to the fire department such as \"Areas where FD is\n                                    active\" and \"FD Activities/Weather Warning/Flood Control Alert\".\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Fukuchiyama Flood Text Reports Data Collection</h5>\n                        <div class=\"row align-items-center justify-content-center\">\n                            <div class=\"col-12 col-md-4 pt-1 pb-md-5\">\n                                <img id=\"text-data-collection\" class=\"img-fluid\" src=\"../../../../public/assets/txt-data-collection.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 pt-2 pb-4 col-md-6\">\n                                <p>\n                                    Our crisis management partners in Fukuchiyama City (FC) compiled <strong>716 Japanese (JA) text transcripts</strong>\n                                    of radio communications from <strong>on-the-ground firefighters</strong> which occurred during the following past FC flood events:\n                                    <ul>\n                                        <li>Typhoon Manyi in 2013</li>\n                                        <li>Heavy Rain Event in August 2014</li>\n                                        <li>Typhoon Lan in 2017</li>\n                                        <li>Heavy Rain Event in July 2018</li>\n                                    </ul>\n                                    The data collection process for during these events is depicted in the neighbouring figure.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Fukuchiyama Flood Text Reports Dataset Characteristics</h5>\n                        <div class=\"row align-items-center justify-content-center\">\n                            <div class=\"col-12 col-md-7 pt-3 pb-2\">\n                                <table id=\"text-characteristics-table\">\n                                        <tr>\n                                            <th><strong>Total Reports</strong></th>\n                                            <th><strong>Reports Labeled for Human Risk</strong></th>\n                                            <th><strong>Reports Labeled for Emergency Operation Center (EOC) Humanitarian Categories</strong></th>\n                                            <th><strong>Unique EOC Humanitarian Categories</strong></th>\n                                        </tr>\n                                        <tr>\n                                            <td>716</td>\n                                            <td>715</td>\n                                            <td>584</td>\n                                            <td>108</td>\n                                        </tr>\n                                    </table>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    We use all 716 reports in our clustering experiments. Since 715 out of the total 716 reports are labeled for Human Risk, we \n                                    use those labeled reports in our classification experiments. \n                                    <br>\n                                    <br>\n                                    To understand how these firefighter crisis text reports compare to other Japanese crisis reports, we compare the FC firefighter reports character length distribution against another Japanese\n                                    crisis text report corpus, the text of Tokyo crisis reports received by RiskMap (RM)\n                                    during Typhoon Hagibis in 2019. We note that there are 68 reports in total for the\n                                    Typhoon Hagibis RM reports dataset. \n                                </p>\n                            </div>\n                            <div class=\"col-10\">\n                                <img id=\"dataset-comparison\" src=\"../../../../public/assets/character_box_and_whisk_fc_rm.png\" class=\"img-fluid\" alt=\"First slide\">\n                            </div>\n                            <div class=\"col-6 col-lg-4 pt-3\">\n                                <h6>FC Firefighter Reports Characteristics</h6>\n                                <div>\n                                    <p>\n                                        <ul>\n                                            <li>N = 716 Reports</li>\n                                            <li>Median = 22 Characters</li>\n                                        </ul> \n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-6 col-lg-4 pt-3\">\n                                <h6>Typhoon Hagibis RM Reports Characteristics</h6>\n                                <p id=\"hagibis-details\">\n                                    <ul>\n                                        <li>N = 68 Reports</li>\n                                        <li>Median = 14 Characters</li>\n                                    </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    Most Japanese (JA) crisis text reports are between only a <strong>few characters</strong> to about <strong>50 characters</strong>. We also observe that each distribution is right-skewed. This is further seen by Twitter research, which \n                                    finds that <strong>JA tweets</strong> have a mode of <strong>15 characters</strong>, with a character distribution exhibiting right-skew. It is noted\n                                    that <strong>English (EN) tweets</strong> have a mode of <strong>34 characters</strong>, which as the authors state,\n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <h6>\n                                    This is because in languages like Japanese, Korean, \n                                    and Chinese you can convey about double the amount of \n                                    information in one character as you can in many other languages, \n                                    like English, Spanish, Portuguese, or French\"<sup><a href=\"#fnA\" id=\"refA\">1</a></sup>\n                                </h6>\n                            </div>\n                            <div class=\"carousel-text col-12\">\n                                <p>\n                                    <strong>Comparing these various dataset distributions suggests that the FC firefighter text reports are of similar character length to RM JA reports and JA tweets.</strong>\n                                </p>\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnA\">1. A. Rosen and I. Ihara, Giving you more characters to express yourself. <a href=\"https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself\" target=\"_blank\">https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself</a>, Sept. 2017.<a href=\"#refA\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Text Preprocessing & Featurization Pipeline</h5>\n                        <div class=\"row justify-content-center\">\n                            <div class=\"col-12 col-md-9 pb-2\">\n                                <img id=\"text-pipeline\" class=\"img-fluid\" src=\"../../../../public/assets/text-preprocessing.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"carousel-text col-12\">\n                                <p>\n                                    In order to use the FC firefighter report text data in our classification and clustering experiments, we needed to featurize, or construct\n                                    numerical representations (feature vectors) of the text to use as input to a ML model. In the pipeline we developed for the featurization of Japanese text, <strong>we investigate 4 different featurizations of the text</strong>, namely,\n                                    <strong>Bag-of-Words (BOW)</strong> based on unigram & bigram representations, <strong>Term-Frequency Inverse-Document-Frequency (TF-IDF)</strong> based on unigram, and finally <strong>pretrained BERT embeddings using CLS Pooling</strong>.\n                                </p>\n                                <p>\n                                    Depending on the featurization, we integrate various preprocessing steps in order to perform commonplace Natural Language Processing (NLP) preprocessing\n                                    steps. Due to our limited knowledge of the Japanese language, we make use of popular tokenizers & a lemmatizer pretrained on Japanese text as well as an open-source Japanese stopwords list for preprocessing the JA text.\n                                </p>\n                                <p>\n                                    In the following slide we describe the preprocessing and featurization steps of our pipeline as applied to the input data in order to yield the featurizations we have mentioned. Additionally,\n                                    we note the associated pros and cons of the featurizations.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Text Preprocessing & Featurization Pipeline</h5>\n                        <h5><u>BOW & TF-IDF Preprocessing & Featurization</u></h5>\n                        <div class=\"col-12 pb-2 pt-2\">\n                            <img id=\"n-gram-preprocessing\" class=\"img-fluid\" src=\"../../../../public/assets/n-gram-preprocessing.png\" alt=\"Second slide\">\n                        </div>\n                        <div class=\"row justify-content-center\">\n                            <div class=\"col-12\">\n                                <h6>Preprocessing</h6>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    For the BOW based on unigram, BOW based on bigram, and TF-IDF (based on unigram) featurizations, we leverage a popular, open-source JA tokenizer,\n                                    stopwords list, and lemmatizer to <strong>preprocess the raw input text</strong><sup><a href=\"#fnB\" id=\"refB\">1</a></sup>, these steps can be seen in the figure above:\n                                    <ol>\n                                        <li>\n                                            Break up raw report text into <strong>word tokens</strong>, i.e. tokenize:\n                                            <ul>\n                                                <li>E.g. the road is submerged.  [the, road, is, submerged, .]</li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            We remove stopwords (e.g. the, as, it, is, .)  otherwise could add noise to the input:\n                                            <ul>\n                                                <li>E.g. [<strong>the</strong>, road, <strong>is</strong>, submerged, .]  [road, submerged]</li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            We <strong>lemmatize</strong> word tokens, i.e. convert word to its lemma, or dictionary form:\n                                            <ul>\n                                                <li>E.g. [road, <strong>submerged</strong>]  [road, <strong>submerge</strong>] </li>\n                                            </ul>\n                                        </li>\n                                    </ol>\n                                </p>\n                            </div>\n                            <h6>Featurization</h6>\n                            <div class=\"col-12\">\n                                <p>\n                                    After preprocessing the raw text, we then use the preprocessed input to form an n-gram representation, which in our work was limited to unigram and bigram representations, but we note our pipeline generalizes to produce n-gram representations. For example, \n                                    the unigram representation of the processed example used above [\"road\", \"submerge\"] would be\n                                    as [\"road\", \"submerge\"] and the bigram representation would be [\"road submerge\"]. Once the n-gram representation is computed from the preprocessed input, we convert the preprocessed word tokens into the BOW or TF-IDF feature vector representations, or featurizations, which can be used as inputs to ML models.\n                                    The values of the <strong>BOW n-gram features</strong> are simply their associated <strong>frequency</strong> in a text report. We show the resulting feature vector for BOW based on unigram for the [\"road\", \"submerge\"] example below:\n                                </p> \n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"bow-unigram-ex\" class=\"img-fluid\" src=\"../../../../public/assets/bag-of-word-features.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    For the TF-IDF featurizations based on unigrams, the feature values are computed by considering both the <strong>frequency</strong> of the unigrams in the report as well as the <strong>occurence\n                                    of the unigram across all reports</strong>. This value gives a relative importance to a unigram that considers the unigram in a specific report and across all reports. We show the resulting feature vector for TF-IDF based on unigram for the [\"road\", \"submerge\"] example below as if it were part of a collection of reports, or a text corpus:\n                                </p>\n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"tfidf-unigram-ex\" class=\"img-fluid\" src=\"../../../../public/assets/tfidf-features.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-7 col-md-3 pt-3 pl-md-5\">\n                                <h6><u>Pros:</u></h6>\n                                <div>\n                                    <p>\n                                        <ul>\n                                            <li>Interpretable</li>\n                                            <li><strong>Language-agnostic</strong></li>\n                                        </ul>\n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-lg-4 pt-3\">\n                                <h6><u>Cons:</u></h6>\n                                <p>\n                                    <ul>\n                                        <li>Sparse (many 0's) & High-Dimensional</li>\n                                        <li>Doesn't do well for <strong>Out-of-Vocab (OOV)</strong> word tokens</li>\n                                        <li><strong>Language-agnostic</strong> (i.e. inability to capture specificity to a particular language)</li>\n                                        <li><strong>Severely limited ability</strong> to capture <strong>token similarity, long-range dependencies, and understanding of a language</strong></li>\n                                    </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    Although these n-gram-based featurizations have the benefit of being language-agnostic, we note that they have the limitations\n                                    of being high-dimensional and sparse in which most entries of the feature vector are zero, an inability to model long-range dependencies between tokens in the context of a\n                                    document, and a severely limited ability to capture token similarity and understanding of a language. Therefore, we investigate a featurization strategy that yields\n                                    dense, contextualized document representations specific to Japanese text documents. This strategy uses a pretrained Japanese Masked Language Modeling (MLM) Bidirectional Encoder Representations from Transformers (BERT) model and the CLS pooling\n                                    technique.\n                                </p>\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnB\">1. We note that in this work we make use of the <a href=\"https://pypi.org/project/fugashi/1.1.2/\" target=\"_blank\">fugashi</a> (version: 1.1.2) open-source morphological tool for <strong>tokenizing and lemmatizing</strong> Japanese text. Since fugashi requires a dictionary to operate, we use the <a href=\"https://pypi.org/project/unidic-lite/1.0.8/\" target=\"_blank\">Unidic Lite</a> dictionary (version: 1.0.8). Finally, for the stopwords list, we use a versioned, open-source list available <a href=\"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/5a000f6a62f9e3a12f436f36d168e2fcd2fb1878/stopwords-ja.json\" target=\"_blank\">here</a>.<a href=\"#refB\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Text Preprocessing & Featurization Pipeline</h5>\n                        <h5><u>Pretrained Japanese MLM BERT Model Embeddings Preprocessing & Featurization</u></h5>\n                        <div class=\"col-12 pb-2 pt-2\">\n                                <img id=\"bert-features-preprocessing\" class=\"img-fluid\" src=\"../../../../public/assets/bert-preprocessing.png\" alt=\"Second slide\">\n                            </div>\n                        <div class=\"row justify-content-center\">\n                            <div class=\"col-12\">\n                                <h6>Preprocessing</h6>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    The BERT MLM Deep learning (NN) model is optimized to <strong>predict</strong> a randomly <strong>masked words</strong> by using the <strong>context</strong>, \n                                    or the words that surround them. \n                                </p>\n                                <p>\n                                    Tohoku University Researchers <strong>pretrained a MLM BERT</strong> model using a dataset of approx. <strong>30M sentences</strong> of the \n                                    <strong>Japanese version of Wikipedia</strong>. \n                                </p>\n                                <p>\n                                    Raw text data is word tokenized using Fugashi & Unidic Lite library. Word tokens are further split into subwords \n                                    using the <strong>WordPiece algorithm</strong>, yielding a token vocabulary of <strong>32768 unique tokens.</strong><sup><a href=\"#fnC\" id=\"refC\">1</a></sup>\n                                </p>\n                            </div>\n                            <h6>Featurization</h6>\n                            <div class=\"col-12\">\n                                <p>\n                                    The text input tokens from resulting from the preprocessing steps are prepended with a <strong>[CLS]</strong> token, \n                                    the Classification token. Deep in the BERT model, we extract a <strong>contextualized numerical \n                                    representation</strong> of the report for classification tasks by grabbing the <strong>final hidden state</strong> \n                                    corresponding to this token. This is <strong><i>CLS Pooling</i></strong>, yielding a dense, contextualized feature vector of 768 dimensions. \n                                    We refer to these feature vectors as BERT embeddings. We show an example of a featurized text report below:\n                                </p> \n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"bert-features-ex\" class=\"img-fluid\" src=\"../../../../public/assets/bert-features.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-5 pt-3 pl-md-5\">\n                                <h6><u>Pros:</u></h6>\n                                <div>\n                                    <p>\n                                        <ul>\n                                            <li><strong>Optimized for the Japanese language</strong></li>\n                                            <li><strong>Dense features </strong> (i.e. lower dimensions than previous n-gram based features & not sparse)</li>\n                                            <li>\n                                                <strong>Contextualized representations</strong>, which can better capture long-range dependencies between words in a \n                                                report & State-of-the-art <strong>language understanding</strong>\n                                            </li>\n                                            <li>\n                                                Better-equipped to handle <strong>OOV tokens</strong> due to the use of the WordPiece algorithm\n                                            </li>\n                                        </ul>\n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-lg-4 pt-3\">\n                                <h6><u>Cons:</u></h6>\n                                <p>\n                                    <ul>\n                                        <li>Features are <strong>NOT interpretable</strong></li>\n                                        <li><strong>Specific to JA</strong></li>\n                                        <li><strong>Not finetuned to crisis text corpus</strong>, although we note this could be done in a future work</li>\n                                    </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    Having preprocessed the text and produced various featurizations with various pros and cons, we utilized all of the featurizations in our \n                                    classification experiments discussed in the next slides and TF-IDF based on unigram features as well as BERT embeddings in our clustering experiments.\n                                </p>\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnC\">1. <a href=\"https://huggingface.co/cl-tohoku/bert-base-japanese-v2\" target=\"_blank\">Link to Pretrained Japanese BERT MLM Model</a><a href=\"#refC\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Human Risk Text Classification</h5>\n                        <div class=\"row justify-content-center\">\n                            <div class=\"col-12\">\n                                <p>\n                                    Since 715 out of the 716 firefighter text reports were labeled for the binary <strong>Human Risk/No Human Risk classes</strong>, we chose to focus our text classification\n                                    experiments on this classification task. Additional with the text analysis module, we aimed to develop a task that <strong>better met the information needs</strong> of crisis managers during\n                                    a crisis event. We did this by using the labels our crisis management partners in Fukuchiyama provided to us directly.\n                                </p>\n                            </div>\n                            <h6>Task Description</h6>\n                            <div class=\"col-12\">\n                                <p>\n                                    <i>\n                                        The Human Risk text classification task \n                                        <strong>determines whether or not a crisis text report indicates if \n                                        there are people in need of rescue from a crisis.</strong> This includes people \n                                        being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), \n                                        and/or being in need of life-saving emergency medical care.\n                                    </i>\n                                </p>\n                                <p>\n                                    In an effort to transition from abstract class descriptions to something more specific, e.g. a checklist, we choose to both provide a definition of the task and further detail the Human Risk classes as \n                                    bulleted lists containing the specific traits/descriptors contained in a text report which are characteristic of each class with the aim of enhancing the clarity of each class and the task overall:\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7 pt-3 pl-md-5\">\n                                <h6><strong>Human Risk</strong> Class Descriptors</h6>\n                                <div>\n                                    <p>\n                                        <ul>\n                                            <li>Rescue being requested (to the Fire Department (FD))</li>\n                                            <li>Evacuation support being requested (to the FD)</li>\n                                            <li>Human missed the chance to evacuate from their own house, at work, shopping center, etc.</li>\n                                            <li>Vulnerable population (elderly, disabled, small children) being left in the house in the flooding area</li>\n                                            <li>Water rising inside the house above the floor (human inside)</li>\n                                            <li>Water current is fast inside the house and hard to move upstairs (human inside)\n                                            <li>Sediment flowing into the house (human inside)</li>\n                                            <li>Human being trapped in elevator, submerged car, or a car which is not submerged yet</li>\n                                            <li>Human being washed away in a river</li>\n                                            <li>Rescue team dispatched</li>\n                                            <li>Rescue team in activity (such as helping evacuation, rescuing, etc.)</li>\n                                            <li>Rescue activity completed</li>\n                                            <li>Landslide occurrence on the highway - possible vehicle being involved</li>\n                                        </ul>\n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-4 pt-3\">\n                                <h6><strong>No Human Risk</strong> Class Descriptors</h6>\n                                <p>\n                                    <ul>\n                                        <li>Dam Discharge</li>\n                                        <li>Meteorological Information</li>\n                                        <li>River Water Level Information</li>\n                                        <li>Weather Alert</li>\n                                        <li>Road Closure</li>\n                                        <li>Road Flood Risk (not flooded yet)</li>\n                                        <li>Area Flood Risk (not flooded yet)</li>\n                                    </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"human-risk-classification\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-diagram.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    In addition to using labels/classes which better meet the information needs of crisis managers during crisis, \n                                    we aimed to develop the classification model for this task <strong>using a performance metric which better aligns with the priorities of the crisis managers</strong>\n                                    as it pertains to this task and also <strong>considers the nature of the data (i.e. class imbalance)</strong>. We discuss these considerations and how we <strong>determined the\n                                    performance metric</strong> for this task in the next slides. We note that <strong>this process</strong> of constructing a task from <strong>crisis manager's labels</strong> to determining the appropriate \n                                    performance metric to utilize for developing a model which considers both the <strong>properties of the data</strong> and the <strong>insights we gained from crisis managers</strong> is a <strong>novel contribution of this research.</strong>\n                                </p>\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnD\">1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who determined these class definitions from examining the original Japanese reports.<a href=\"#refD\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <h5>Human Risk Classification - Determination of the Performance Evaluation Metric</h5>\n                            <div class=\"col-12\">\n                                <p>\n                                    In our determination of the performance metric to use for assessing the performance of the Human Risk classifier we consider both the class imbalance of the task\n                                    and insights we gained from the workshops conducted in the image analysis module.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-md-5\">\n                                <img id=\"human-riks-label-distribution\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-label-distribution.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 pt-3 col-md-6\">\n                                <h5>Substantial Class Imbalance</h5>\n                                <p>\n                                   <ul>\n                                        <li>\n                                            Across the 715 reports labeled for Human Risk, we observe that there are disporportionately more \n                                            \"No Human Risk\" data points than \"Human Risk\" data points, thus the <strong>\"Human Risk\" class is the minority class</strong> for this task.\n                                        </li>\n                                        <li>\n                                            <strong>Accuracy</strong> of the classifier which always predicts \"No Human Risk\": <strong>86.7%</strong>, i.e. the percentage of\n                                            \"No Human Risk\" labels in the dataset \n                                        </li>\n                                        <li>\n                                            Need to account for this imbalance in the metric, otherwise conclusions about the model's ability to perform the task can be\n                                            misleading, as seen with using accuracy as the performance metric\n                                        </li>\n                                   </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-md-3 pb-md-5\">\n                                <img id=\"insights-from-workshops\" class=\"img-fluid\" src=\"../../../../public/assets/workshop-insights.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"carousel-text col-12 col-md-6\">\n                                <h5>Insights from Image Annotation Workshops</h5>\n                                <p>\n                                   <ul>\n                                        <li>\n                                            From the workshops, we understand that in assessing the potential for human casualities, the cost associated with \n                                            <strong>NOT investigating the potential for human casualities when there ARE human casualities (False Negative (FN))</strong> is considered <strong>higher</strong> than the cost\n                                            of <strong>investigating potential human casualities when there ARE NONE (False Positive (FP))</strong>  <strong>Performance on \"Human Risk\" class is paramount</strong>.\n                                        </li>\n                                        <li>\n                                            <strong>Accuracy</strong> does NOT tell us how well the model performs on the <strong>\"Human Risk\"</strong> class \n                                            <strong> Although Recall (Minimizing FNs)</strong> & <strong>Precision (Minimizing FPs)</strong> have different priorities, both focus\n                                            on the performance of the \"Human Risk\" class\n                                        </li>\n                                        <li>\n                                            Ideally, we'd like to minimize both FN & FP, which is captured in the <strong>F1 score</strong>, however F1 score treats recall as equally as important as precision\n                                        </li>\n                                        <li>\n                                            <strong>F2 score</strong> treats recall as 2x as important as precision, i.e. <strong>the relative cost of FN \n                                                is twice as much as the cost of FP</strong>\n                                        </li>\n                                   </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12 pb-5\">\n                                <p>\n                                     We thus used the <strong>properties of the data</strong> (i.e. class imbalance) & \n                                    <strong>the insights we gained from crisis experts</strong> to determine the performance metric to evaluate the model we develop for the Human Risk task \n                                     the <strong>F2 score</strong>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <h5>Human Risk Classification - Data Splits & Algorithm Selection</h5>\n                            <div class=\"col-12 col-md-6 pt-3\">\n                                <h5>Train/Test Splits</h5>\n                                <p>\n                                    We split the full dataset of 715 reports into non-overlapping train and test splits in percentages of 80%/20%, respectively. Additionally, we preserve\n                                    the class imbalance using stratified splitting. \n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-4 pt-3 pb-md-5\">\n                                <img id=\"data-splitting\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-data-splits.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12\">\n                                <h5>Nested Cross Validation for Algorithm Selection</h5>\n                                <p>\n                                    We were interested in investigating multiple ML algorithms for the Human Risk classification task, each with their own set of tunable hyperparameters. We aimed to determine which\n                                    algorithm paired with a corresponding hyperparameter grid search procedure (e.g. Grid Search), i.e. fitting a model to each unique hyperparameter combination in the grid, had the best estimated generalization\n                                    performance and use that algorithm for the final model evaluation. We note that since we had insufficient data to use train/dev/test splits, we used a variation of K-fold Cross Validation (CV).\n                                </p>\n                                <p>\n                                    Since using the same K-fold CV procedure\n                                    for both performing hyperparameter tuning and estimating generalization performance can yield an estimated generalization performance that is biased and overly-optimistic, we elected\n                                    to use <strong>Nested CV</strong>. Nested CV is typically inpractical in large data settings as it is substantially more computationally expensive to perform as compared to K-fold CV; for our low-data setting it was feasible to use. Nested CV is a useful variation of CV as it mitigates the bias in the\n                                    generalization performance estimate of the algorithm and its corresponding search procedure by nesting the hyperparameter optimization within the generalization performance estimation procedure. \n                                </p>\n                                <p>\n                                    For the algorithm selection procedure, we investigated the following classification algorithms:\n                                </p>\n                            </div>\n                            <div class=\"col-8 col-md-4\">\n                                <p>\n                                    <ul>\n                                        <li>Logistic Regression</li>\n                                        <li>Decision Tree</li>\n                                        <li>Random Forest</li>\n                                        <li>Support Vector Machine</li>\n                                        <li>Multinomial Naive Bayes</li>\n                                        <li>K-Nearest Neighbors</li>\n                                    </ul>\n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    We note that we perform 5 x 5 Nested CV on the train split data & treat the various text featurizations offered by our featurization pipeline as hyperparameters in the hyperparameter grid for each\n                                    algorithm.\n                                </p>\n                                <p>\n                                    For the model evaluation on the test set, we select the algorithm (and corresponding search procedure) which \n                                    had the highest relative mean F2 score with the lowest variance across fold (low standard deviation) from the nested CV procedure.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-8 pt-3 pb-5\">\n                                <img id=\"nested-cv\" class=\"img-fluid\" src=\"../../../../public/assets/nested-cv.png\" alt=\"Second slide\">\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Human Risk Classification</h5>\n                                <h5>Algorithm Selection Results</h5>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-md-5\">\n                                <img id=\"algo-selection-table\" class=\"img-fluid\" src=\"../../../../public/assets/nested-cv-table.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-md-5 pb-3\">\n                                <img id=\"algo-selection-graph\" class=\"img-fluid\" src=\"../../../../public/assets/nested-cv-graph.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    The results presented above are determined from the generalization performance estimation found from \n                                    the performance (by F2 score) on the outer loop 5-fold CV in Nested CV. We make available the intermediate and final results of Nested CV for each algorithm and\n                                    corresponding hyperparameter grid.<sup><a href=\"#fnD\" id=\"refD\">1</a></sup>\n                                </p>\n                                <p>\n                                    From the results, we determined that the performance of the <strong>Support Vector Machine (SVM)</strong> algorithm with its\n                                    corresponding hyperparameter search procedure yielded the highest mean F2 score,\n                                    82.0%, and the lowest standard deviation, 4.22%. We therefore select the Support\n                                    Vector Machine (SVM) algorithm and its corresponding hyperparameter grid for the\n                                    final human risk model evaluation on the test set.\n                                </p>\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnD\">1. <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module/Classification/Nested%20CV\" target=\"_blank\">Link to Results & Hyperparameters of Nested CV</a><a href=\"#refD\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Human Risk Classification</h5>\n                                <h5>Final Model Evaluation</h5>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-3\">\n                                <img id=\"model-evalution-diagram\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-model-evaluation.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12\">\n                                <h5>Tuning the SVM Model</h5>\n                                <p>\n                                    Prior to performing the final evaluation of the SVM on the test split data, we performed 5-fold CV on the full train split data, applying grid search with the hyperparameter\n                                    grid associated with the SVM algorithm to find optimal hyperparameter values. The optimal hyperparameter values found for the SVM are shown in the table below:\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-3\">\n                                <img id=\"svm-hyperparameters\" class=\"img-fluid\" src=\"../../../../public/assets/svm-hyperparameters.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 pb-5\">\n                                <p>\n                                    We report the estimated generalization performance of the tuned SVM found\n                                    from the 5-fold CV mentioned above, noting that this is <strong>likely a biased estimate of\n                                    generalization performance</strong> as the 5-fold CV procedure was also used to tune the\n                                    model. The <strong>tuned SVM model</strong> achieves a <strong>mean F2 score of 85.0%</strong> and has\n                                    a <strong>standard deviation of 7.40%</strong>.\n                                </p>\n                                <p>\n                                    After tuning the SVM model to find the optimal hyperparameters above, we fit the SVM algorithm using those optimal hyperparameters on the <strong>entire train split data</strong>.\n                                    We then use this <strong>fitted SVM model to predict on the unseen test split data</strong>. The model's predictions on the test split yield the final evaluation of \n                                    the model's generalization performance. As part of our framework, we report aggregate metrics including the F2 and Area Under the Precision-Recall Curve (AUCPR) score and compare our trained model to baseline scores.\n                                    Lastly, we report per-class performance metrics and the confusion matrix of the model's predictions.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Human Risk Classification</h5>\n                                <h5>Final Model Evaluation - Results</h5>\n                            </div>\n                            <div class=\"col-12\">\n                                <h5>Aggregate Metrics & Comparison to Baseline Scores</h5>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    We report an <strong>F2 score of 92.8% on the test split data.</strong> The baseline classifier which always predicts \"Human Risk\", has an F2 score of\n                                    43.4%, so the <strong>trained classifier is a significant improvement over the baseline classifier</strong>.\n                                </p>\n                                <p>\n                                    In addition to F2, we plot the Precision-Recall curve, visualizing the tradeoff between precision and recall for different classification thresholds used by the classifier\n                                    when classifying the data. It is advised to use the PR curve over the Receiver Operating Characteristic (ROC) curve in the case of imbalanced data, as ROC can give an optimistic estimate of the classifiers output\n                                    quality by considering true negatives in the computation, which in high quantity can dramatically lessen the effect of the false positives, false negatives, and true positives\n                                    in the performance estimate, giving a misleadingly high estimate of performance.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-4 pt-3 pb-3\">\n                                    <img id=\"aucpr-curve\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-aucpr.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-8\">\n                                <p>\n                                    The better the classifier (higher recall and higher precision), the closer the AUCPR score is to 1.\n                                    For AUCPR, the performance of a <strong>baseline model has a score which is given by the proportion of positive samples to the total number\n                                    of samples</strong> in the test dataset, which in this case is <strong>0.133</strong>.\n                                </p>\n                                <p>\n                                    We report an <strong>AUCPR of 0.919 for the SVM model</strong> on the test split data, a significant improvement over the baseline score.\n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <h5>Confusion Matrix & Per-Class Metrics</h5>\n                            </div>\n                            <div class=\"col-12 col-md-5 pt-3 pb-5\">\n                                <img id=\"human-risk-confusion-matrix\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-cm-svm.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-5 pt-3 pb-md-5\">\n                                <img id=\"human-risk-per-class\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-per-class-metric.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 pb-5\">\n                                <p>\n                                    From the confusion matrix, we see that the model made very few\n                                    misclassifications on the test split data. Specifically, the model only misclassified one\n                                    data point which was labeled as \"Human Risk\" as \"No Human Risk\" out of all 19\n                                    data points labeled as \"Human Risk\", thus the model had <strong>low false negatives</strong>. The\n                                    model misclassified 3 \"No Human Risk\" data points as \"Human Risk\" out of a total\n                                    of 124 \"No Human Risk\" data points, thus the model predicted <strong>3 false positives</strong>. We\n                                    note that the model had more false positives than false negatives, but <strong>few of each</strong>.\n                                </p>\n                                <p>\n                                    The model performs well by all per-class metrics on the \"No\n                                    Human Risk\" class achieving scores at and above 0.976. Comparatively lower\n                                    performance is observed across all metrics for the \"Human Risk\" class with precision,\n                                    recall, and F1 scores of 0.857, 0.947, and 0.9, respectively.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around\">\n                            <div class=\"carousel-text col-12\">\n                                <h5>Human Risk Text Classification - Discussion</h5>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            When determing the performance metric for the human risk task, we asked the following technical questions:\n                                            <ol>\n                                                <li><strong>Does the metric account for imbalance present in the data distribution?</strong></li>\n                                                <li><strong>Once the metric is determined, what is the performance of the baseline model for the task?</strong></li>\n                                            </ol>\n                                        </li>\n                                    </ul>\n                                </p>\n                                <p>\n                                    While these technical questions are no doubt important for assessing model efficacy for the task, \n                                    we underscore that there were other questions we asked which <strong><u>could only be answered by our engagement with crisis managers.</u></strong>\n                                </p>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            Before we began developing the human risk model, we asked a question about the task itself: \n                                            <strong>Do the classes for the task sufficiently capture the expressed information needs of crisis managers during a crisis?</strong>\n                                            <ul>\n                                                <li>\n                                                    Since these <strong>labels were provided directly by crisis managers</strong> and given the <strong>crisis manager's insight into \n                                                    the importance of assessing the potential of human casualty</strong>, we determined that these classes sufficiently capture\n                                                    an important information need of crisis managers during crisis.\n                                                </li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            For the determining the performance metric for the task, questions which required crisis manager engagement included:\n                                            <ul>\n                                                <li>\n                                                    <strong>\n                                                        Does the metric incorporate the priorities of the crisis managers as\n                                                        it relates to the task, e.g. the cost of a false negative is significantly higher than\n                                                        the cost of a false positive for assessing human risk?\n                                                    </strong>\n                                                </li>\n                                                <li>\n                                                    <strong>\n                                                        Are there multiple metrics that should considered in assessing model efficacy in performing the classification task,\n                                                        e.g. precision and recall, or F2?\n                                                    </strong>\n                                                </li>\n                                            </ul>\n                                        </li>\n                                    </ul>\n                                </p>\n                                <p>\n                                    Asking these questions allowed us to both consider the technical intricacies for\n                                    the task (i.e. data imbalance and baseline performance) and directly embed the\n                                    information needs and priorities of crisis managers into our text ML methodology\n                                    and evaluation, which are important aims of our framework.\n                                </p>\n                                <p>\n                                    The <strong>significant improvement over the baseline classifier by F2</strong> suggests the tuned <strong>SVM model is a useful classifier for\n                                    the human risk task and performs the task reasonably well</strong>. This is further evidenced from the\n                                    Precision-Recall curve, where the tuned SVM model achieved an AUCPR score of\n                                    0.919, a significant improvement over the typical baseline classifier used for\n                                    that metric which achieves an AUCPR of 0.133\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Clustering of Crowdsourced Japanese Crisis Text Data</h5>\n                                <h5>Overview</h5>\n                            </div>\n                            <div class=\"col-12\">\n                                <p>\n                                    Beyond investigating the human risk classification task, we aimed to explore the\n                                    Fukuchiyama firefighter flood crisis report corpus to see if we could uncover other\n                                    coherent categories which may exist in the data. These uncovered categories could\n                                    inform the development of classification tasks in future work, in addition to any\n                                    of the humanitarian categories provided by crisis managers.\n                                </p>\n                                <p>\n                                    This exploration is powered by a series of unsupervised learning techniques including dimensionality reduction and clustering. \n                                    We developed a pipeline that utilizes these unsupervised techniques to perform the clustering experiments we conducted\n                                    to uncover coherent categories in the data.\n                                </p>\n                                <p>\n                                    For our clustering experiments, we note that we use <strong>all 716 FC firefighter crisis reports</strong>. Additionally, we focus on using the <strong>TF-IDF based on unigrams embeddings</strong> and \n                                    <strong>Pretrained Japanese BERT embeddings</strong>, which we refer to as BERT embeddings for brevity. Since these featurizations are 1489 and 768 dimensions respectively, this\n                                    motivated our incorporation of dimensionality reduction techniques into our clustering pipeline.\n                                </p>\n                                <div class=\"col-12 pt-3 pb-3\">\n                                    <img id=\"clustering-features\" class=\"img-fluid\" src=\"../../../../public/assets/text-features-for-clustering.png\" alt=\"Second slide\">\n                                </div>\n                            </div>\n                            <div class=\"col-12\">\n                                <h6>Evaluation Overview</h6>\n                            </div>\n                            <div class=\"col-12 pb-5\">\n                                <p>\n                                    The evaluation of our clustering experiments consisted of multiple stages. <strong>First, we perform quantitative analysis</strong>, producing <strong>Within-Cluster Sum of Squares (WCSS)</strong>, or \"Elbow\" plots for each combination of\n                                    featurization type, dimensionality reduction technique, and clustering algorithm (12 combos in total). We refer to these as configuration combinations. Using these plots, we identify <strong>a query subset</strong> of the combinations to further investigate for our\n                                    <strong>qualitative evaluation</strong>. In the first stage of our qualitative evaluation we used the <strong>english translations of the closest documents to each cluster center</strong> to select a configuration combination from the query subset \n                                    for the <strong>final stage of qualitative assessment</strong>. In the final stage, a <strong>fluent Japanese speaker</strong> investigated the raw Japanese reports in the clusters made by the selected configuration combination and \n                                    <strong>determined a human-interpretable label to describe the cluster overall</strong> for each cluster. \n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Clustering Pipeline & Experiments</h5>\n                            </div>\n                            <div class=\"col-12 col-md-5 pt-3 pb-3\">\n                                    <img id=\"clustering-pipeline\" class=\"img-fluid\" src=\"../../../../public/assets/clustering-pipeline.png\" alt=\"Pipeline for Clustering\">\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-5\">\n                                <h6>Featurize, Reduce Dimensions, and Cluster</h6>\n                                <p>\n                                    Using the devised clustering pipeline, we can sequentially reduce the dimensions of the input features to <strong>2-dimensions</strong>, using\n                                    <strong>Principle Component Analysis (PCA)</strong>, <strong>t-distributed Stochastic Neighbor Embedding (t-SNE)</strong>, or we <strong>do not apply dimensionality reduction</strong> at all. \n                                    We finally cluster the reduced data using either <strong>K-means</strong> or <strong>K-medoids</strong>, which is more robust to outliers present in the data. These hyperparameters to \n                                    the clustering pipeline are summarized in the neighboring figure.\n                                </p>\n                                <div class=\"row justify-content-center pb-3\">\n                                    <img id=\"clustering-hyperparameters\" class=\"img-fluid\" src=\"../../../../public/assets/clustering-hyperparameters.png\" alt=\"Hyperparameters for Clustering\">\n                                </div>\n                                <h6>Outputs of Clustering Pipeline</h6>\n                                <p>\n                                    <strong>Having selected a configuration to use and a K-value to use</strong>, our clustering pipeline produces the clustered data points, the text associated with\n                                    the <strong>20 closest documents to the cluster center (in JA & EN translations)</strong> for each cluster, and the top 20 unigrams (in JA) by TF-IDF score for the \n                                    document formed from concatenating the documents in a cluster, for each cluster, forming a cluster-level document corpus. We note that our pipeline can take any positive values x & y for displaying the \n                                    top x unigrams or top y documents in a cluster.\n                                </p>\n                            </div>\n                            <div class=\"col-12 pb-5\">\n                                <h5>Clustering Experiments - Identifying the Query Subset</h5>\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                We investigated all 12 featurization, dimensionality reduction, and clustering algorithm combinations by investigating the corresponding \n                                                <strong>WCSS or Elbow plot</strong> for <strong>K = 2, , 20 clusters</strong>\n                                            </p>\n                                            <p>\n                                                <strong>WCSS</strong> captures extent to which data points within a cluster are at a close distance to each other, ideally we want this to be low, \n                                                but not too low. Minimizing WCSS is the same as maximizing the distance between data points in different clusters.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"pb-3\">\n                                    <img id=\"elbow-plot\" class=\"img-fluid\" src=\"../../../../public/assets/elbow-plot.png\" alt=\"Second slide\">\n                                </div>\n                                <p>\n                                    <ol>\n                                        <li value=\"2\">\n                                            Select a subset of combinations which have <strong>relatively lower WCSS scores</strong> across \n                                            all K values and <strong>have an elbow in the elbow plot</strong> to qualitatively investigate further. We call this the <strong>query subset</strong>.\n                                        </li>\n                                    </ol>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Clustering Qualitative Evaluation & Results</h5>\n                                <h5>Preliminary Qualitative Assessment (in EN): Investigating the Query Subset</h5>\n                            </div>\n                            <div id=\"workflow-and-configs\" class=\"col-12 col-md-6 pt-3 pb-5\">\n                                <h6>Preliminary Assessment Workflow</h6>\n                                <img id=\"qualitative-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/preliminary-assessment.png\" alt=\"Second slide\">\n                                <br>\n                                <br>\n                                <h6>Query Subset</h6>\n                                <img id=\"query-subset\" class=\"img-fluid\" src=\"../../../../public/assets/query-subset.png\" alt=\"Configuration Combinations in the Query Subset\">\n                                <br>\n                                <br>\n                                <h6>Qualitative Summaries of Clusters and Possible Labels</h6>\n                                <img id=\"qualitative-summaries\" class=\"img-fluid\" src=\"../../../../public/assets/qualitative-summaries.png\" alt=\"Qualitative Summaries of Resultant Clustering for Each Cluster\">\n                            </div>\n                            <div id=\"workflow\" class=\"col-12 col-md-6 pt-3 pb-1\">\n                                <h6>Preliminary Assessment Workflow</h6>\n                                <img id=\"qualitative-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/preliminary-assessment.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-2 pb-md-5\">\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                <strong>For each combination in the subset, we determine an elbow</strong> from the corresponding elbow \n                                                plot as the K value to use in our qualitative analysis.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <img id=\"identified-elbow\" class=\"img-fluid pb-3\" src=\"../../../../public/assets/identified-elbow.png\" alt=\"Second slide\">\n                                <p>\n                                    <ol>\n                                        <li value=\"2\">\n                                            <p>\n                                                For each cluster, we investigate the <strong>top 20 reports within a cluster which were closest to the cluster center</strong>. \n                                                We note that for the <strong>preliminary assessment</strong>, we used <strong>English translations of the reports given by DeepL neural translation.</strong><sup><a href=\"#fnE\" id=\"refE\">1</a></sup>\n                                            </p>\n                                            <p>\n                                                When investigating the representative reports in each cluster, we answered the question: \n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <p id=\"research-question\">\n                                    <strong>When looked at together, does the content of the representative reports in a cluster elicit an interpretable label? If so, what is it?</strong>\n                                </p>\n                                <p>\n                                    <ol>\n                                        <li value=\"3\">\n                                            <p>\n                                                We then identified the configuration combination in the query subset which yielded <strong>the most interpretable labels across combinations</strong>,\n                                                i.e. selected the combination which had the highest number of clusters that had\n                                                representative documents which elicited an interpretable label.\n                                            </p>\n                                            <p>\n                                                In the neighboring graphics, we showcase the configuration combinations in the query subset and\n                                                we report qualitative summaries for each of the cluster configurations. We note that the configuration combination which gave the highest number of clusters which had a coherent, interpretable label (9 in total)\n                                                was the <strong>BERT embedding, t-SNE (2 components), and K-medoids clustering</strong> combination, which is <strong>bolded</strong>.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                            </div>\n                            <div id=\"configs\" class=\"col-12 pt-3 pb-5\">\n                                <h6>Query Subset</h6>\n                                <img id=\"query-subset\" class=\"img-fluid\" src=\"../../../../public/assets/query-subset.png\" alt=\"Configuration Combinations in the Query Subset\">\n                                <br>\n                                <br>\n                                <h6>Qualitative Summaries of Clusters and Possible Labels</h6>\n                                <img id=\"qualitative-summaries\" class=\"img-fluid\" src=\"../../../../public/assets/qualitative-summaries.png\" alt=\"Qualitative Summaries of Resultant Clustering for Each Cluster\">\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnE\">1. <a href=\"https://www.deepl.com/en/translator\" target=\"_blank\">Link to DeepL.</a> We acknowledge Saeko Baird of the Urban Risk Lab at MIT who cleaned these translations of their inaccuracies.<a href=\"#refE\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Clustering Qualitative Evaluation & Results</h5>\n                                <h5>Final Qualitative Assessment (in JA): Investigating the Optimal Configuration Combination determined from the Preliminary Assessment</h5>\n                            </div>\n                            <div id=\"workflow-and-clusters\" class=\"col-12 col-md-6 pt-3 pb-0\">\n                                <h6>Final Assessment Workflow</h6>\n                                <img id=\"qualitative-final-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/final-assessment.png\" alt=\"Final Qualitative Assessment Workflow\">\n                                <br>\n                                <br>\n                                <h6>Unlabeled Clusters</h6>\n                                <img id=\"unlabled-clusters-img\" class=\"img-fluid\" src=\"../../../../public/assets/unlabeled-clusters.png\" alt=\"Unlabeled Clusters\">\n                            </div>\n                            <div id=\"final-assessment-workflow\" class=\"col-12 col-md-6 pt-3 pb-1\">\n                                <h6>Final Assessment Workflow</h6>\n                                <img id=\"qualitative-final-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/final-assessment.png\" alt=\"Final Qualitative Assessment Workflow\">\n                            </div>\n                            <div id=\"steps-with-unlabeled-clusters\" class=\"col-12 col-md-6 pt-2\">\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \n                                                <strong>top 20 closest reports</strong> within the cluster to the cluster center & the <strong>top 20 unigrams in by TF-IDF score for cluster-level document corpus</strong>.\n                                            </p>\n                                            <p>\n                                                Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"row justify-content-center pb-4\">\n                                    <h6>Unlabeled Clusters</h6>\n                                    <img id=\"unlabeled-clusters-img\" class=\"img-fluid\" src=\"../../../../public/assets/unlabeled-clusters.png\" alt=\"Unlabeled Clusters\">\n                                </div>\n                                <p>\n                                    <ol>\n                                        <li value=\"2\">\n                                            <p>\n                                                Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA<sup><a href=\"#fnF\" id=\"refF\">1</a></sup> investigated each cluster and assigned an interpretable label to it.\n                                            </p>\n                                            <p>\n                                                The interpretable label given for each cluster is depicted in the figure below:\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"col-12\">\n                                    <img id=\"cluster-labels\" class=\"img-fluid\" src=\"../../../../public/assets/cluster-labels.png\" alt=\"Cluster Labels\">\n                                </div>\n                            </div>\n                            <div id=\"steps-without-unlabeled-clusters\" class=\"col-12 col-md-6\">\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \n                                                <strong>top 20 closest reports</strong> within the cluster to the cluster center & the <strong>top 20 unigrams in by TF-IDF score for cluster-level document corpus</strong>.\n                                            </p>\n                                            <p>\n                                                Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters.\n                                            </p>\n                                        </li>\n                                        <li>\n                                            <p>\n                                                Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA<sup><a href=\"#fnF\" id=\"refF\">1</a></sup> investigated each cluster and assigned an interpretable label to it.\n                                            </p>\n                                            <p>\n                                                The interpretable label given for each cluster is depicted in the figure below:\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"col-12\">\n                                    <img id=\"cluster-labels\" class=\"img-fluid\" src=\"../../../../public/assets/cluster-labels.png\" alt=\"Cluster Labels\">\n                                </div>\n                            </div>\n                            <div id=\"labeled-clusters\" class=\"col-12 col-md-7 pb-3\">\n                                <p id=\"process-arrow\">&#8595;</p>\n                                <h6>Labeled Clusters</h6>\n                                <img id=\"labeled-clusters-img\" class=\"img-fluid\" src=\"../../../../public/assets/labeled-clusters.png\" alt=\"Labeled Clusters\">\n                            </div>\n                        </div>\n                        <p>\n                            <sup id=\"fnF\">1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who assigned an interpretable label\n                                to each cluster.<a href=\"#refF\" title=\"Jump back to footnote 1 in the text.\"></a>\n                            </sup>\n                        </p>\n                        <br>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around\">\n                            <div class=\"carousel-text col-12 col-md-7\">\n                                <h5>Clustering Firefighter Flood Crisis Reports - Discussion</h5>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            Suggests what is deemed important to report during flood crisis by FC firefighters on-the-ground.\n                                        </li>\n                                        <li>\n                                            Since we applied the clustering on-the-ground firefighter reports, these results can be used to devise <strong>classification tasks with labels which better embed the information needs of EOC</strong> & can be cross-referenced with EOC.\n                                        </li>\n                                        <li>\n                                            Experiment can also be applied on Japanese RiskMap reports or crisis tweets to see if similar cluster labels are unveiled by resident reporting.\n                                        </li>\n                                        <li>\n                                            This method has the <strong>drawback of permitting data points to be in only one cluster (hard clustering)</strong>, and \n                                            we observed that some of the data points have content which is indicative of multiple of the unveiled interpretable labels.\n                                        </li>\n                                    </ul>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../../constants';\n\nexport default {\n  name: 'TextAnalysisCarousel',\n  mounted() {\n    enableScrollUpOnCarousel('#TextAnalysisCarousel')\n    enableSwipeOnCarousel('#TextAnalysisCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h5 {\n    color: white;\n}\n\n.characteristics {\n    text-align: center;\n}\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n#hagibis-details {\n    color: palevioletred;\n}\n\n#twitter-quote {\n    text-align: left;\n}\n\n#pika-gif {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 60vh;\n    height: 40vh;\n}\n\n#identified-elbow {\n    max-height: 40vh;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n#workflow-and-configs {\n    display: none;\n}\n\n#workflow-and-clusters {\n    display: none;\n}\n\n#steps-without-unlabeled-clusters {\n    display: none;\n}\n\n#process-arrow {\n    font-size: 6vh;\n    text-align: center;\n}\n\n@media (min-width: 768px) {\n    \n    #clustering-hyperparameters {\n        max-width: 40vw;\n    }\n\n    #workflow-and-configs {\n        display: inline;\n    }\n\n    #workflow {\n        display: none;\n    }\n\n    #configs {\n        display: none;\n    }\n\n    #workflow-and-clusters {\n        display: inline;\n    }\n\n    #final-assessment-workflow {\n        display: none;\n    }\n\n    #steps-without-unlabeled-clusters {\n        display: inline;\n    }\n\n    #steps-with-unlabeled-clusters {\n        display: none;\n    }\n\n    #process-arrow {\n        font-size: 10vh;\n        text-align: center;\n    }\n\n}\n\n@media (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n#text-characteristics-table {\n  font-family: Arial, Helvetica, sans-serif;\n  border-collapse: collapse;\n  color: black;\n  width: 100%;\n}\n\n#text-characteristics-table td, #text-characteristics-table th {\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n\n#text-characteristics-table tr:nth-child(even){background-color: #f2f2f2;}\n#text-characteristics-table tr:hover{background-color: #ddd;}\n#text-characteristics-table tr:nth-child(odd) {background-color: #ddd;}\n\n#text-characteristics-table th {\n  padding-top: 12px;\n  padding-bottom: 12px;\n  text-align: center;\n  background-color: darkturquoise;\n  color: white\n}\n\n</style>","import mod from \"-!../../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./TextAnalysisCarousel.vue?vue&type=template&id=753752de&scoped=true&\"\nimport script from \"./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"\nexport * from \"./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"\nimport style0 from \"./TextAnalysisCarousel.vue?vue&type=style&index=0&id=753752de&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"753752de\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-8\"},[_c('h3',[_vm._v(\"Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"NLPIntLitDevCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#NLPIntLitDevCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#NLPIntLitDevCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#NLPIntLitDevCarousel\",\"data-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-target\":\"#NLPIntLitDevCarousel\",\"data-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-target\":\"#NLPIntLitDevCarousel\",\"data-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-target\":\"#NLPIntLitDevCarousel\",\"data-slide-to\":\"5\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/int-dev-results.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Project Presentation, Report, and Code\")]),_c('p',[_vm._v(\" This was a final project for 6.864: Advanced Natural Language Processing. This presentation focuses on introducing the project, the specific parts I worked on, and the main results from our analysis. \"),_c('br'),_vm._v(\" A brief overview of the motivation, methods, and results is available in this \"),_c('a',{attrs:{\"href\":\"./assets/int-dev-gray-lit.pdf\",\"target\":\"_blank\"}},[_vm._v(\"presentation PDF.\")]),_c('br'),_vm._v(\" The more thorough report of our methodology, visualizations, and findings \"),_c('a',{attrs:{\"href\":\"./assets/6_864_Project.pdf\",\"target\":\"_blank\"}},[_vm._v(\"here.\")]),_c('br'),_vm._v(\" The code for this project was written in Python. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/6.864-fp\",\"target\":\"_blank\"}},[_vm._v(\"Here's the GitHub Repo.\")])]),_c('p',[_vm._v(\" For this project, my main contributions were the supervised methodology/Named Entity Recognition stream discussed in the following slides \")])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Abstract\")]),_c('p',[_vm._v(\" In fields like international development, decision-makers prioritize making evidence-based decisions for funding and implementing future projects. This aim is made difficult because of the plethora of information being published each year, and the nature of the research corpus as unstructured text or grey literature. To make informed decisions and understand the growing corpus of research available, researchers have turned to evidence synthesis - the process of compiling information and knowledge from many sources and disciplines to inform decisions. However, the manual evidence synthesis process takes extensive time (often 18 months to 3 years) and effort, and may soon be impossible at the worlds increasing rate of research output. To address these problems, we employ natural language processing techniques on a international development literature corpus of 244 documents to extract information from the title and abstract of international development documents, and to automatically cluster documents based on their content. We classify documents by Country of Study using a pretrained transformer Named Entity Recognition model and achieve an accuracy of 91.0%. Using K-Means clustering, we uncover informative and distinctive groupings of the documents which share similar semantic content. These methods reduce the time it takes for manual evidence synthesis for international development grey literature by enabling country of study filtering and clustering documents by semantic similarity. \")])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Named Entity Recognition (NER) for Country of Study (CoS) Classification - Models\")]),_c('p',[_vm._v(\" The country of study (CoS) associated with each paper is quite pertinent to the international development domain. Our dataset is labeled with the CoS of each paper in our corpus. However, since our dataset is rather small (244 documents), we sought to evaluate whether pretrained NER models which extract a variety of entity types from text, could accurately extract the CoS for the papers in our corpus, which have a variety of text fields. \")]),_c('h6',[_vm._v(\"Country of Study Extraction and Classification\")]),_c('p',[_vm._v(\" We create a lower-cased, alphabetically-ordered, list of countries, which we construct using countryinfo \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/countryinfo/\",\"target\":\"_blank\"}},[_vm._v(\"(Link to CountryInfo PyPI page)\")]),_vm._v(\", a Python package which contains a large dictionary of countries, their alternative names, and ISO information. We ensure the strings of the countries present in our corpus match their respective string in the alphabetically-sorted list of countries. We note that Myanmar and Kosovo are countries present in our corpus, but are not present in the countryinfo dictionary, so we add them to the final list of alphabetically-sorted countries. Since nationality is a type of named entity that NER models typically extract in addition to countries, using a comprehensive, open-source nationality-country mapping \"),_c('a',{attrs:{\"href\":\"https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv\",\"target\":\"_blank\"}},[_vm._v(\"(Demonym-Country Mapping Link)\")]),_vm._v(\", we construct a lower-cased, alphabetically-ordered list of nationalities as well as a dictionary mapping nationality to country. We note that we use the words nationality and demonym interchangeably. These lists and dictionary are useful for performing the country of study (CoS) classification using extracted entities from input text or determining if a nationality or country is a substring contained in the input text string. \")]),_c('h6',[_vm._v(\"Simple Substring Matcher (SSM) Algorithm Baseline & CoS Extraction & Classification\")]),_c('p',[_vm._v(\" As a baseline to our CoS prediction task, we devise a simple, non-ML, deterministic algorithm, called the Simple Substring Matcher (SSM) Algorithm. This method begins by making the input text lower-cased. To predict a CoS, it then scans through the alphabetically-sorted list of countries and classifies the first country which is a substring in the input text as the CoS. If no country is found as a substring in the text, the method then scans the alphabetically-sorted list of nationalities. If a nationality is found as a substring of the input, the method maps the nationality to the corresponding country and classifies the paper as having that country as the CoS. If neither country nor nationality is found as a substring in the text, the method classifies the paper's CoS as a \"),_c('i',[_vm._v(\"None\")]),_vm._v(\" value. We refer to this classification model as the Simple Substring Matcher (SSM) model. \")]),_c('h6',[_vm._v(\"CoS Extraction & Classification by Pretrained NER Models\")]),_c('p',[_vm._v(\" Although we utilize different pretrained NER models in our experiments as shown in the next slide, the process for classifying CoS using predicted entities is the same. Each model takes the raw text as input, predicts various non-overlapping entities present in the text into one of several entity categories. For the CoS classification task, we only consider the predicted \"),_c('b',[_vm._v(\"NORP\")]),_vm._v(\" (nationalities, or religious, or political groups) entities and the \"),_c('b',[_vm._v(\"GPE\")]),_vm._v(\" (countries, cities, states) entities as we assume that these categories are the only ones which would contain the country or relevant demonym associated with the CoS. We now begin our discussion of the classification procedure for the pretrained NER models. First, we make all NORP and GPE entities lower-cased. Next, we map any demonyms present among the NORP entities to their corresponding country. We then combine the resulting unique NORP and GPE entities into an alphabetically-ordered list. We scan this list of NORP and GPE entities checking if any of them exist in the countries list mentioned above, classifying the CoS as the first entity-country match found. If no match is found, we make a final attempt to determine the CoS by providing each of the entities as input to the GeoPy Geocoder \"),_c('a',{attrs:{\"href\":\"https://geopy.readthedocs.io/en/stable/\",\"target\":\"_blank\"}},[_vm._v(\"(Link to GeoPy API)\")]),_vm._v(\" object, which provides an address-location object if a location is found for the provided entity or no value otherwise. We do this for each entity, and if a location is found for a particular entity, we classify the paper's CoS as the country associated with the found address-location. If no country is found for all the entities, we classify the paper's CoS as \"),_c('i',[_vm._v(\"None\")]),_vm._v(\". \")])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/ner-results.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"NER for Country of Study Classification - Results\")]),_c('p',[_vm._v(\" In addition to testing different classification models, I experimented with different input strings to see how results change with various text fields and concatenations between them. These various inputs to the models include the title, abstract, intervention description, outcome description, and various concatenations of these text fields. \")]),_c('p',[_vm._v(\" All of the pretrained spaCy NER models have 0.0% accuracy when using the just the intervention description, however the SSM model achieves 13.9% accuracy on the intervention description. All models attain an accuracy of 2.9% when using just the outcome description. The title and abstract individually appear to be good input fields for predicting the CoS, however the concatenation of title and abstract appears to be the most informative input, as this is the input that yields the highest performance across all of the models. Overall, we observe that the baseline simple substring checker is a fairly competitive model against the pretrained ML models, outperforming all the ML models on intervention description, performing the same as the ML models on outcome description, and only falling a few percentage points below even the best ML model on the other inputs. With the exception of the title, intervention description, and outcome description, the ML models in increasing order of complexity, do increasingly better on the CoS extraction task, in the following order from least performant to most performant: ESMS, ESMM, ESML, and ESMT. With the exception of the intervention description and output description inputs, we observe that the ESMT model performs best across all other inputs. Furthermore, we see that the concatenated title and abstract input and the ESMT model combination performs the best across all input-model combinations with 91.0% accuracy. We use this top-performing model by accuracy to construct AI-assisted tools which could assist researcher in the evidence in the following slide. \")])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/ner-res-tools.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Predictions by Pretrained Transformer NER Model for International Development Gray Literature Map and Filter Function\")]),_c('h6',[_vm._v(\"Map of International Development Literature Gray Corpus\")]),_c('p',[_vm._v(\" Using the CoS predictions from the pretrained NER transformer model on the concatenated title and abstract input, we construct a geographical map of the corpus as shown in the left image. For each paper, which had a non-null prediction by the ESMT model, we place a tooltip at location coordinate associated with the predicted CoS. These location coordinates were pulled using the GeoPy Geocoder object from the GeoPy Python package. We added slight, uniform random jitter to each of the coordinates, so papers with the same predicted CoS don't directly overlap. When a user hovers over the tooltip, they will see the title of the paper associated with that tooltip. The webpage for this map can be downloaded \"),_c('a',{attrs:{\"href\":\"https://drive.google.com/file/d/1Q2P6ouwcDWrnXsq8LMpa4YqCuD7qsbtO/view?usp=sharing\",\"target\":\"_blank\"}},[_vm._v(\"here\")]),_vm._v(\" for view in a browser. \")]),_c('h6',[_vm._v(\"Filtering by Predicted CoS\")]),_c('p',[_vm._v(\" For large corpora of International Development Gray Literature, the utility of the CoS prediction task is most evident by the robust filtering capability it enables. For instance, by concatenating only the title and abstract of papers in the corpus, and using them as input to generate CoS predictions by the pretrained transformer model used in this study, this enables the ability for unlabeled papers in the corpus to be accurately filtered to identify studies which had a specific CoS. This method would greatly reduce the time necessary for manual CoS annotation while also yielding higher accuracy than a simple substring matcher, simplifying a step in the international literature review process with high accuracy. An example of this filtering functionality is shown in the right image for papers in the corpus, which were predicted as having Guatemala as the CoS. The CoS was predicted using the pretrained transformer NER model with the concatenated title and abstract as input. We display the corresponding title and abstract for quick scanning of results for relevancy to research topic. Additionally, we provide the option to filter the corpus for papers which had a predicted CoS as \"),_c('i',[_vm._v(\"None\")]),_vm._v(\". \")])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/int-dev-results.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Conclusion\")]),_c('p',[_vm._v(\" The manual evidence synthesis for international development gray literature is a time-consuming process. We have demonstrated that certain components of the evidence synthesis process in international development gray literature such as filtering corpora for papers which have a specific country of study or grouping similar documents together can benefit greatly from the use of methods of information extraction and unsupervised learning. More specifically, we have utilized a pretrained transformer NER model to accurately predict the country of study for the papers present in the corpus used in this study, thus enabling accurate filtering of the corpus for papers with a specific predicted country of study. After tuning to find the optimal number of clusters in K-Means clustering, we uncovered informative and distinctive clusters of documents with similar content in the corpus. The automation of these components in the evidence synthesis process for international development grey literature mitigates the effort and time that is required for manual evidence synthesis. \")])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12 col-md-8\">  \n            <h3>Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"NLPIntLitDevCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#NLPIntLitDevCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#NLPIntLitDevCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#NLPIntLitDevCarousel\" data-slide-to=\"2\"></li>\n                    <li data-target=\"#NLPIntLitDevCarousel\" data-slide-to=\"3\"></li>\n                    <li data-target=\"#NLPIntLitDevCarousel\" data-slide-to=\"4\"></li>\n                    <li data-target=\"#NLPIntLitDevCarousel\" data-slide-to=\"5\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/int-dev-results.png\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Project Presentation, Report, and Code</h5>\n                            <p>\n                                This was a final project for 6.864: Advanced Natural Language Processing. This presentation focuses on\n                                introducing the project, the specific parts I worked on, and the main results from our analysis.\n                                <br>\n                                A brief overview of the motivation, methods,\n                                and results is available in this <a href=\"./assets/int-dev-gray-lit.pdf\" target=\"_blank\">presentation PDF.</a>\n                                <br>\n                                The more thorough report of our methodology, visualizations, and findings <a href=\"./assets/6_864_Project.pdf\" target=\"_blank\">here.</a>\n                                <br>\n                                The code for this project was written in Python. <a href=\"https://github.com/dyllew/6.864-fp\" target=\"_blank\">Here's the GitHub Repo.</a>\n                            </p>\n                            <p>\n                                For this project, my main contributions were the supervised methodology/Named Entity Recognition stream discussed in the following slides\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Abstract</h5>\n                            <p>\n                                In fields like international development, decision-makers prioritize making evidence-based decisions for funding and implementing future projects. \n                                This aim is made difficult because of the plethora of information being published each year, and the nature of the research corpus as unstructured \n                                text or grey literature. To make informed decisions and understand the growing corpus of research available, researchers have turned to evidence \n                                synthesis - the process of compiling information and knowledge from many sources and disciplines to inform decisions. However, the manual evidence \n                                synthesis process takes extensive time (often 18 months to 3 years) and effort, and may soon be impossible at the worlds increasing rate of research \n                                output. To address these problems, we employ natural language processing techniques on a international development literature corpus of 244 documents \n                                to extract information from the title and abstract of international development documents, and to automatically cluster documents based on their content. \n                                We classify documents by Country of Study using a pretrained transformer Named Entity Recognition model and achieve an accuracy of 91.0%. Using K-Means clustering, \n                                we uncover informative and distinctive groupings of the documents which share similar semantic content. These methods reduce the time it takes for manual evidence \n                                synthesis for international development grey literature by enabling country of study filtering and clustering documents by semantic similarity.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Named Entity Recognition (NER) for Country of Study (CoS) Classification - Models</h5>\n                            <p> \n                                The country of study (CoS) associated \n                                with each paper is quite pertinent to the international development domain. Our dataset is labeled \n                                with the CoS of each paper in our corpus. However, since our dataset is rather small (244 documents),\n                                 we sought to evaluate whether pretrained NER models which extract a variety of entity types from text, \n                                 could accurately extract the CoS for the papers in our corpus, which have a variety of text fields.\n                            </p>\n                            <h6>Country of Study Extraction and Classification</h6>\n                            <p> \n                                We create a lower-cased, alphabetically-ordered, list of countries, which we construct using countryinfo\n                                <a href=\"https://pypi.org/project/countryinfo/\" target=\"_blank\">(Link to CountryInfo PyPI page)</a>, a Python package \n                                which contains a large dictionary of countries, their alternative names, and ISO information. We ensure \n                                the strings of the countries present in our corpus match their respective string in the alphabetically-sorted list of countries. \n                                We note that Myanmar and Kosovo are countries present in our corpus, but are not present in the countryinfo dictionary, so we add \n                                them to the final list of alphabetically-sorted countries. Since nationality is a type of named entity that NER models typically \n                                extract in addition to countries, using a comprehensive, open-source nationality-country mapping <a href=\"https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv\" target=\"_blank\">(Demonym-Country Mapping Link)</a>, \n                                we construct a lower-cased, alphabetically-ordered list of nationalities as well as a dictionary mapping nationality to country. \n                                We note that we use the words nationality and demonym interchangeably. These lists and dictionary are useful for performing the country of study (CoS) \n                                classification using extracted entities from input text or determining if a nationality or country is a substring contained in the input text string.\n                            </p>\n                            <h6>Simple Substring Matcher (SSM) Algorithm Baseline & CoS Extraction & Classification</h6>\n                            <p> \n                                As a baseline to our CoS prediction task, we devise a simple, non-ML, deterministic algorithm, called the Simple Substring Matcher (SSM) Algorithm. This \n                                method begins by making the input text lower-cased. To predict a CoS, it then scans through the alphabetically-sorted list of countries and classifies \n                                the first country which is a substring in the input text as the CoS. If no country is found as a substring in the text, the method then scans the alphabetically-sorted \n                                list of nationalities. If a nationality is found as a substring of the input, the method maps the nationality to the corresponding country and classifies the paper as \n                                having that country as the CoS. If neither country nor nationality is found as a substring in the text, the method classifies the paper's CoS as a <i>None</i> value. \n                                We refer to this classification model as the Simple Substring Matcher (SSM) model.\n                            </p>\n                            <h6>CoS Extraction & Classification by Pretrained NER Models</h6>\n                            <p> \n                                Although we utilize different pretrained NER models in our experiments as shown in the next slide, the process for classifying CoS using predicted entities is the same. \n                                Each model takes the raw text as input, predicts various non-overlapping entities present in the text into one of several entity categories. For the CoS classification task, \n                                we only consider the predicted <b>NORP</b> (nationalities, or religious, or political groups) entities and the <b>GPE</b> (countries, cities, states) entities as we assume \n                                that these categories are the only ones which would contain the country or relevant demonym associated with the CoS. We now begin our discussion of the classification procedure \n                                for the pretrained NER models.\n\n                                First, we make all NORP and GPE entities lower-cased. Next, we map any demonyms present among the NORP entities to their corresponding country. We then combine the resulting unique \n                                NORP and GPE entities into an alphabetically-ordered list. We scan this list of NORP and GPE entities checking if any of them exist in the countries list mentioned above, \n                                classifying the CoS as the first entity-country match found. If no match is found, we make a final attempt to determine the CoS by providing each of the entities as input to the GeoPy Geocoder <a href=\"https://geopy.readthedocs.io/en/stable/\" target=\"_blank\">(Link to GeoPy API)</a>\n                                object, which provides an address-location object if a location is found for the provided entity or no value otherwise. We do this for each entity, and if a location is found for a particular entity, \n                                we classify the paper's CoS as the country associated with the found address-location. If no country is found for all the entities, we classify the paper's CoS as <i>None</i>.  \n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/ner-results.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>NER for Country of Study Classification - Results</h5>\n                            <p>\n                                In addition to testing different classification models, \n                                I experimented with different input strings to see how results change with various text fields and concatenations between them. \n                                These various inputs to the models include the title, abstract, intervention description, outcome description, and various concatenations of these text fields.\n                                \n                            </p>\n                            <p> \n                                All of the pretrained spaCy NER models have 0.0% accuracy when using the just the intervention description, \n                                however the SSM model achieves 13.9% accuracy on the intervention description. All models attain an accuracy of \n                                2.9% when using just the outcome description. The title and abstract individually appear to be good input fields \n                                for predicting the CoS, however the concatenation of title and abstract appears to be the most informative input, \n                                as this is the input that yields the highest performance across all of the models. Overall, we observe that the baseline \n                                simple substring checker is a fairly competitive model against the pretrained ML models, outperforming all the ML models\n                                on intervention description, performing the same as the ML models on outcome description, and only falling a few percentage \n                                points below even the best ML model on the other inputs. With the exception of the title, intervention description, \n                                and outcome description, the ML models in increasing order of complexity, do increasingly better on the CoS extraction task, \n                                in the following order from least performant to most performant: ESMS, ESMM, ESML, and ESMT. With the exception of the \n                                intervention description and output description inputs, we observe that the ESMT model performs best across all other inputs. \n                                Furthermore, we see that the concatenated title and abstract input and the ESMT model combination performs the best across \n                                all input-model combinations with 91.0% accuracy. We use this top-performing model by accuracy to construct\n                                AI-assisted tools which could assist researcher in the evidence in the following slide.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/ner-res-tools.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Predictions by Pretrained Transformer NER Model for International Development Gray Literature Map and Filter Function</h5>\n                            <h6>Map of International Development Literature Gray Corpus</h6>\n                            <p> \n                                Using the CoS predictions from the pretrained NER transformer model on the concatenated title and abstract input,  \n                                we construct a geographical map of the corpus as shown in the left image. For each paper, which had a non-null prediction \n                                by the ESMT model, we place a tooltip at location coordinate associated with the predicted CoS. These location coordinates \n                                were pulled using the GeoPy Geocoder object from the GeoPy Python package. We added slight, uniform random jitter to each of the coordinates, \n                                so papers with the same predicted CoS don't directly overlap. When a user hovers over the tooltip, they will see the title of the paper associated with that tooltip.\n                                The webpage for this map can be downloaded <a href=\"https://drive.google.com/file/d/1Q2P6ouwcDWrnXsq8LMpa4YqCuD7qsbtO/view?usp=sharing\" target=\"_blank\">here</a> for view in a browser.\n                            </p>\n                            <h6>Filtering by Predicted CoS</h6>\n                            <p> \n                                For large corpora of International Development Gray Literature, the utility of the CoS prediction task is most evident by the robust filtering capability it enables. For instance, by \n                                concatenating only the title and abstract of papers in the corpus, and using them as input to generate CoS predictions by the pretrained transformer model used in this study, this \n                                enables the ability for unlabeled papers in the corpus to be accurately filtered to identify studies which had a specific CoS. This method would greatly reduce the time necessary for \n                                manual CoS annotation while also yielding higher accuracy than a simple substring matcher, simplifying a step in the international literature review process with high accuracy. An example \n                                of this filtering functionality is shown in the right image for papers in the corpus, which were predicted as having Guatemala as the CoS. The CoS was predicted using the pretrained transformer \n                                NER model with the concatenated title and abstract as input. We display the corresponding title and abstract for quick scanning of results for relevancy to research topic. Additionally, we provide \n                                the option to filter the corpus for papers which had a predicted CoS as <i>None</i>.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/int-dev-results.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Conclusion</h5>\n                            <p> \n                                The manual evidence synthesis for international development gray literature is a time-consuming process. \n                                We have demonstrated that certain components of the evidence synthesis process in international development gray \n                                literature such as filtering corpora for papers which have a specific country of study or grouping similar documents \n                                together can benefit greatly from the use of methods of information extraction and unsupervised learning. More specifically, \n                                we have utilized a pretrained transformer NER model to accurately predict the country of study for the papers present in the corpus \n                                used in this study, thus enabling accurate filtering of the corpus for papers with a specific predicted country of study. After tuning \n                                to find the optimal number of clusters in K-Means clustering, we uncovered informative and distinctive clusters of documents with similar \n                                content in the corpus. The automation of these components in the evidence synthesis process for international development grey literature \n                                mitigates the effort and time that is required for manual evidence synthesis. \n                            </p>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../constants';\nexport default {\n  name: 'NLPIntDevGrayLit',\n  mounted() {\n    enableScrollUpOnCarousel('#NLPIntLitDevCarousel');\n    enableSwipeOnCarousel('#NLPIntLitDevCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\nh6 {\n    font-weight: bold;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.int-dev-lit-carousel-item img {\n  height: 40vh;\n  max-height: 500px;\n}\n\n@media (max-width: 500px) {\n    h3 {\n        font-size: 4vw;\n    }\n}\n\n@media (max-width: 800px) {\n    .int-dev-lit-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./NLPIntDevGrayLit.vue?vue&type=template&id=7915a20e&scoped=true&\"\nimport script from \"./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"\nexport * from \"./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"\nimport style0 from \"./NLPIntDevGrayLit.vue?vue&type=style&index=0&id=7915a20e&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"7915a20e\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h3',[_vm._v(\"Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"TaxiCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#TaxiCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#TaxiCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#TaxiCarousel\",\"data-slide-to\":\"2\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active taxi-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/taxi-fare-and-surge-pred.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Motivation\")]),_c('p',[_vm._v(\" With the advent of ridesharing apps, the New York City taxi industry must provide accurate predictions for taxi fares and demand surges in order to remain competitive. It is important that it provides riders with accurate estimates for the price of trip fare for quality customer service. It is beneficial for the taxi industry to accurately predict locations that will experience increased demand or surges in taxi demand to effectively allocate drivers and cabs to these areas in a timely manner. In this project, we used a large public dataset provided by the NYC Taxi & Limousine Commission containing yellow taxi trips records for every month from 2015-Present. We specifically utilized data from January 2019-June 2019. \")])])]),_c('div',{staticClass:\"carousel-item taxi-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/fare-surge-graph-pred.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Graph Neural Networks for Taxi Fare and Surge Prediction\")]),_c('p',[_vm._v(\" The Yellow Taxi Trip Records contain data about taxi trips including pickup and dropoff locations, date and time of pickup and dropoff, distance traveled, and the associated fare of the trip. This data has inherent graph structure in which the nodes are the different pickup/dropoff locations and the edges are the directed trips between them. Due to the increasing advances of Graph Neural Networks (GNNs) in recent years and the advent of efficient frameworks like Deep Graph Library (DGL), we evaluated the performance of GNNs against other classical machine learning methods to assess the viability of GNNs as an accurate model which leverages the inherent graph structure in the data used for these tasks. \")])])]),_c('div',{staticClass:\"carousel-item taxi-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/fare-surge-graph-pred.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Results\")]),_c('p',[_vm._v(\" Using the raw yellow taxi trip data from January 2019-June 2019, we contrived two datasets to pose node and regression problems for graphical methods. Using GraphSage GNNs, we explored and benchmarked a new application of GNNs to taxi data against classical ML approaches. For fare prediction, the GNN model performed slightly worse than Linear Regression, Random Forests, and a Fully-Connected Neural Network (FC NN), with the FC NN performing the best, however, it is highly-parameterized. For surge prediction, the GNN performed slightly better than a FC NN. For both taxi prediction tasks, we demonstrated that a less complex GNN model can perform comparably to a highly-parameterized FC NN. \")])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h3>Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"TaxiCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#TaxiCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#TaxiCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#TaxiCarousel\" data-slide-to=\"2\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active taxi-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/taxi-fare-and-surge-pred.png\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Motivation</h5>\n                            <p>\n                                With the advent of ridesharing apps, the New York City taxi industry must provide accurate predictions for taxi fares and demand surges in order to remain competitive.\n                                It is important that it provides riders with accurate estimates for the price of trip fare for quality customer service.\n                                It is beneficial for the taxi industry to accurately predict locations that will experience increased demand or surges in taxi demand to effectively allocate\n                                drivers and cabs to these areas in a timely manner. In this project, we used a large public dataset provided by the \n                                NYC Taxi & Limousine Commission containing yellow taxi trips records for every month from 2015-Present. \n                                We specifically utilized data from January 2019-June 2019.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item taxi-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/fare-surge-graph-pred.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Graph Neural Networks for Taxi Fare and Surge Prediction</h5>\n                            <p> \n                                The Yellow Taxi Trip Records contain data about taxi trips including pickup and dropoff locations, date and time of pickup and dropoff,\n                                distance traveled, and the associated fare of the trip.  This data has inherent graph structure in which the nodes are the different pickup/dropoff locations\n                                and the edges are the directed trips between them. Due to the increasing advances of Graph Neural Networks (GNNs) in recent years and the advent of efficient frameworks like Deep Graph Library (DGL),\n                                we evaluated the performance of GNNs against other classical machine learning methods to assess the viability of GNNs as an accurate model which leverages the inherent graph structure in the data used for these tasks.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item taxi-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/fare-surge-graph-pred.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Results</h5>\n                            <p> \n                                Using the raw yellow taxi trip data from January 2019-June 2019, we contrived two datasets\n                                to pose node and regression problems for graphical methods. Using GraphSage GNNs, we explored and benchmarked\n                                a new application of GNNs to taxi data against classical ML approaches. For fare prediction, the GNN model performed slightly worse\n                                than Linear Regression, Random Forests, and a Fully-Connected Neural Network (FC NN), with the FC NN performing the best, however, it is highly-parameterized. For surge prediction, the GNN performed slightly better than a FC NN.\n                                For both taxi prediction tasks, we demonstrated that a less complex GNN model can perform comparably to a highly-parameterized FC NN.\n                            </p>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../constants';\n\nexport default {\n  name: 'Taxi',\n  mounted() {\n    enableScrollUpOnCarousel('#TaxiCarousel');\n    enableSwipeOnCarousel('#TaxiCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.taxi-carousel-item img {\n  height: 450px;\n  max-height: 500px;\n}\n\n@media (max-width: 800px) {\n    .taxi-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./GNNsTaxiPrediction.vue?vue&type=template&id=385cd9ad&scoped=true&\"\nimport script from \"./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"\nexport * from \"./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"\nimport style0 from \"./GNNsTaxiPrediction.vue?vue&type=style&index=0&id=385cd9ad&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"385cd9ad\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h1',[_vm._v(\"Trump Campaign Speech Analysis\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"TrumpCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#TrumpCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#TrumpCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#TrumpCarousel\",\"data-slide-to\":\"2\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active trump-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/trump-campaign.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Main Puzzle\")]),_c('p',[_vm._v(\" There have been concerns that nationalist, right-wing sentiments have gained momentum over the years of the Trump presidency. Our group wanted to investigate how Donald Trumps rhetoric may have influenced public sentiment on a regional level. To this end, we analyzed Trump's campaign speeches and the tweets by locals from 4 cities he visited on his campaign and Florida, a swing state. \")])])]),_c('div',{staticClass:\"carousel-item trump-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/negative-positive.jpg\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Most Frequent Negative and Positive Words in Trump's Campaign Speeches\")]),_c('p',[_vm._v(\" Trump's positive sentiment words tend to be adjectives with \\\"great\\\" far exceeding the rest. Among words with negative sentiment, there are more meaningful words related to his speech topics such as \\\"investigation\\\", \\\"defense\\\", \\\"deficit\\\", & \\\"press\\\". \")])])]),_c('div',{staticClass:\"carousel-item trump-carousel-item\"},[_c('img',{staticClass:\"col-12 rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/RelativeWordFrequencyDiff.png\")}}),_c('img',{staticClass:\"col-12 rounded img-fluid pt-4\",attrs:{\"src\":require(\"../../../public/assets/RelativeWordFreqDiffFlorida.png\")}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Trump's Most Frequently Used Words Across his Entire Campaign & Across Florida Campaign\")]),_c('p',[_vm._v(\" In Trump's speeches across the entire campaign, his most frequent words, normalized on Romney's campaign speeches, include \\\"Hillary\\\", \\\"don't\\\" \\\"great\\\", \\\"deal\\\", as well as words related to his election platform such as \\\"border\\\", \\\"wall\\\", \\\"Mexico\\\", \\\"ISIS\\\", \\\"trade\\\", and \\\"China\\\". Words used to thwart Hillary Clinton's campaign such as \\\"Hillary\\\", \\\"email\\\", \\\"lies\\\", \\\"corrupt\\\", \\\"crook\\\", and \\\"FBI\\\" in regards to Clinton's email scandal appear more frequently in Trump's Florida campaign speeches than across all of his campaign speeches, showing that in swing states, Trump strategizes to mention the scandal more frequently to win voters to tip the scale. \")])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h1>Trump Campaign Speech Analysis</h1>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"TrumpCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#TrumpCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#TrumpCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#TrumpCarousel\" data-slide-to=\"2\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active trump-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/trump-campaign.png\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Main Puzzle</h5>\n                            <p>\n                                There have been concerns that nationalist, right-wing sentiments have gained momentum over the years of the Trump presidency. \n                                Our group wanted to investigate how Donald Trumps rhetoric may have influenced public sentiment on a regional level. \n                                To this end, we analyzed Trump's campaign speeches and the tweets by locals from 4 cities he visited on his campaign and Florida,\n                                a swing state.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item trump-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/negative-positive.jpg\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Most Frequent Negative and Positive Words in Trump's Campaign Speeches</h5>\n                            <p> \n                                Trump's positive sentiment words tend to be adjectives with \"great\" far exceeding the rest. Among words with negative sentiment, \n                                there are more meaningful words related to his speech topics such as \"investigation\", \"defense\", \"deficit\", & \"press\". \n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item trump-carousel-item\">\n                        <img class=\"col-12 rounded img-fluid\" src=\"../../../public/assets/RelativeWordFrequencyDiff.png\">\n                        <img class=\"col-12 rounded img-fluid pt-4\" src=\"../../../public/assets/RelativeWordFreqDiffFlorida.png\">\n                        <div class=\"carousel-text\">\n                            <h5>Trump's Most Frequently Used Words Across his Entire Campaign & Across Florida Campaign</h5>\n                            <p> \n                                In Trump's speeches across the entire campaign, his most frequent words, normalized on Romney's campaign speeches, include\n                                \"Hillary\", \"don't\" \"great\", \"deal\", as well as words related to his election platform such as \"border\", \"wall\", \"Mexico\", \"ISIS\", \"trade\", \n                                and \"China\". Words used to thwart Hillary Clinton's campaign such as \"Hillary\", \"email\", \"lies\", \"corrupt\", \"crook\", and \"FBI\" in regards to Clinton's email scandal \n                                appear more frequently in Trump's Florida campaign speeches than across all of his campaign speeches, showing that in swing states, \n                                Trump strategizes to mention the scandal more frequently to win voters to tip the scale.\n                            </p>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../constants';\n\nexport default {\n  name: 'Trump',\n  mounted() {\n    enableScrollUpOnCarousel('#TrumpCarousel');\n    enableSwipeOnCarousel('#TrumpCarousel')\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1 {\n    color: white;\n}\n\nh5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.trump-carousel-item img {\n  height: 450px;\n  max-height: 500px;\n}\n\n@media (max-width: 800px) {\n    .trump-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./TrumpSpeechAnalysis.vue?vue&type=template&id=3b803d90&scoped=true&\"\nimport script from \"./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"\nexport * from \"./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"\nimport style0 from \"./TrumpSpeechAnalysis.vue?vue&type=style&index=0&id=3b803d90&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"3b803d90\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h3',[_vm._v(\"Evolution of the U.S. TV News Narrative on Climate Change\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"ClimateNewsCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#ClimateNewsCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#ClimateNewsCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#ClimateNewsCarousel\",\"data-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-target\":\"#ClimateNewsCarousel\",\"data-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-target\":\"#ClimateNewsCarousel\",\"data-slide-to\":\"4\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/final-project-overview.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Project Code, Poster, and Report\")]),_c('p',[_vm._v(\" This was a final project for IDS.131: Statistics, Computation, and Applications. This presentation focuses on introducing the project, the specific parts I worked on, and the main findings from our analysis. \"),_c('br'),_vm._v(\" A brief overview of the methods, visualizations, and results is available in this \"),_c('a',{attrs:{\"href\":\"./assets/IDS131_Poster.pdf\",\"target\":\"_blank\"}},[_vm._v(\"poster PDF.\")]),_c('br'),_vm._v(\" The more thorough report of our findings with all visualizations is \"),_c('a',{attrs:{\"href\":\"./assets/IDS131_Final_Report.pdf\",\"target\":\"_blank\"}},[_vm._v(\"here.\")]),_c('br'),_vm._v(\" The code for this project was written in Python. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/ids.131-fp\",\"target\":\"_blank\"}},[_vm._v(\"Here's the GitHub Repo.\")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_c('strong',[_vm._v(\"Motivation & Research Question\")])]),_c('p',[_vm._v(\" Print and televised media reporting on climate change influences the public perception of climate change, which in turn affects support for systemic policies to reduce greenhouse gas emissions and for individual actions to mitigate climate change. Over two thirds of Americans get their news often or sometimes from television. In this analysis, we looked at ten years of data from three television stations: CNN, Fox News, and MSNBC to address the following research question: \")]),_c('br'),_c('strong',{attrs:{\"id\":\"research-question\"}},[_vm._v(\" How has the frequency and content of top American English-speaking news media coverage of climate change evolved in the past ten years (July 2009-January 2020)-and what environmental and political factors have influenced the trends? \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/network_tfidf_wordclouds.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Dataset, Exploration, and Preprocessing\")]),_c('p',[_vm._v(\" The data used for this analysis includes text snippets of 15-seconds of TV news audio transcripts of climate change mentions on CNN, MSNBC, and Fox News from July 2009-January 2020, provided by the \"),_c('a',{attrs:{\"href\":\"https://blog.gdeltproject.org/a-new-dataset-for-exploring-climate-change-narratives-on-television-news-2009-2020/\",\"target\":\"_blank\"}},[_vm._v(\"GDELT Project.\")]),_vm._v(\" The features of the data points include time of day and date of the mention, the TV news network, the show, and the text snippet of the transcribed audio. This dataset provides the ability to compare the TV networks over time on the subject of climate change in order to answer the research question posed. We followed standard Natural Language Processing (NLP) text preprocessing by removing punctuation and numbers, converting all letters to lower-case (the data was already provided as lowercase), lemmatizing, removing standard English stopwords & corpus-specific stopwords, and tokenizing the data into words. We conducted our analysis with two distinct analysis streams: Frequency Analysis and Content Analysis. Our Frequency Analysis entailed methods of time-series analysis of climate change mentions by the different TV networks over time and dynamic time-warping & STL decomposition. Content Analysis used methods of TF-IDF document embeddings, Cosine Similarity as a proxy for content similarity between documents, and Topic Modeling & Change-Point Detection. The above figure was created as part of an exploratory component of the TF-IDF Embedding analysis, in which we wished to extract the most important words to each of the networks across the entire corpus. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/networks_and_years_cosine_similarity.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Content Analysis: TF-IDF Document Embedding & Cosine Similarity\")]),_c('p',[_vm._v(\" My contribution to this project was primarly focused in the Content Analysis stream, specifically TF-IDF document embeddings & Cosine Similarity between documents. To conduct our content analysis, we needed to featurize the news snippets, or collections of snippets, which form the documents in our corpus. I first formed a document for each TV network in our dataset (e.g. all snippets for CNN), and transformed the documents into a L2-normalized unit vector TF-IDF vector embedding. From this featurization, I formed a document-term matrix, where the rows correspond to the TF-IDF embedding of a document and the column represents a unique word in the corpus (~34,000 words). Thus, entry i, j corresponds to the normalized TF-IDF score of word j in document i (i.e. word j's relative importance for the ith document). I also constructed document-term matrices for documents representing each year of our dataset (e.g. all snippets in 2009) as well as for documents constructed from networks in specific years (e.g. all CNN snippets in 2015). Finally, for each of the document-term matrices mentioned above, I calculated the pairwise cosine similarity between the document embeddings to yield a measure of content similarity between the documents. A heatmap constructed from the computed cosine similarities between the network & year documents is shown above. The main findings from my analysis were that the content of climate mentions in the latter years of the dataset, 2016-2020, are most dissimilar to earlier years in the dataset, 2009-2012. Additionally, in the years 2010 & 2018, the content of MSNBC differed greatly from the other news networks in those years and with other networks and itself in other years. This similarly occurred for CNN in 2012 & 2013. Lastly, with the exception of these years, the similarity of content of climate mentions between CNN and MSNBC, the liberal-leaning networks, has been increasing over the years, although the pairwise content similarity between all of the networks is fairly high over time. \")])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/final-project-overview.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Results\")]),_c('p',[_vm._v(\" Climate change TV news media coverage frequency and content appears to be significantly driven by political events more so than environmental factors. The frequency of climate change mentions follow similar patterns by network, with clear influence of political events such as the 2009 UNCCC, 2015 Paris Agreement, and 2019 Democratic primary debates driving climate news coverage. This is reflected in the content of the climate mentions over time as words describing the political events occurring at the time tend to be the most important words for each of the networks in that specific year coupled with the tendency of different networks in the same year to have high content similarity. Topic analysis also finds that the majority of 15 topics found in topic analysis had significant changes in mean on some of the topics at the time of Donald Trump's inauguration. \")])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h3>Evolution of the U.S. TV News Narrative on Climate Change</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"ClimateNewsCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#ClimateNewsCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#ClimateNewsCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#ClimateNewsCarousel\" data-slide-to=\"2\"></li>\n                    <li data-target=\"#ClimateNewsCarousel\" data-slide-to=\"3\"></li>\n                    <li data-target=\"#ClimateNewsCarousel\" data-slide-to=\"4\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active cc-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/final-project-overview.png\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Project Code, Poster, and Report</h5>\n                            <p>\n                                This was a final project for IDS.131: Statistics, Computation, and Applications. This presentation focuses on\n                                introducing the project, the specific parts I worked on, and the main findings from our analysis.\n                                <br>\n                                A brief overview of the methods, visualizations, \n                                and results is available in this <a href=\"./assets/IDS131_Poster.pdf\" target=\"_blank\">poster PDF.</a>\n                                <br>\n                                The more thorough report of our findings with all visualizations is <a href=\"./assets/IDS131_Final_Report.pdf\" target=\"_blank\">here.</a>\n                                <br>\n                                The code for this project was written in Python. <a href=\"https://github.com/dyllew/ids.131-fp\" target=\"_blank\">Here's the GitHub Repo.</a>\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5><strong>Motivation & Research Question</strong></h5>\n                            <p>\n                                Print and televised media reporting on climate change influences \n                                the public perception of climate change, which in turn affects support for \n                                systemic policies to reduce greenhouse gas emissions and for individual actions to \n                                mitigate climate change. Over two thirds of Americans get their news often or \n                                sometimes from television. In this analysis, we looked at ten years of \n                                data from three television stations: CNN, Fox News, and MSNBC to address the \n                                following research question:\n                            </p>\n                            <br>\n                            <strong id='research-question'>\n                                How has the frequency and content of \n                                top American English-speaking news media coverage of climate change \n                                evolved in the past ten years (July 2009-January 2020)-and what environmental and political factors \n                                have influenced the trends?\n                            </strong>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/network_tfidf_wordclouds.png\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Dataset, Exploration, and Preprocessing</h5>\n                            <p>\n                                The data used for this analysis includes text snippets of 15-seconds of TV news audio transcripts\n                                of climate change mentions on CNN, MSNBC, and Fox News from \n                                July 2009-January 2020, provided by the <a href=\"https://blog.gdeltproject.org/a-new-dataset-for-exploring-climate-change-narratives-on-television-news-2009-2020/\" target=\"_blank\">GDELT Project.</a>\n                                The features of the data points include time of day and date of the mention, \n                                the TV news network, the show, and the text snippet of the transcribed audio. \n                                This dataset provides the ability to compare the TV networks over time on the subject of \n                                climate change in order to answer the research question posed. \n                                We followed standard Natural Language Processing (NLP) text preprocessing by removing punctuation and numbers, \n                                converting all letters to lower-case (the data was already provided as lowercase), lemmatizing, removing standard English stopwords & \n                                corpus-specific stopwords, and tokenizing the data into words. We conducted our analysis with two distinct\n                                analysis streams: Frequency Analysis and Content Analysis. Our Frequency Analysis entailed methods of time-series analysis of climate change mentions\n                                by the different TV networks over time and dynamic time-warping & STL decomposition. Content Analysis used methods of TF-IDF document embeddings,\n                                Cosine Similarity as a proxy for content similarity between documents, and Topic Modeling & Change-Point Detection. The above figure was created as part of \n                                an exploratory component of the TF-IDF Embedding analysis, in which we wished to extract the most important words to each of the networks across the entire corpus.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/networks_and_years_cosine_similarity.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Content Analysis: TF-IDF Document Embedding & Cosine Similarity</h5>\n                            <p> \n                                My contribution to this project was primarly focused in the Content Analysis stream, specifically TF-IDF document embeddings & \n                                Cosine Similarity between documents. To conduct our content analysis, we needed to featurize the news snippets, or collections of snippets, \n                                which form the documents in our corpus. I first formed a document for each TV network in our dataset\n                                (e.g. all snippets for CNN), and transformed the documents into a L2-normalized unit vector TF-IDF vector embedding. \n                                From this featurization, I formed a document-term matrix, where the rows correspond to the TF-IDF embedding of a document and the column \n                                represents a unique word in the corpus (~34,000 words). Thus, entry i, j corresponds to the normalized \n                                TF-IDF score of word j in document i (i.e. word j's relative importance for the ith document). I also constructed document-term matrices for documents representing each year of our dataset\n                                (e.g. all snippets in 2009) as well as for documents constructed from networks in specific years (e.g. all CNN snippets in 2015). Finally, for each of the document-term matrices mentioned above, \n                                I calculated the pairwise cosine similarity between the document embeddings to yield a measure of content similarity between the documents. \n                                A heatmap constructed from the computed cosine similarities between the network & year documents is shown above. The main findings from my analysis were \n                                that the content of climate mentions in the latter years of the dataset, 2016-2020, are most dissimilar to earlier years in the dataset, 2009-2012. \n                                Additionally, in the years 2010 & 2018, the content of MSNBC differed greatly from the other news networks in those years and with other networks and itself in other years. \n                                This similarly occurred for CNN in 2012 & 2013. Lastly, with the exception of these years, the similarity of content of climate mentions between CNN and MSNBC, \n                                the liberal-leaning networks, has been increasing over the years, although the pairwise content similarity between all of the networks is fairly high over time.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/final-project-overview.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Results</h5>\n                            <p> \n                                Climate change TV news media coverage frequency and \n                                content appears to be significantly driven by political events\n                                 more so than environmental factors. The frequency of climate change mentions \n                                 follow similar patterns by network, with clear influence of political events such as \n                                 the 2009 UNCCC, 2015 Paris Agreement, and 2019 Democratic primary debates driving \n                                 climate news coverage. This is reflected in the content of the \n                                 climate mentions over time as words describing the political events \n                                 occurring at the time tend to be the most important words for each of the networks \n                                 in that specific year coupled with the tendency of different networks in the same \n                                 year to have high content similarity. Topic analysis also finds that the majority of 15 topics \n                                 found in topic analysis had significant changes in mean on some of the topics at the time of \n                                 Donald Trump's inauguration.\n                            </p>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../constants';\nexport default {\n  name: 'ClimateChangeNews',\n  mounted() {\n    enableScrollUpOnCarousel('#ClimateNewsCarousel');\n    enableSwipeOnCarousel('#ClimateNewsCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\n\nh1, h3, h5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.cc-carousel-item img {\n  max-height: 30vw;\n}\n\n@media (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ClimateChangeNews.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ClimateChangeNews.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./ClimateChangeNews.vue?vue&type=template&id=37456a1b&scoped=true&\"\nimport script from \"./ClimateChangeNews.vue?vue&type=script&lang=js&\"\nexport * from \"./ClimateChangeNews.vue?vue&type=script&lang=js&\"\nimport style0 from \"./ClimateChangeNews.vue?vue&type=style&index=0&id=37456a1b&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"37456a1b\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\",attrs:{\"id\":\"title\"}},[_c('h1',[_vm._v(\"Boomerang\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"BoomerangCarousel\",\"data-ride\":\"carousel\",\"data-interval\":\"false\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-target\":\"#BoomerangCarousel\",\"data-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-target\":\"#BoomerangCarousel\",\"data-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-target\":\"#BoomerangCarousel\",\"data-slide-to\":\"2\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active boomerang-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/boomerang-home.jpg\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Sign Up/Login Page\")]),_c('p',[_vm._v(\"When my group decided on how we wanted to split up the tasks for Boomerang, I decided to design & create the sign up/login page & the create account flow. I thought this would be a good section of the application to practice and enhance my visual design skills and to create a UI that was intuitive. \")])])]),_c('div',{staticClass:\"carousel-item boomerang-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/create-account.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Create Account Part I\")]),_c('p',[_vm._v(\" In the sections of Boomerang I built, I created the front-end using Vue.js which is the same front-end framework I used to build this website. The backend was built using Express.js. The fields above checked for user input to ensure that the account username was not already taken and that the each of the fields was in the correct format, notifying the user instantly on submission if their input was invalid. \")])])]),_c('div',{staticClass:\"carousel-item boomerang-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/join-communities.png\"),\"alt\":\"Third slide\"}}),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Create Account Part II\")]),_c('p',[_vm._v(\" Account creation for any app is an important user flow as it lets the user understand both the purpose of an app & how they can engage with it completely. For these reasons, I decided to include descriptions of the main concepts of the application (Communities, Channels, etc.) as this would reduce the time it would take for a user to get immersed in the app. \")])])])])])])])}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div id=\"title\" class=\"col-12\">  \n            <h1>Boomerang</h1>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"BoomerangCarousel\" class=\"carousel slide\" data-ride=\"carousel\" data-interval=\"false\">\n                <ol class=\"carousel-indicators\">\n                    <li data-target=\"#BoomerangCarousel\" data-slide-to=\"0\" class=\"active\"></li>\n                    <li data-target=\"#BoomerangCarousel\" data-slide-to=\"1\"></li>\n                    <li data-target=\"#BoomerangCarousel\" data-slide-to=\"2\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active boomerang-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/boomerang-home.jpg\" alt=\"First slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Sign Up/Login Page</h5>\n                            <p>When my group decided on how we wanted to split up the tasks for Boomerang,\n                                I decided to design & create the sign up/login page & the create account flow. \n                                I thought this would be a good section of the application to practice and enhance my visual design skills\n                                and to create a UI that was intuitive.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item boomerang-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/create-account.png\" alt=\"Second slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Create Account Part I</h5>\n                            <p> \n                                In the sections of Boomerang I built, I created the front-end using \n                                Vue.js which is the same front-end framework I used to build this website. \n                                The backend was built using Express.js. The fields above checked for user input\n                                to ensure that the account username was not already taken and that the each of the fields was\n                                in the correct format, notifying the user instantly on submission if their input was invalid.\n                            </p>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item boomerang-carousel-item\">\n                        <img class=\"rounded img-fluid\" src=\"../../../public/assets/join-communities.png\" alt=\"Third slide\">\n                        <div class=\"carousel-text\">\n                            <h5>Create Account Part II</h5>\n                            <p> \n                                Account creation for any app is an important user flow as it\n                                lets the user understand both the purpose of an app & how they can engage with it completely.\n                                For these reasons, I decided to include descriptions of the main concepts of the application (Communities, Channels, etc.)\n                                as this would reduce the time it would take for a user to get immersed in the app.\n                            </p>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { enableScrollUpOnCarousel, enableSwipeOnCarousel } from '../../constants';\nexport default {\n  name: 'Boomerang',\n  mounted() {\n    enableScrollUpOnCarousel(\"#BoomerangCarousel\");\n    enableSwipeOnCarousel(\"#BoomerangCarousel\");\n  }\n}\n</script>\n\n<style scoped>\n\nh1, h5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.boomerang-carousel-item img {\n  height: 450px;\n  max-height: 500px;\n}\n\n@media (max-width: 800px) {\n    .boomerang-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Boomerang.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Boomerang.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Boomerang.vue?vue&type=template&id=461c2fc6&scoped=true&\"\nimport script from \"./Boomerang.vue?vue&type=script&lang=js&\"\nexport * from \"./Boomerang.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Boomerang.vue?vue&type=style&index=0&id=461c2fc6&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"461c2fc6\",\n  null\n  \n)\n\nexport default component.exports","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col pt-5 justify-content-center\"},[_c('h2',[_vm._v(\"Uh Oh! Looks like you went to a page that doesn't exist on dyllew.github.io\")]),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/\"}},[_vm._v(\"Click this link to go home.\")])],1)}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"col pt-5 justify-content-center\">\n      <h2>Uh Oh! Looks like you went to a page that doesn't exist on dyllew.github.io</h2>\n      <router-link class=\"router-link\" to=\"/\">Click this link to go home.</router-link>\n    </div>  \n</template>\n\n<script>\nexport default {\n  name: 'NotFoundComponent'\n}\n</script>\n\n<style scoped>\n\n.router-link {\n  color: #61DAFB;\n  font-size: 30px;\n}\n\n.router-link:hover {\n  color: whitesmoke;\n}\n\n.router-link-active {\n  color: white;\n  text-decoration: underline;\n}\n\n\n</style>","import mod from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NotFoundComponent.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/cache-loader/dist/cjs.js??ref--12-0!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NotFoundComponent.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./NotFoundComponent.vue?vue&type=template&id=4ffa6c61&scoped=true&\"\nimport script from \"./NotFoundComponent.vue?vue&type=script&lang=js&\"\nexport * from \"./NotFoundComponent.vue?vue&type=script&lang=js&\"\nimport style0 from \"./NotFoundComponent.vue?vue&type=style&index=0&id=4ffa6c61&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"4ffa6c61\",\n  null\n  \n)\n\nexport default component.exports","import Vue from 'vue';\nimport VueRouter from 'vue-router';\n\nVue.use(VueRouter);\n\n// import components below\nimport Home from './components/Home'\nimport About from './components/About';\nimport Projects from './components/Projects';\nimport Artwork from './components/Artwork';\nimport Resume from './components/Resume';\nimport MLForCrowdsourcedCrisisData from './components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData';\nimport ImageAnalysisCarousel from './components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel';\nimport TextAnalysisCarousel from './components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel';\nimport NLPIntDevGrayLit from './components/project-pages/NLPIntDevGrayLit';\nimport GNNsTaxiPrediction from './components/project-pages/GNNsTaxiPrediction';\nimport TrumpSpeechAnalysis from './components/project-pages/TrumpSpeechAnalysis';\nimport ClimateChangeNews from './components/project-pages/ClimateChangeNews';\nimport Boomerang from './components/project-pages/Boomerang';\nimport NotFoundComponent from './components/NotFoundComponent';\n\n// store router -> components mappings\nconst router = [\n    {\n        path: '/',\n        component: Home\n    },\n    {\n        path: '/about',\n        component: About\n    },\n    {\n        path: '/projects',\n        component: Projects\n    },\n    {\n        path: '/projects/boomerang',\n        component: Boomerang\n    },\n    {\n        path: '/projects/ml-for-crowdsourced-crisis-data',\n        component: MLForCrowdsourcedCrisisData\n    },\n    {\n        path: '/projects/ml-for-crowdsourced-crisis-data/image-analysis-module',\n        component: ImageAnalysisCarousel\n    },\n    {\n        path: '/projects/ml-for-crowdsourced-crisis-data/text-analysis-module',\n        component: TextAnalysisCarousel\n    },\n    {\n        path: '/projects/nlp-for-int-dev-gray-lit',\n        component: NLPIntDevGrayLit\n    },\n    {\n        path: '/projects/trump-speech-analysis',\n        component: TrumpSpeechAnalysis\n    },\n    {\n        path: '/projects/gnns-taxi-prediction',\n        component: GNNsTaxiPrediction\n    },\n    {\n        path: '/projects/climate-change-news',\n        component: ClimateChangeNews\n    },\n    {\n        path: '/artwork',\n        component: Artwork\n    },\n    {\n        path: '/resume',\n        component: Resume\n    },\n    { \n        path: '/404', \n        component: NotFoundComponent\n    },  \n    {\n        path: '*',\n        redirect: '/404'\n    }\n\n];\n\nconst vueRouter = new VueRouter({\n    mode: 'hash',\n    routes: router\n});\n\nexport default vueRouter;","import Vue from 'vue';\nimport { BootstrapVue, IconsPlugin } from 'bootstrap-vue';\nimport App from './App.vue';\n\nimport router from './router';\nimport 'bootstrap/dist/css/bootstrap.css';\nimport 'bootstrap-vue/dist/bootstrap-vue.css';\n\nVue.config.productionTip = false;\nVue.use(BootstrapVue);\nVue.use(IconsPlugin);\n\nnew Vue({\n  router,\n  render: h => h(App)\n}).$mount('#app')\n","module.exports = __webpack_public_path__ + \"img/iaa.ec440fef.png\";","module.exports = __webpack_public_path__ + \"img/human-risk-per-class-metric.640f3447.png\";","module.exports = __webpack_public_path__ + \"img/boomerang-home.2b52b305.jpg\";","module.exports = __webpack_public_path__ + \"img/bert-features.ad97d9cb.png\";","module.exports = __webpack_public_path__ + \"img/test-set-eval.d9e2b629.png\";","module.exports = __webpack_public_path__ + \"img/network_tfidf_wordclouds.295ee901.png\";","module.exports = __webpack_public_path__ + \"img/character_box_and_whisk_fc_rm.f4c1b25c.png\";","module.exports = __webpack_public_path__ + \"img/clustering-pipeline.f790fd33.png\";","module.exports = __webpack_public_path__ + \"img/elbow-plot.3191280c.png\";","module.exports = __webpack_public_path__ + \"img/in-fc.af1ca061.png\";","module.exports = __webpack_public_path__ + \"img/flood_confusion_matrix.7d011586.png\";","module.exports = __webpack_public_path__ + \"img/informativeness_confusion_matrix.e14fee3e.png\";","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./About.vue?vue&type=style&index=0&id=e91b748a&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./About.vue?vue&type=style&index=0&id=e91b748a&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/cluster-labels.25777af2.png\";","import mod from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NLPIntDevGrayLit.vue?vue&type=style&index=0&id=7915a20e&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./NLPIntDevGrayLit.vue?vue&type=style&index=0&id=7915a20e&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/text-preprocessing.9ad10bff.png\";","module.exports = __webpack_public_path__ + \"img/tf-idf-features.59e0d1a5.png\";","module.exports = __webpack_public_path__ + \"img/image-analysis-module-modified.2f3d5731.png\";","module.exports = __webpack_public_path__ + \"img/NER.042f94f2.png\";","module.exports = __webpack_public_path__ + \"01fbdfc68fb18b120d93f81bebddbbe3.pdf\";","module.exports = __webpack_public_path__ + \"img/workshop-preface-questions.fe9b1412.png\";","module.exports = __webpack_public_path__ + \"img/human-risk-cm-svm.14428877.png\";","module.exports = __webpack_public_path__ + \"img/qualitative-summaries.35be113c.png\";","module.exports = __webpack_public_path__ + \"img/fare-surge-graph-pred.16940831.png\";","module.exports = __webpack_public_path__ + \"img/preliminary-assessment.1b02d22b.png\";","module.exports = __webpack_public_path__ + \"img/years_cosine_similarity.1e4e7357.png\";","module.exports = __webpack_public_path__ + \"img/nested-cv-table.5caba9aa.png\";","module.exports = __webpack_public_path__ + \"img/trump-campaign.81aaea0e.png\";","module.exports = __webpack_public_path__ + \"img/project-collage.6966ef19.png\";","module.exports = __webpack_public_path__ + \"img/tfidf-networks.c48a6bce.png\";","module.exports = __webpack_public_path__ + \"img/hc-fc.f248f6e2.png\";","module.exports = __webpack_public_path__ + \"img/damage_severity_per_class_metric.b0e58f3a.png\";","module.exports = __webpack_public_path__ + \"img/ds-train.fff83a4a.png\";","module.exports = __webpack_public_path__ + \"img/txt-data-collection.0d5d50dc.png\";","module.exports = __webpack_public_path__ + \"img/pika-gif.6999dd5c.gif\";","module.exports = __webpack_public_path__ + \"img/join-communities.392bb659.png\";","module.exports = __webpack_public_path__ + \"img/resume.851be6c2.png\";","module.exports = __webpack_public_path__ + \"img/reptile.b479d166.png\";","module.exports = __webpack_public_path__ + \"img/masters-thesis-overview.b92e2d31.png\";","module.exports = __webpack_public_path__ + \"img/damage_severity_confusion_matrix.baa9b21a.png\";","module.exports = __webpack_public_path__ + \"img/network_cosine_similarity.0a83a38c.png\";","module.exports = __webpack_public_path__ + \"57c143e9704cb3120e7e05eaf2a3a1a6.pdf\";","module.exports = __webpack_public_path__ + \"img/in-train.cce4b41a.png\";","module.exports = __webpack_public_path__ + \"img/hc-train.314e090e.png\";","module.exports = __webpack_public_path__ + \"img/bag-of-word-features.b23627fe.png\";","module.exports = __webpack_public_path__ + \"img/feelings.7216f530.jpg\";","module.exports = __webpack_public_path__ + \"img/dylan-n-leo.621aa4fc.jpg\";","import mod from \"-!../../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./MLForCrowdsourcedCrisisData.vue?vue&type=style&index=0&id=79893eb2&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./MLForCrowdsourcedCrisisData.vue?vue&type=style&index=0&id=79893eb2&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/networks_and_years_cosine_similarity.3eaf2c24.png\";","module.exports = __webpack_public_path__ + \"img/linkedin-profpic.00cde042.jpg\";","module.exports = __webpack_public_path__ + \"img/query-subset.f2ebb281.png\";","import mod from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TrumpSpeechAnalysis.vue?vue&type=style&index=0&id=3b803d90&scoped=true&lang=css&\"; export default mod; export * from \"-!../../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./TrumpSpeechAnalysis.vue?vue&type=style&index=0&id=3b803d90&scoped=true&lang=css&\"","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ProjectCard.vue?vue&type=style&index=0&id=7bfc7dfc&scoped=true&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./ProjectCard.vue?vue&type=style&index=0&id=7bfc7dfc&scoped=true&lang=css&\"","module.exports = __webpack_public_path__ + \"img/humanitarian_categories_confusion_matrix.cd985937.png\";","module.exports = __webpack_public_path__ + \"img/image-analysis-module.c12d2c01.png\";","module.exports = __webpack_public_path__ + \"img/final-project-overview.13f71d33.png\";","module.exports = __webpack_public_path__ + \"img/create-account.b25be796.png\";","module.exports = __webpack_public_path__ + \"img/algo-selection-table.f0de65c9.png\";","module.exports = __webpack_public_path__ + \"img/taxi-proj-thumbnail.6dd3b85f.png\";","module.exports = __webpack_public_path__ + \"img/leo_n_me.2868644e.jpg\";","module.exports = __webpack_public_path__ + \"img/labeled-clusters.99662e14.png\";","module.exports = __webpack_public_path__ + \"img/tfidf-features.0e3e3126.png\";","module.exports = __webpack_public_path__ + \"img/RelativeWordFreqDiffFlorida.80584f3b.png\";","module.exports = __webpack_public_path__ + \"img/taxi-fare-and-surge-pred.064e07ad.png\";","module.exports = __webpack_public_path__ + \"img/clustering-hyperparameters.7c50d714.png\";","import mod from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Artwork.vue?vue&type=style&index=0&lang=css&\"; export default mod; export * from \"-!../../node_modules/mini-css-extract-plugin/dist/loader.js??ref--6-oneOf-1-0!../../node_modules/css-loader/dist/cjs.js??ref--6-oneOf-1-1!../../node_modules/vue-loader/lib/loaders/stylePostLoader.js!../../node_modules/postcss-loader/src/index.js??ref--6-oneOf-1-2!../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Artwork.vue?vue&type=style&index=0&lang=css&\"","module.exports = __webpack_public_path__ + \"img/nested-cv.2b8fde59.png\";"],"sourceRoot":""}