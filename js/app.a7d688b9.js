(function(e){function t(t){for(var i,r,n=t[0],c=t[1],l=t[2],h=0,u=[];h<n.length;h++)r=n[h],Object.prototype.hasOwnProperty.call(s,r)&&s[r]&&u.push(s[r][0]),s[r]=0;for(i in c)Object.prototype.hasOwnProperty.call(c,i)&&(e[i]=c[i]);d&&d(t);while(u.length)u.shift()();return o.push.apply(o,l||[]),a()}function a(){for(var e,t=0;t<o.length;t++){for(var a=o[t],i=!0,n=1;n<a.length;n++){var c=a[n];0!==s[c]&&(i=!1)}i&&(o.splice(t--,1),e=r(r.s=a[0]))}return e}var i={},s={app:0},o=[];function r(t){if(i[t])return i[t].exports;var a=i[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,r),a.l=!0,a.exports}r.m=e,r.c=i,r.d=function(e,t,a){r.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},r.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},r.t=function(e,t){if(1&t&&(e=r(e)),8&t)return e;if(4&t&&"object"===typeof e&&e&&e.__esModule)return e;var a=Object.create(null);if(r.r(a),Object.defineProperty(a,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var i in e)r.d(a,i,function(t){return e[t]}.bind(null,i));return a},r.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return r.d(t,"a",t),t},r.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},r.p="/";var n=window["webpackJsonp"]=window["webpackJsonp"]||[],c=n.push.bind(n);n.push=t,n=n.slice();for(var l=0;l<n.length;l++)t(n[l]);var d=c;o.push([0,"chunk-vendors"]),a()})({0:function(e,t,a){e.exports=a("56d7")},"0052":function(e,t,a){},"034f":function(e,t,a){"use strict";var i=a("85ec"),s=a.n(i);s.a},"06be":function(e,t,a){e.exports=a.p+"26b941884f7f4d6a0809608739be75ab.pdf"},"07c0":function(e,t,a){},"0abc":function(e,t,a){e.exports=a.p+"img/informativeness_per_class_metric.3013ed91.png"},"0af6":function(e,t,a){e.exports=a.p+"img/text-analysis-module.55d5baf1.png"},"0c67":function(e,t,a){e.exports=a.p+"img/flood_presence_per_class_metric.cf8e214a.png"},"166a":function(e,t,a){e.exports=a.p+"img/fp-train.88f043ab.png"},1913:function(e,t,a){var i={"./17_835_Poster.pdf":"4131","./6_864_Project.pdf":"1a9a","./Dylan_Lewis_Resume.pdf":"8050","./FrequencyPlot.png":"339e","./IDS131_Final_Report.pdf":"06be","./IDS131_Poster.pdf":"b720","./NER.png":"7d1a","./RelativeWordFreqDiffFlorida.png":"f061","./RelativeWordFrequencyDiff.png":"3ea6","./agg-metrics-fc.png":"2a76","./boomerang-home.jpg":"5e93","./create-account.png":"e1a7","./damage_severity_confusion_matrix.png":"aec1","./damage_severity_per_class_metric.png":"9469","./ds-fc.png":"4bd4","./ds-train.png":"964e","./dylan-n-leo.jpg":"c9e4","./fare-surge-graph-pred.png":"84e6","./feelings.jpg":"c207","./final-project-overview.png":"d8cb","./flood_confusion_matrix.png":"6e61","./flood_presence_per_class_metric.png":"0c67","./fp-fc.png":"1d5c","./fp-train.png":"166a","./hc-fc.png":"9409","./hc-train.png":"be19","./humanitarian_categories_confusion_matrix.png":"d54d","./humanitarian_categories_per_class_metric.png":"4f00","./iaa.png":"5984","./image-analysis-module-modified.png":"7b9c","./image-analysis-module.png":"d730","./in-fc.png":"6a64","./in-train.png":"bb54","./informativeness_confusion_matrix.png":"70f6","./informativeness_per_class_metric.png":"0abc","./int-dev-gray-lit.pdf":"4cdb","./int-dev-results.png":"269c","./join-communities.png":"a21f","./leo_n_me.jpg":"ee29","./linkedin-profpic.jpg":"cbeb","./masters-thesis-overview.png":"ab31","./negative-positive.jpg":"3bc0","./ner-res-tools.png":"2981","./ner-results.png":"332f","./network_cosine_similarity.png":"afd1","./network_tfidf_wordclouds.png":"64e1","./networks_and_years_cosine_similarity.png":"cad8","./pika-gif.gif":"9faf","./portrait.jpg":"4906","./project-collage.png":"8b08","./reptile.png":"a2c3","./resume.png":"a297","./taxi-fare-and-surge-pred.png":"f124","./taxi-proj-thumbnail.png":"e75b","./test-set-eval.png":"621b","./text-analysis-module.png":"0af6","./tfidf-networks.png":"9271","./trump-campaign.png":"89e4","./workshop-preface-questions.png":"835a","./years_cosine_similarity.png":"878d"};function s(e){var t=o(e);return a(t)}function o(e){if(!a.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}s.keys=function(){return Object.keys(i)},s.resolve=o,e.exports=s,s.id="1913"},"1a9a":function(e,t,a){e.exports=a.p+"6b8bd84b37ff0d5c4d59da59cd2fd588.pdf"},"1d5c":function(e,t,a){e.exports=a.p+"img/fp-fc.ef341c25.png"},"222a":function(e,t,a){"use strict";var i=a("f1bb"),s=a.n(i);s.a},"25d6":function(e,t,a){},"269c":function(e,t,a){e.exports=a.p+"img/int-dev-results.d88be6ab.png"},2981:function(e,t,a){e.exports=a.p+"img/ner-res-tools.ffa2482a.png"},"2a76":function(e,t,a){e.exports=a.p+"img/agg-metrics-fc.ecfcaeb6.png"},"2db4":function(e,t,a){"use strict";var i=a("38a8"),s=a.n(i);s.a},"2db8":function(e,t,a){"use strict";var i=a("d67d"),s=a.n(i);s.a},"2f27":function(e,t,a){"use strict";var i=a("3ccd"),s=a.n(i);s.a},3092:function(e,t,a){"use strict";var i=a("5db7"),s=a.n(i);s.a},"332f":function(e,t,a){e.exports=a.p+"img/ner-results.0168e441.png"},"336d":function(e,t,a){},"339e":function(e,t,a){e.exports=a.p+"img/FrequencyPlot.970f0c80.png"},"34b4":function(e,t,a){"use strict";var i=a("336d"),s=a.n(i);s.a},"38a8":function(e,t,a){},"3bc0":function(e,t,a){e.exports=a.p+"img/negative-positive.6d580dd8.jpg"},"3ccd":function(e,t,a){},"3ea6":function(e,t,a){e.exports=a.p+"img/RelativeWordFrequencyDiff.ea5b86c8.png"},4129:function(e,t,a){"use strict";var i=a("07c0"),s=a.n(i);s.a},4131:function(e,t,a){e.exports=a.p+"37b2baebe5028084933fc1d6ed8b2604.pdf"},4162:function(e,t,a){"use strict";var i=a("746f"),s=a.n(i);s.a},4781:function(e,t,a){"use strict";var i=a("5545"),s=a.n(i);s.a},4906:function(e,t,a){e.exports=a.p+"img/portrait.b2c89646.jpg"},"4bd4":function(e,t,a){e.exports=a.p+"img/ds-fc.33490c85.png"},"4cdb":function(e,t,a){e.exports=a.p+"6e061964729851cc0f11c268e292463e.pdf"},"4f00":function(e,t,a){e.exports=a.p+"img/humanitarian_categories_per_class_metric.d3a64368.png"},"54e2":function(e,t,a){"use strict";var i=a("0052"),s=a.n(i);s.a},5545:function(e,t,a){},"56d7":function(e,t,a){"use strict";a.r(t);var i=a("2b0e"),s=a("5f5b"),o=a("b1e0"),r=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"container-fluid p-3 min-vh-100",attrs:{id:"app"}},[a("Header"),"/"!==this.$route.path&&"/404"!==this.$route.path?a("NavBar"):e._e(),a("router-view")],1)},n=[],c=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row justify-content-end"},[a("div",{staticClass:"col-12 col-md-4 col-lg-4",attrs:{id:"website-title"}},[a("h1",{attrs:{id:"name animate__fadeInDown"},on:{click:e.goHome}},[e._v(" Dylan Lewis ")])]),e._m(0)])},l=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-md-4 my-auto",attrs:{id:"logos-col"}},[a("div",{staticClass:"d-flex flex-row justify-content-center justify-content-md-end"},[a("a",{attrs:{href:"mailto: dylanrl97@gmail.com"}},[a("i",{staticClass:"fa fa-envelope-o fa-3x"}),a("i",{staticClass:"fa fa-envelope-o fa-2x"})]),a("a",{attrs:{href:"https://www.linkedin.com/in/dyllew/",target:"_blank"}},[a("i",{staticClass:"fa fa-linkedin-square fa-3x"}),a("i",{staticClass:"fa fa-linkedin-square fa-2x"})]),a("a",{attrs:{href:"https://github.com/dyllew/",target:"_blank"}},[a("i",{staticClass:"fa fa-github fa-3x"}),a("i",{staticClass:"fa fa-github fa-2x"})])])])}],d={name:"Header",methods:{goHome(){this.$router.push("/")}}},h=d,u=(a("3092"),a("2877")),m=Object(u["a"])(h,c,l,!1,null,"ea434f30",null),p=m.exports,f=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row pt-2 justify-content-center"},[a("div",{staticClass:"col-md-12",attrs:{id:"nav-bar"}},[a("router-link",{staticClass:"router-link",attrs:{to:"/about"}},[e._v("About Me")]),e._v(" | "),a("router-link",{staticClass:"router-link",attrs:{to:"/projects"}},[e._v("Projects")]),e._v(" | "),a("router-link",{staticClass:"router-link",attrs:{to:"/artwork"}},[e._v("Artwork")]),e._v(" | "),a("router-link",{staticClass:"router-link",attrs:{to:"/resume"}},[e._v("Resume")])],1)])},g=[],v={name:"NavBar"},y=v,w=(a("222a"),Object(u["a"])(y,f,g,!1,null,"bbe68fba",null)),b=w.exports,_={name:"App",components:{Header:p,NavBar:b}},C=_,k=(a("034f"),Object(u["a"])(C,r,n,!1,null,null,null)),x=k.exports,T=a("8c4f"),F=function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"row pt-4 align-items-md-start justify-content-around"},[i("div",{staticClass:"col-6 pt-4 col-md-3 pt-md-5",attrs:{id:"about"}},[i("div",{staticClass:"img-container"},[i("h4",[e._v("About Me")]),i("div",{staticClass:"img-holder",on:{click:e.goToAbout}},[i("img",{staticClass:"rounded img-fluid upper-img",attrs:{src:a("c9e4")}})])])]),i("div",{staticClass:"col-6 pt-4 col-md-3 pt-md-5",attrs:{id:"resume"}},[i("div",{staticClass:"img-container"},[i("h4",[e._v("Resume")]),i("div",{staticClass:"img-holder",on:{click:e.goToResume}},[i("img",{staticClass:"rounded img-fluid upper-img",attrs:{src:a("a297")}})])])]),i("div",{staticClass:"col-md-4",attrs:{id:"artwork-and-projs"}},[i("div",{staticClass:"img-container"},[i("h4",[e._v("Projects")]),i("div",{staticClass:"img-holder",on:{click:e.goToProjects}},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("8b08")}})])]),i("div",{staticClass:"img-container mt-4"},[i("h4",[e._v("Artwork")]),i("div",{staticClass:"img-holder",on:{click:e.goToArtwork}},[i("img",{staticClass:"rounded img-fluid lower-img",attrs:{src:a("4906")}})])])]),i("div",{staticClass:"col-5 pt-5 pt-md-0 col-md-2",attrs:{id:"artwork"}},[i("div",{staticClass:"img-container"},[i("h4",[e._v("Artwork")]),i("div",{staticClass:"img-holder",on:{click:e.goToArtwork}},[i("img",{staticClass:"rounded img-fluid lower-img",attrs:{src:a("4906")}})])])]),i("div",{staticClass:"col-6 pt-5 pt-md-0 col-md-4",attrs:{id:"projects"}},[i("div",{staticClass:"img-container"},[i("h4",[e._v("Projects")]),i("div",{staticClass:"img-holder",on:{click:e.goToProjects}},[i("img",{staticClass:"rounded img-fluid lower-img",attrs:{src:a("8b08")}})])])])])},j=[],I={name:"Home",data(){return{windowWidth:window.innerWidth}},methods:{goToAbout(){this.$router.push("/about")},goToProjects(){this.$router.push("/projects")},goToArtwork(){this.$router.push("/artwork")},goToResume(){this.$router.push("/resume")}}},M=I,P=(a("2db8"),Object(u["a"])(M,F,j,!1,null,"6eb776f2",null)),S=P.exports,N=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},A=[function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"row pt-3 pt-lg-5 justify-content-center align-items-center",attrs:{id:"home-container"}},[i("div",{staticClass:"col-6 col-md-5 col-lg-5 col-xl-3 mr-xl-5"},[i("img",{staticClass:"rounded img-fluid",attrs:{id:"leo-and-dylan-pic",src:a("ee29"),alt:"Leo and Dylan"}})]),i("div",{staticClass:"col-11 col-md-6 col-lg-5 col-xl-5",attrs:{id:"about-description"}},[i("p",{attrs:{id:"intro-paragraph"}},[e._v(" Hey there! I'm a Masters grad from MIT where I studied Computer Science, specifically Artificial Intelligence. My research focused on machine learning tools for assisting crisis managers during climate crises using crowdsourced climate crisis data. You can learn more about my research"),i("a",{attrs:{href:"#/projects/ml-for-crowdsourced-crisis-data"}},[e._v("here!")]),e._v("I'm also a proud dad to my son, Leo ðŸ• ")]),i("p",[e._v(" I'm interested in Software Engineering, Data Science & Machine Learning, and Web Development! You can contact me through"),i("a",{attrs:{href:"mailto: dylanrl97@gmail.com"}},[e._v("email,")]),e._v("add me on"),i("a",{attrs:{href:"https://www.linkedin.com/in/dyllew/",target:"_blank"}},[e._v("LinkedIn,")]),e._v("or checkout my"),i("a",{attrs:{href:"https://github.com/dyllew/",target:"_blank"}},[e._v("GitHub!")]),e._v(" I'm currently looking for remote Data Science and Software Engineering positions! ")]),i("p",[e._v(" In my freetime I love to explore nature, play video games (Zelda/DMC/Soulsborne/Skyrim/Kingdom Hearts/Fallout), cook/bake something new, make and experiment with different forms of art, develop new skills, and go on adventures with my pup! ")])])])}],D={name:"About"},E=D,L=(a("c7dc"),Object(u["a"])(E,N,A,!1,null,"191887ad",null)),W=L.exports,R=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row align-items-center justify-content-center"},e._l(e.projects,(function(e){return a("div",{key:e.link,staticClass:"col-12 col-md-4 col-lg-4 pt-3"},[a("ProjectCard",{attrs:{project:e}})],1)})),0)},U=[],q=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"card border-info bg-transparent align-items-center"},[a("h5",{staticClass:"card-header"},[e._v(e._s(e.project.title))]),a("img",{staticClass:"card-img-top",class:e.project.src.sizeClass?e.project.src.sizeClass:"",attrs:{id:e.project.id,src:e.getImgURL(e.project.src.imgFilename),alt:"Project Image"}}),a("div",{staticClass:"card-body"},[a("p",{staticClass:"card-text"},[e._v(e._s(e.project.desc))]),e.project.projectWebsite?a("div",{attrs:{id:e.project.projectWebsite.id}},[a("a",{staticClass:"btn btn-light col-5",attrs:{href:e.project.projectWebsite.url,target:"_blank"}},[e._v(e._s(e.project.projectWebsite.btnText))]),a("span"),a("a",{staticClass:"btn btn-primary text-light col-5",on:{click:function(t){return e.goToProjectPage(e.project.link)}}},[e._v(e._s(e.projectBtnText))])]):a("div",{attrs:{id:"button-holder"}},[a("a",{staticClass:"btn btn-primary text-light",on:{click:function(t){return e.goToProjectPage(e.project.link)}}},[e._v("See Project Details")])])])])},O=[];const z=[{id:"ml-for-crowdsourced-crisis-data",link:"/projects/ml-for-crowdsourced-crisis-data",src:{imgFilename:"masters-thesis-overview.png"},title:"Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response",desc:"Masters thesis/Research project on constructing labeled crowdsourced crisis datasets and developing machine learning models to assist crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response.",projectWebsite:{id:"button-holder",btnText:"See Thesis Document",url:"https://dspace.mit.edu/handle/1721.1/144911"}},{id:"nlp-for-int-dev-gray-lit",link:"/projects/nlp-for-int-dev-gray-lit",src:{imgFilename:"int-dev-results.png"},title:"Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature",desc:"NLP project which investigates Named Entity Recognition (NER) and K-means Clustering on a corpus of 244 documents of International Development literature papers for streamlining evidence synthesis process on international development gray literature.",projectWebsite:{id:"button-holder",btnText:"Presentation PDF",url:"./assets/int-dev-gray-lit.pdf"}},{id:"climate-change-news",link:"/projects/climate-change-news",src:{imgFilename:"final-project-overview.png"},title:"Evolution of the U.S. TV News Narrative on Climate Change",desc:"Data Science & NLP project in Python that investigated the evolution of climate change coverage frequency & content between popular U.S. TV News Networks CNN, Fox News, and MSNBC over July 2009-January 2020.",projectWebsite:{id:"button-holder",btnText:"Poster PDF",url:"./assets/IDS131_Poster.pdf"}},{id:"taxi-pred-img",link:"/projects/gnns-taxi-prediction",src:{imgFilename:"fare-surge-graph-pred.png"},title:"Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction",desc:"Machine Learning project in Python which investigated graph neural networks (GNNs) for the tasks of NYC taxi fare and demand surge prediction."},{id:"trump-img",link:"/projects/trump-speech-analysis",src:{imgFilename:"FrequencyPlot.png",sizeClass:"w-75"},title:"Trump Campaign Speech Analysis",desc:"Data Science project in R which investigated how Donald Trump's 2016 campaign speeches may have influenced public sentiment on a regional level.",projectWebsite:{id:"button-holder",btnText:"Poster PDF",url:"./assets/17_835_Poster.pdf"}},{id:"boomerang-img",link:"/projects/boomerang",src:{imgFilename:"boomerang-home.jpg"},title:"Boomerang",desc:"Boomerang is a full-stack web application where users can efficiently and reliably borrow items from others within their communities.",projectWebsite:{id:"button-holder",btnText:"Go to the Boomerang website",url:"https://team-aesthetech-boomerang.herokuapp.com/"}}],$=[{id:"image-module-img",link:"/projects/ml-for-crowdsourced-crisis-data/image-analysis-module",src:{imgFilename:"image-analysis-module-modified.png"},title:"Image Analysis Module",desc:"ML Module focused on Constructing human-annotated datasets and assessing the quality of the annotations, developing CNN models to classify the crowdsourced crisis image data for various classification tasks, and conducting image annotation workshops with crisis managers from various contexts.",projectBtnText:"See Module Details",projectWebsite:{id:"button-holder",btnText:"See Related Code",url:"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Image%20Analysis%20Module"}},{id:"text-module-img",link:"/projects/ml-for-crowdsourced-crisis-data/text-analysis-module",src:{imgFilename:"text-analysis-module.png"},title:"Text Analysis Module",desc:"ML Module focused on Classification of crowdsourced crisis text (in JA) for Human Risk informed from the identified information needs and priorities of crisis managers and Clustering of crisis text data for uncovering semantic categories in the data.",projectBtnText:"See Module Details",projectWebsite:{id:"button-holder",btnText:"See Related Code",url:"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module"}}];function H(){window.scrollTo(0,0)}var B={name:"ProjectCard",props:["project"],methods:{goToProjectPage(e){this.$router.push(e),H()},getImgURL(e){return a("1913")("./"+e)}},computed:{projectBtnText(){return this.project.projectBtnText?this.project.projectBtnText:"See Project Details"}}},J=B,G=(a("d472"),Object(u["a"])(J,q,O,!1,null,"7bfc7dfc",null)),K=G.exports,V={name:"Projects",components:{ProjectCard:K},data(){return{projects:z}}},Y=V,Q=Object(u["a"])(Y,R,U,!1,null,null,null),X=Q.exports,Z=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},ee=[function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"artwork row align-items-center justify-content-center justify-content-xl-around"},[i("div",{staticClass:"col-8 pt-3 pt-md-0 col-md-4"},[i("h4",{staticClass:"header"},[e._v("Ethereality")]),i("img",{staticClass:"rounded img-fluid",attrs:{src:a("4906")}})]),i("div",{staticClass:"col-8 pt-4 pt-md-2 col-md-4"},[i("h4",{staticClass:"header"},[e._v("Phantom Dragon")]),i("img",{staticClass:"rounded img-fluid",attrs:{src:a("a2c3")}})])])}],te={name:"Artwork"},ae=te,ie=(a("f3bd"),Object(u["a"])(ae,Z,ee,!1,null,null,null)),se=ie.exports,oe=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},re=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"resume row align-items-center justify-content-center"},[a("div",{staticClass:"col-12 mt-4",attrs:{id:"sticky-btn"}},[a("a",{staticClass:"btn btn-primary text-light",attrs:{target:"_blank",href:"./assets/Dylan_Lewis_Resume.pdf"}},[e._v("View PDF in Tab")])]),a("div",{staticClass:"col-12 mt-2"},[a("embed",{staticClass:"pdf",attrs:{src:"./assets/Dylan_Lewis_Resume.pdf"}})])])}],ne={name:"Resume"},ce=ne,le=(a("54e2"),Object(u["a"])(ce,oe,re,!1,null,"28d22fa6",null)),de=le.exports,he=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row align-items-center justify-content-center"},[e._m(0),a("div",{staticClass:"col-md-10"},[a("div",{staticClass:"carousel slide",attrs:{id:"ccCarousel","data-ride":"carousel","data-interval":"false"}},[a("ol",{staticClass:"carousel-indicators"},[a("li",{staticClass:"active",attrs:{"data-target":"#ccCarousel","data-slide-to":"0"},on:{click:e.scrollUp}},[e._m(1)]),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"1"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"2"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"3"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"4"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"5"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"6"},on:{click:e.scrollUp}})]),a("div",{staticClass:"carousel-inner"},[e._m(2),e._m(3),e._m(4),a("div",{staticClass:"carousel-item cc-carousel-item"},[a("div",{staticClass:"carousel-text"},[a("h5",[e._v("Machine Learning Methodology")]),e._v(" With crisis management partners in Fukuchiyama (FC), Japan, we present our framework through two ML modules:"),a("br"),e._v(" Image Analysis Module & Text Analysis Module"),a("br"),a("br"),a("p",[e._v(" We note that our findings from the Image Analysis Module influenced the design of the Text Analysis Module in order to meet our aim of developing machine learning systems for crisis management which iteratively incorporate the feedback received from crisis managers. ")]),a("div",{staticClass:"row align-items-center justify-content-center pb-5"},e._l(e.modules,(function(e){return a("div",{key:e.link,staticClass:"col-12 col-md-6 col-lg-6 pt-3"},[a("ProjectCard",{attrs:{project:e}})],1)})),0),a("p",[e._v(" In the next few slides, we summarize our main contributions to the field of Crisis Informatics. ")])])]),e._m(5),e._m(6),e._m(7)])])])])},ue=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-12 col-md-8"},[a("h3",[e._v("Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response")])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"tooltip"},[a("span",{staticClass:"tooltiptext"},[e._v("Thesis Document & Code")])])},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"carousel-item active cc-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{id:"overview-pic",src:a("ab31"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Thesis Document & Code and Related Open-source Python Packages")]),i("p",[e._v(" Master of Engineering in Electrical Engineering and Computer Science thesis I wrote as a research assistant at the Urban Risk Lab (URL) at MIT which investigates the development of machine learning models to assist crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response. This presentation introduces the motivation behind this work, explains how the investigation unfolded, and reports the main findings and implications from the research. "),i("br"),i("br"),e._v(" The published thesis document can be found "),i("a",{attrs:{href:"https://dspace.mit.edu/handle/1721.1/144911",target:"_blank"}},[e._v("here.")]),i("br"),i("br"),e._v(" The code for this thesis was written in Python using Jupyter Notebooks. "),i("a",{attrs:{href:"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting",target:"_blank"}},[e._v("Here's the GitHub Repo.")]),i("br"),e._v(" Relatedly, with this thesis, we produced two open-source Python packages to better enable readability, reusability, and reproducibility of the computational utilities derived for performing various experiments and analysis on crowdsourced crisis image and text data: "),i("ul",[i("li",[i("a",{attrs:{href:"https://pypi.org/project/url-image-module/0.27.0/",target:"_blank"}},[e._v("URL Image Module")]),e._v(" - Utilities for training, testing, and predicting with pretrained Convolutional Neural Networks for classifying crowdsourced crisis image data and constructing & analyzing annotated datasets")]),i("li",[i("a",{attrs:{href:"https://pypi.org/project/url-text-module/0.6.1/",target:"_blank"}},[e._v("URL Text Module")]),e._v(" - Utilities for featurizing crisis text data, training and testing with a variety of classification machine learning models, and visualizing clusters of featurized text data")])]),i("br"),e._v(" This research was supported by a grant from Google.org and the Tides Foundation. ")])])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"carousel-item cc-carousel-item"},[a("div",{staticClass:"carousel-text"},[a("h5",[a("strong",[e._v("Abstract")])]),a("p",[e._v(" The availability of information during a climate crisis event is critical for crisis managers to assess and respond to crisis impact. During crisis events, affected residents post real-time crisis updates on platforms such as "),a("a",{attrs:{href:"https://riskmap.mit.edu/japan.html",target:"_blank"}},[e._v("RiskMap")]),e._v(" and "),a("a",{attrs:{href:"https://twitter.com/?lang=en",target:"_blank"}},[e._v("Twitter")]),e._v(". These updates provide localized information, which has the potential to enhance crisis awareness and response. However, with limited resources, crisis managers may endure information overload from the inundation of these updates. Prior work has demonstrated the potential of machine learning (ML) methodologies to mitigate this problem. We have identified limitations in the prior work including the lack of involvement of crisis managers in the development and evaluation of a ML methodology. ")]),a("p",[e._v(" To address these limitations, we propose a novel framework and ML methodology which investigate the efficacy of various ML methods in enhancing crisis awareness and response beyond model performance metrics. This framework aims to iteratively embed the information needs and priorities of crisis managers during crisis into the design of the ML methodology. We cooperated with crisis managers in Fukuchiyama City (FC), a city in Japan which is susceptible to flood events, and analyzed crowdsourced crisis image and text data from past FC flood events. We devised the Flood Presence image classification task, constructed Train/Dev/Test splits, and annotated images from FC. We report a weighted F1 score of 92.1% on the test split and 82.5% on the FC images. Using the results of our image analysis ML methodology and the insights we gained from crisis managers, we iterated on the design of our text analysis ML methodology. This led to the creation of the Human Risk text classification task which is tailored to a subset of the identified information needs of the crisis managers. To align with the priorities of crisis managers for this task, we determined the model evaluation metric to be the F2 score. We report an F2 score of 92.8% on an FC crisis text test dataset, which is a significant improvement over the baseline score of 43.4%. ")]),a("h5",[a("strong",[e._v("Research Question")])]),a("strong",{attrs:{id:"research-question"}},[e._v(" In collaborating with crisis managers, how can machine learning methods be utilized and evaluated to effectively reduce the information overload of crowdsourced data on crisis managers during flood events for enhanced crisis awareness and response? ")])])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"carousel-item cc-carousel-item"},[a("div",{staticClass:"carousel-text"},[a("h5",[e._v("Main Research Aims")]),a("p",{attrs:{id:"research-aims"}},[a("ol",[a("li",[a("strong",[e._v("To reduce information overload during a crisis")]),e._v(" using accurate, efficient, automated image and text classification of crisis reports by machine learning (ML) models during crisis.")]),a("li",[a("strong",[e._v("To embed tacit knowledge, information needs, and decision-making priorities of crisis managers")]),e._v(" into the designed ML methodology.")]),a("li",[a("strong",[e._v("To evaluate methodology in collaboration with crisis managers")]),e._v(", e.g. crisis managers in Fukuchiyama, Japan.")]),a("li",[a("strong",[e._v("To incorporate evaluation results and iterate on the ML methodology")]),e._v(" in order to better reach aim of using AI to assist crisis managers during crisis.")])]),e._v(" We investigate various ML techniques that can be applied to mitigate information overload for crisis managers during crisis events while also assessing if those techniques can satisfy the information needs and priorities of crisis managers through qualitative and quantitative evaluation. ")]),a("h5",[e._v("Novel Framework")]),a("p",[e._v(" To achieve the aims above we develop a novel framework in the crisis informatics community consisting of the following components: "),a("ul",[a("li",[a("strong",[e._v("Classification Task Creation")]),e._v(" - We develop new classification tasks using labels provided to us by crisis managers and labels present in open-source datasets")]),a("li",[a("strong",[e._v("Data Annotation Procedure")]),e._v(" - We open-souce the annotation guide we develop for greater transparency into the procurement of human-annotated that is used to train and evaluate ML models")]),a("li",[a("strong",[e._v("Interannotator Agreement/Data Reliability Analysis")]),e._v(" - After performing an annotation effort on data provided by crisis managers, we analyze the quality of the annotations through interannotator agreement analysis to demonstrate the importance of understanding data quality prior to using it for ML purposes")]),a("li",[a("strong",[e._v("Model Development and Evaluation; Per-Class Performance Analysis")]),e._v(" - The metrics we used to evaluate models are derived either from metrics reported in the literature or, more notably, metrics determined from insights provided by crisis managers directly. Additionally, we consider issues of class imbalance and report per-class performance and confusion matrices to provide more granular insight into model performance. Finally, we establish baselines to compare against the models we develop to assess the degree to which the models we develop outperform the baseline")]),a("li",[a("strong",[e._v("Qualitative Analysis through Workshops with Crisis Managers")]),e._v(" - We sought to cooperate with crisis managers with the intent of using this framework to iteratively design ML systems, such as the specific classification tasks performed by the models and their associated performance metric, by iteratively incorporating feedback and insights form crisis managers, so that the system better aligns with their information needs and decision-making priorities")])]),e._v(" This framework situates "),a("i",[e._v("Model Development and Evaluation")]),e._v(", which is commonplace in prior work, as part of a larger, contextualized analysis. On that note, we expand on the notion of "),a("i",[e._v("Model Development and Evaluation")]),e._v(" beyond what is typically seen in the prior work. ")])])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"carousel-item cc-carousel-item"},[a("div",{staticClass:"carousel-text"},[a("h4",[e._v("Main Contributions 1/3")]),a("h5",[e._v("Novel Framework for Information Overload Mitigation of Crowdsourced Crisis Data")]),a("p",[e._v(" Our main contribution is a novel framework that unifies a variety of machine learning techniques, analysis, and evaluation on images and text present in crowdsourced crisis data, which aims to reduce the information overload of crisis managers, specifically during flood crisis events. This approach leverages classical machine learning models and deep learning models aimed to provide accurate and efficient classifications on individual flood reports to mitigate information overload of crisis managers. This framework addresses the limitations in the prior work such as the lack of engagement of crisis managers in the development of the ML methodology and an evaluation which expands beyond model metrics. In this framework, we embed various utilities for conducting analysis such as computing properties and statistics of annotated datasets and model performance. ")]),a("h5",[e._v("Open-source Python Packages/Code")]),a("p",[e._v(" To ensure the reuse of the analysis conducted in this work, we release two open-source Python packages: One for the "),a("a",{attrs:{href:"https://pypi.org/project/url-image-module/0.27.0/",target:"_blank"}},[e._v("Image Analysis Module")]),e._v(" and the other for the "),a("a",{attrs:{href:"https://pypi.org/project/url-text-module/0.6.1/",target:"_blank"}},[e._v("Text Analysis Module")]),e._v(". These packages include utilities for training, testing, and prediction using the models presented in this work, computing statistics for interannotator agreement, and computing metrics for model performance. For text data, there are also utilities for featurization of text, performing nested cross validation, and clustering. We note that there are utilities for plotting such as methods for producing the plots shown throughout this project. ")]),a("p",[e._v(" In addition to these packages, we release an "),a("a",{attrs:{href:"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting",target:"_blank"}},[e._v("open-source repository")]),e._v(" containing Jupyter notebooks, relevant documents (e.g. the annotation guide mentioned above), and other code required to reproduce the experiments and reuse the analysis conducted in this work. ")]),a("h5",[e._v("Flood Presence Task Creation, Labeled Image Dataset, and Performance Benchmark")]),a("p",[e._v(" Since we focused on flood crises, we defined the image prediction task of Flood Presence classification. The Flood Presence task is the binary classification task of determining whether or not there is presence of flood in an image. We construct a labeled dataset for the task consisting of âˆ¼23.6ð‘˜ images by combining various opensource datasets which were labeled with labels useful for this task, although they were originally developed for other adjacent tasks. We contribute this dataset for further research in crisis informatics. In addition, we provide Train/Dev/Test splits and an associated benchmark performance on the test split using a state-of-the-art Convolutional Neural Network (CNN) model, EfficientNet-B1. ")]),a("h5",[e._v("Data Annotation Procedure and Analysis of Interannotator Agreement")]),a("p",[e._v(" We developed a procedure for annotating images in order to label the unlabeled image data provided to us by our partners in Fukuchiyama, Japan. This procedure included creating an annotation guide to assist annotators in their labeling. This guide included the name of each task, the names of the mutually-exclusive classes associated with each task, and an associated description and example image for each class. We then had annotators from the Urban Risk Lab independently annotate the images using this guide. ")]),a("p",[e._v(" After the annotation effort was completed, we were able analyze the interannotator agreement between the annotators and construct ground-truth labels for these images. We computed interannotator reliability statistics to get a sense of how reproducible the annotation procedure was for each task as well as to have transparency of the data quality prior to using it for ML purposes. Finally, we created ground-truth datasets using these labels for the Fukuchiyama images to use in evaluating the image classification ML models we developed. ")])])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"carousel-item cc-carousel-item"},[a("div",{staticClass:"carousel-text"},[a("h4",[e._v("Main Contributions 2/3")]),a("h5",[e._v("Classification and Clustering of Crowdsourced Japanese Crisis Text")]),a("p",[e._v(" Research in crisis informatics on crowdsourced crisis data focuses mostly on English, and thus research on crowdsourced Japanese crisis text is sparse. The Text Analysis Module developed in this work focused exclusively on Japanese text data. We explored various numerical representations of the Japanese crisis text reports provided by our partners and developed a pipeline for preprocessing the raw text and producing the numerical representations. Additionally, our partners provided labels along with the text reports, so we experimented with classifying the text. Lastly, we explored the text data using unsupervised learning, specifically, we employed clustering methods to help inform development of classification tasks in future work ")]),a("h6",[e._v("Pipeline for Japanese Crisis Text Preprocessing, Tokenization, and Featurization")]),a("p",[e._v(" In order to use the text reports as input to the various ML models employed in this work, we represent the raw text string of each report as a numerical vector, or featurization. Depending on the featurization we choose to use for a text report, we may first preprocess the text. This preprocessing included various steps including tokenization, stopword removal, and lemmatization, which we performed using open-source software (i.e. tokenizer and lemmatizer) and publicly available data (i.e. stopwords list) for the Japanese language. We provide a pipeline for preprocessing and performing the following featurizations on Japanese text: "),a("ul",[a("li",[e._v("n-gram Bag-of-Words (BOW)")]),a("li",[e._v("n-gram Term Frequency-Inverse Document Frequency (TF-IDF)")]),a("li",[e._v("Pretrained Japanese Masked Language Modeling (MLM) BERT Model with Classification (CLS) Pooling Embedding")])]),e._v(" The resultant feature vectors representing the text enabled us to use them as input to ML models. Thus, we can then employ classification and clustering techniques on the text data. ")]),a("h6",[e._v("Human Risk Task Creation and Performance Metric Determination")]),a("p",[e._v(" We devised a new text classification task, Human Risk classification. The human risk text classification task determines whether or not a crisis text report indicates if there are people in need of rescue from a crisis. This includes people being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), and/or being in need of life-saving emergency medical care. This classification task was unique among the classification tasks presented in this work because it was devised using labels that came with the text reports given to us by our crisis management partners. Relatedly, we determine the metric to use in model performance evaluation using the insights we gained from our partners. ")]),a("h6",[e._v("Exploratory Analysis of Japanese Crisis Text using Unsupervised Learning")]),a("p",[e._v(" With the intention of finding cohesive groupings within the Japanese crisis text corpus, which can inform the development of future text classification tasks, we devise a pipeline for featurizing Japanese crisis text, reducing the high-dimensional text feature vector to 2 dimensions, and clustering the data. We evaluate this pipeline both quantitatively and qualitatively, experimenting with various text featurizations including unigram TF-IDF features and pretrained Japanese MLM BERT with CLS Pooling embeddings mentioned above, t-Distributed Stochastic Neighbor Embedding (t-SNE) and Principle Component Analysis (PCA) for dimensionality reduction, and finally K-means and K-medoids for the algorithm which clusters the data. ")]),a("p",[e._v(" After we determine the optimal combination of text embedding, dimensionality reduction technique, and clustering algorithm, we create brief summaries of each cluster using the unigrams with highest TF-IDF score for each cluster and the closest reports (by euclidean distance) within each cluster to the cluster center to help in the determination of a label for each cluster. Lastly, a member of the Urban Risk Lab at MIT who is fluent in Japanese used these summaries to determine an interpretable label to accompany each cluster found. We thus provide various labels which can be used for classification experiments and analysis in a future work. ")])])])},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"carousel-item cc-carousel-item"},[a("div",{staticClass:"carousel-text"},[a("h4",[e._v("Main Contributions 3/3")]),a("h6",[e._v("Quantitative and Qualitative Evaluation in Japanese Flood Crisis Context")]),a("p",[e._v(" Prior work has typically evaluated ML methods using quantitative measures, mainly classification performance metrics, e.g. accuracy, precision, recall, F1, and AUROC (Area Under the Receiver Operating Characteristic Curve) and their macro and micro variants. However, with the framework we present in this thesis, we aimed to expand the evaluation of the efficacy ML models have in reducing information overload to not only include similar quantitative measures mentioned above, but also qualitative evaluation derived from engaging with our crisis management partners. Beyond having good performance, we hoped to use the qualitative evaluation used in this study to gain a broader understanding of the efficacy a model can provide crisis managers in mitigating information overload and gaining situational awareness. ")]),a("p",[e._v(" We held image annotation workshops with various crisis managers and aimed to understand what type of information they seek to gain from a crowdsourced image during a flood crisis event. With their insights, we began to understand how our models can be refined or improved, or how new models can be created in order to better serve the information needs of crisis managers more effectively, such as by tailoring the labels and their associated meanings to the information needs of crisis managers suggested from their annotations. Additionally, we gained more insight into the appropriate metrics to use when evaluating models based on the priorities of crisis managers as it relates to the task. From these workshops, we share key lessons that can influence the design of this framework and AI-augmented crisis information systems of the future. In fact, within this work, we used the lessons learned from the image analysis workshops to assist us in determining the performance metric to use when developing models for the human risk text classification task. This exercise exhibited the principle of iterative development our framework intends to promote. ")])])])}],me={name:"MLForCrowdsourcedCrisisData",components:{ProjectCard:K},data(){return{modules:$}},methods:{scrollUp(){H()}}},pe=me,fe=(a("ffa5"),Object(u["a"])(pe,he,ue,!1,null,"6c63a12f",null)),ge=fe.exports,ve=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row align-items-center justify-content-center"},[e._m(0),a("div",{staticClass:"col-12 col-md-10"},[a("div",{staticClass:"carousel slide",attrs:{id:"ccCarousel","data-ride":"carousel","data-interval":"false"}},[a("ol",{staticClass:"carousel-indicators"},[a("li",{staticClass:"active",attrs:{"data-target":"#ccCarousel","data-slide-to":"0"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"1"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"2"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"3"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"4"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"5"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"6"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"7"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"8"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"9"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"10"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"11"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"12"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"13"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"14"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"15"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"16"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"17"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"18"},on:{click:e.scrollUp}})]),e._m(1)])])])},ye=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-8"},[a("h3",[e._v("Image Analysis Module")])])},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item cc-carousel-item active"},[i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12 col-md-6 pt-3 pb-5"},[i("img",{staticClass:"img-fluid",attrs:{id:"image-analysis-module",src:a("d730"),alt:"Second slide"}})]),i("div",{staticClass:"carousel-text col-12 col-md-6"},[i("h5",[e._v("Image Analysis Methodology Overview")]),i("p",[e._v(" The goal of the Image Analysis Module is to utilize pretrained Convolutional Neural Network models to yield efficient and accurate predictions from image data in crowdsourced crisis reports such as those on RiskMap & Twitter. The classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP) form a diverse suite of labels for assisting crisis managers in automatically classifying the data. In a fraction of a second, the model predictions made for these tasks provide a series of categorizations for an individual report. We leverage state-of-the-art CNNs, which strike a necessary balance between model complexity, memory and storage constraints, and model performance to provide these predictions. "),i("br"),i("br"),e._v(" To achieve this aim, we use large, labeled, open-source datasets for training and evaluating the models. We formulate a new crisis image classification task for detecting flood presence in an image and construct a new dataset altogether using flood images from various open-source datasets, which contained flood-adjacent labels. We were given unlabeled flood crisis images from crisis managers in Fukuchiyama (FC), Japan. We devise an annotation procedure to label this data and analyze the results of human-annotations on the images. Since this data was not used to train or develop the models, we aimed to see if our trained models could accurately classify unseen images from the FC context. We held various image annotation workshops with crisis managers to identify the limitations in our approach and understand how we could iterate on the design of our ML methodology. This evaluation procedure thus enabled us to evaluate the use of image classification models in assisting crisis managers using quantitative metrics as well as qualitatively through the feedback we got through the image annotation workshops. This evaluation directly influenced our approach for devising the Text Analysis Module. ")])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Image Datasets & Train/Dev/Test Splits for Model Development & Evaluation")]),i("div",{staticClass:"row justify-content-around"},[i("div",{staticClass:"carousel-text col-12 col-md-9"},[i("p",[e._v(" Since we used CNN models, specifically the EfficientNet-B1 architecture pretrained on ImageNet, we wanted to make use of large open-source Twitter crisis image datasets for finetuning the models to perform the classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP). ")]),i("h6",[e._v("Open-source Consolidated Crisis Image Datasets")]),i("p",[e._v(" For the DS, HC, and IN tasks, we made use of the open-source train/dev/test splits made available at "),i("a",{attrs:{href:"https://crisisnlp.qcri.org/crisis-image-datasets-asonam20",target:"_blank"}},[e._v("CrisisNLP")]),e._v(". We train the models which perform these tasks using the train and dev splits. For evaluation, we make use of the respective test splits for each task. ")]),i("h6",[e._v("Flood Presence Task Creation and Dataset Formation")]),i("p",[e._v(' In this work we focus on flood-crisis events, thus we used various open-source image datasets which have flood-adjacent labels and map them to the binary labels of "Flood"/"Not Flood". Using the resulting dataset, we create randomized, non-overlapping Train/Dev/Test splits. Similar to the above, we use the train & dev splits to develop the FP model, and the test split to evaluate the model. ')])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12 col-md-8"},[i("h5",[e._v("Flood Presence Dataset Composition")]),e._v(" Composition of the Flood Presence dataset from the original datasets and the number of images for each label. "),i("table",{attrs:{id:"fp-table"}},[i("tr",[i("th",[i("strong",[e._v("Dataset")])]),i("th",[i("strong",[e._v("Flood")])]),i("th",[i("strong",[e._v("Not Flood")])]),i("th",[i("strong",[e._v("Total")])])]),i("tr",[i("td",[i("i",[i("strong",[e._v("Consolidated Disaster Types")]),i("br"),e._v(" ("),i("a",{attrs:{href:"https://arxiv.org/abs/2011.08916",target:"_blank"}},[e._v("Alam et al. 2020")]),e._v(") ")])]),i("td",[e._v("3201")]),i("td",[e._v("14310")]),i("td",[e._v("17511")])]),i("tr",[i("td",[i("i",[i("strong",[e._v("Central European Floods 2013")]),i("br"),e._v(" ("),i("a",{attrs:{href:"https://arxiv.org/abs/1908.03361",target:"_blank"}},[e._v("Barz et al. 2018")]),e._v(") ")])]),i("td",[e._v("3151")]),i("td",[e._v("559")]),i("td",[e._v("3710")])]),i("tr",[i("td",[i("i",[i("strong",[e._v("Harz Region Floods 2017")]),i("br"),e._v(" ("),i("a",{attrs:{href:"https://arxiv.org/abs/2011.05756",target:"_blank"}},[e._v("Barz et al. 2020")]),e._v(") ")])]),i("td",[e._v("264")]),i("td",[e._v("405")]),i("td",[e._v("669")])]),i("tr",[i("td",[i("i",[i("strong",[e._v("Rhine River Floods 2018")]),i("br"),e._v(" ("),i("a",{attrs:{href:"https://arxiv.org/abs/2011.05756",target:"_blank"}},[e._v("Barz et al. 2020")]),e._v(") ")])]),i("td",[e._v("730")]),i("td",[e._v("1007")]),i("td",[e._v("1737")])]),i("tr",[i("td",[e._v("Total")]),i("td",[e._v("7346")]),i("td",[e._v("16281")]),i("td",[e._v("23627")])])])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Image Training Sets")]),i("div",{staticClass:"row align-items-center justify-content-between"},[i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("964e")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("be19")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("bb54")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("166a")}})])]),i("br"),i("p",[e._v(' As part of the framework we developed, we investigated the class imbalance for each of the image classification training sets. We observed significant imbalance in the label distributions for the Damage Severity and the Humanitarian Categories tasks. We note that this imbalance could be problematic for the performance of the models on the minority classes for those tasks, e.g. the "Mild", "Rescue, Volunteering, or Donation Effort", and "Affected, Injured, or Dead People" classes. ')])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12"},[i("h5",[e._v("Model Evaluation on Test Splits & Flood Presence Benchmark Performance")])]),i("div",{staticClass:"col-12 col-md-8"},[i("p",[e._v(" Performance of image classification models on their respective test splits. [1] as referred to in the table can be found in the footnote below."),i("sup",[i("a",{attrs:{href:"#fn1",id:"ref1"}},[e._v("1")])])]),i("br"),i("img",{staticClass:"img-fluid",attrs:{id:"test-set-eval",src:a("621b")}})])]),i("br"),i("p",[e._v(" Similar to the authors in [1]"),i("sup",[i("a",{attrs:{href:"#fn1",id:"ref1"}},[e._v("1")])]),e._v(", we report overall model performance as weighted metrics in order to take into account class imbalance present in the test splits. From the table above, we observe that the models we finetuned achieve a weighted F1 score within a 1% difference compared to the model performances reported in [1]. "),i("strong",[e._v("We report a benchmark performance of 92.1% weighted F1 for the Flood Presence (FP) task.")]),e._v(" We postulate the comparatively higher performance of FP task to be due to the task being binary as well as being the most clear and objective task, thus being a comparatively simipler task for the model to learn. We explore this more through interannotator agreement analysis discussed in the next slides. ")]),i("hr"),i("p",[i("sup",{attrs:{id:"fn1"}},[e._v("1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, "),i("a",{attrs:{href:"https://arxiv.org/pdf/2011.08916.pdf",target:"_blank"}},[e._v("Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response")]),e._v(", In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020."),i("a",{attrs:{href:"#ref1",title:"Jump back to footnote 1 in the text."}},[e._v("â†©")])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Annotating Fukuchiyama Crisis Images")]),i("p",[e._v(" To evaluate our developed models and framework in a context which is susceptible to flood events, we cooperated with crisis managers in Fukuchiyama, Japan to attain "),i("strong",[e._v("658 images")]),e._v(" from previous flood events as well as non-crisis normal days in Fukuchiyama, which were collected on the ground, similar to RiskMap & Twitter crisis images. This evaluation enabled us to understand how well training the models on the large, consolidated crisis datasets, which cover a diverse set geographical locations and a multitude of crisis events, would perform on the unseen data from flood events in Fukuchiyama. To form evaluation/test sets from this data, we needed to label the images for each of the four image classification tasks we've discussed. ")]),i("p",[i("ul",[i("li",[e._v("The images were labeled independently by Urban Risk Lab researchers for each task. The researchers used "),i("a",{attrs:{href:"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/blob/main/Image%20Analysis%20Module/Annotation/Image%20Labeling%20Guide.docx",target:"_blank"}},[e._v("this guide")]),e._v(" to inform their decisions.")]),i("li",[e._v("Each image was independently provided a "),i("strong",[e._v("single label for each task")]),e._v(" by 3 annotators.")]),i("li",[e._v("We enforced independent labeling by hiding the labels given by the other labelers while someone was labeling.")])])]),i("p",[e._v(" Since each image was given three labels for a task, we use the plurality, or most frequent label, given to the image as the ground-truth label for that image. We chose this method of ground-truthing to minimize any specific person's contributed bias towards the ground-truth label. If there was not a plurality label, we do not label the image and thus do not put that image in the test set for that task, but did make note of the disagreement for later analysis. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("h5",[e._v("Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images")]),i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12 col-md-6 pt-3 pb-2"},[i("h2",[e._v('â€œ... a computer cannot generally agree with annotators at a rate that is higher than the rate at which the annotators agree with each other."'),i("sup",[i("a",{attrs:{href:"#fn2",id:"ref2"}},[e._v("1")])])])]),i("div",{staticClass:"carousel-text col-12 col-md-6"},[i("p",[e._v(" Since the data we have is "),i("strong",[e._v("human-annotated")]),e._v(" and thus permits subjectivity, we aimed to assess and make transparent "),i("strong",[e._v("the quality of the annotated datasets")]),e._v(", thus we computed measures of inter-annotator agreement (IAA). "),i("br"),i("br"),e._v(" This IAA analysis enabled us to determine, for each task: "),i("ul",[i("li",[e._v("How reproducible labeling for the task is")]),i("li",[e._v("If our annotation procedure can be improved:")]),i("ul",[i("li",[e._v("Refinement of task label definitions")]),i("li",[e._v("Clarifying data points of disagreement between annotators")]),i("li",[e._v("Adding more examples for each class")])])]),e._v(" This analysis has the advantage of happening before any model development, focusing on "),i("strong",[e._v("improvement of classification task formulation itself rather than building a model which will likely perform poorly on an ill-formed task.")])])])]),i("p",[i("sup",{attrs:{id:"fn2"}},[e._v("1. D. T. Nguyen, K. Al-Mannai, S. R. Joty, H. Sajjad, M. Imran, and P. Mitra, "),i("a",{attrs:{href:"https://arxiv.org/abs/1608.03902",target:"_blank"}},[e._v("â€œRapid classification of crisis-related data on social networks using convolutional neural networks,â€")]),e._v(" CoRR, vol. abs/1608.03902, 2016."),i("a",{attrs:{href:"#ref2",title:"Jump back to footnote 2 in the text."}},[e._v("â†©")])])]),i("br")]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12"},[i("h5",[e._v("Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images")])]),i("div",{staticClass:"col-12 col-md-6"},[e._v(" Agreement Measures by Task for Labeled Fukuchiyama Images "),i("img",{staticClass:"img-fluid",attrs:{src:a("5984")}})])]),i("br"),i("p",[i("strong",[e._v("Fleiss' Kappa")]),e._v(" [-1, +1] score incorporates the level of agreement attained if the annotators did not look at the data when labeling, i.e. "),i("strong",[e._v("random chance agreement")]),e._v(', which is an advantage over the complete agreement percentage ("Unanimous Agreement Percentage" pictured above), in which all annotators agree on the same label for an image. '),i("ul",[i("li",[e._v("By Fleiss' Kappa, we observe smaller improvements over random chance agreement for Damage Severity (0.413), Humanitarian Categories (0.304), and Informativeness (0.313) as compared to Flood Presence (0.829)")]),i("ul",[i("li",[e._v(" This suggests that further investigation should be conducted in refining the label definitions and the annotation guide for clarity by understanding potential causes for disagreement on the tasks with lower Fleiss' Kappa scores, in order to improve agreement and thus the quality of the dataset.")])]),i("li",[e._v("We use the plurality labels found for each task to form the ground-truth Fukuchiyama datasets for each of the tasks which we evaluate the trained CNN models on.")])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Annotated Fukuchiyama Image Test Sets")]),i("div",{staticClass:"row align-items-center justify-content-between"},[i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("4bd4")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("9409")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("6a64")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-3 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("1d5c")}})])]),i("br"),i("p",[e._v(" The ground-truth datasets are formed from the plurality labels found from the annotations given to the Fukuchiyama images. We again observe imbalance in the resulting datasets, albeit to varying degrees. Therefore, we again make use of weighted aggregate metrics for model evaluation, however, for a more granular insight into model performance, we also investigate the per-class performance of each model by precision, recall, and F1 score for each class and visualize the confusion matrix. Lastly, we establish a comparison to a baseline classifier using the Cohen's Kappa score. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("h5",[e._v("Model Evaluation on Fukuchiyama Data - Per-Class Metrics")]),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Damage Severity")]),i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-12 col-md-6 col-lg-4 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("aec1")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-5 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("9469")}})])]),i("br"),i("p",[e._v(' For the damage severity model, we notice that when the model mispredicts the "Little or None" class it predicts "Mild" far more than "Severe". Relatedly, when the damage severity model mispredicts the "Mild" class, it far more often predicts "Little or None" than "Severe". Finally, when the model mispredicts the "Severe" class, it predicts "Mild" more than either "Little or None" or "Severe". ')])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("h5",[e._v("Model Evaluation on Fukuchiyama Data - Per-Class Metrics")]),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Humanitarian Categories")]),i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-12 col-md-6 col-lg-4 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("d54d")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-5 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("4f00")}})])]),i("br"),i("p",[e._v(' When analyzing the per-class performance for each of the models on the Fukuchiyama datasets, we observe that the humanitarian categories model performance varies greatly between the classes for the task. Namely, we observed that the AIDP class has scores of 0 across all metrics. From the training set distributions discussed earlier, we observe that the AIDP class is only 6.12% of the entire training set for the humanitarian categories task. This is the smallest training set class proportion for any of the image classification tasks examined in this work, with the next lowest training set class proportions being those for the RVDE class in the humanitarian categories task and the "Mild" class of damage severity at 14.0% and 14.4%, respectively. This suggests that the imbalance of the humanitarian categories training set impacts the performance of the humanitarian categories severely on the minority classes, especially the AIDP class, the class with the lowest proportion. ')]),i("br")])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("h5",[e._v("Model Evaluation on Fukuchiyama Data - Per-Class Metrics")]),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Informativeness")]),i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-12 col-md-6 col-lg-4 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("70f6")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-5 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("0abc")}})])]),i("br"),i("p",[e._v(' By nature of the labeled Fukuchiyama crisis image data being almost exclusively related to crisis events or "normal day" photos, the "Not Informative" class is only 69 images as opposed to the 589 "Informative" photos. We note that in the original conception of the task in [1]'),i("sup",[i("a",{attrs:{href:"#fn3",id:"ref3"}},[e._v("1")])]),e._v(', the informativeness classifier is intended to be used for filtering noisy tweets which are completely unrelated to crisis events from relevant tweets, however as we report, it is not an adequate classifier for filtering images indicative of crisis impact and those of "normal-day" scenes, because, as we have learned, that is a different task altogether. We observe that the model correctly classifies most of the images labeled "Informative" with a recall score of 0.781. When classifying the images labeled as "Not Informative", the classifier classifies 53.6% of these images incorrectly as "Informative" and 46.4% of these images correctly as "Not Informative". Across all metrics, the model performs reasonably well on the "Informative" class, but significantly worse on the "Not Informative". ')]),i("hr"),i("p",[i("sup",{attrs:{id:"fn3"}},[e._v("1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, "),i("a",{attrs:{href:"https://arxiv.org/pdf/2011.08916.pdf",target:"_blank"}},[e._v("Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response")]),e._v(", In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020."),i("a",{attrs:{href:"#ref3",title:"Jump back to footnote 1 in the text."}},[e._v("â†©")])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("h5",[e._v("Model Evaluation on Fukuchiyama Data - Per-Class Metrics")]),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Flood Presence")]),i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-12 col-md-6 col-lg-4 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("6e61")}})]),i("div",{staticClass:"col-12 col-md-6 col-lg-5 pt-3"},[i("img",{staticClass:"img-fluid",attrs:{src:a("0c67")}})])]),i("br"),i("p",[e._v(" Unlike the models for damage severity, humanitarian categories, and informativeness, the flood presence model performs consistently well (less variation and higher values) by all metrics across all classes in the task, attaining metric scores at or above 0.793 across all metrics for both classes. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12"},[i("h5",[e._v("Model Evaluation on Fukuchiyama Data - Aggregate Metrics")])]),i("div",{staticClass:"col-12 col-md-8"},[e._v(" Performance of Image Classification models on task-respective Fukuchiyama Data "),i("img",{staticClass:"img-fluid",attrs:{id:"agg-metrics",src:a("2a76")}})])]),i("br"),i("p",[e._v(" By Cohen's Kappa score, we see that the damage severity and informativeness tasks provide a relatively small improvement over the random classifier for their corresponding datasets as compared to the humanitarian categories model and far more so for the flood presence model, which provides the most improvement over the random classifier for its dataset. ")]),i("p",[e._v(" We observe across all of the weighted metrics for all tasks, the performances of the models on the Fukuchiyama data are lower than the performances achieved on the consolidated crisis image test splits and flood presence test set reported earlier. This is most apparent for the damage severity model, which achieves a weighted F1 score of 43.2% on the Fukuchiyama data. Similar to the performance of the flood presence model on flood presence test split, the flood presence model performs relatively well on the Fukuchiyama flood presence task images achieving a weighted F1 of 82.5%. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Model Evaluation on Fukuchiyama Data - Discussion")]),i("br"),i("h6",[e._v("Performance on Fukuchiyama Data")]),i("p",[i("ul",[i("li",[e._v(" Investigating the "),i("strong",[e._v("per-class performance")]),e._v(" allowed us to see which classes may be suffering from "),i("strong",[e._v("class imbalance issues")]),e._v(' (e.g. Affect, Injured, or Dead People) as well as the common mistakes a model makes when predicting, such as when the damage severity model commonly mispredicts "mild" for actual "little-or-none" images. ')]),i("li",[e._v("There are likely multiple reasons why the model performance is comparatively lower for the Fukuchiyama data as compared to the test splits for the consolidated crisis image datasets and the flood presence dataset: "),i("ul",[i("li",[e._v("May be in part due to "),i("strong",[e._v("concept drift")]),e._v(" between the data the models were "),i("strong",[e._v("trained on")]),e._v(" and the Fukuchiyama data which the models were "),i("strong",[e._v("evaluated on")])]),i("li",[e._v("Labeled Fukuchiyama data may have been of "),i("strong",[e._v("poorer data quality")]),e._v(" as suggested from the relatively "),i("strong",[e._v("low Fleissâ€™ Kappa coefficients")]),e._v(" for the damage severity, humanitarian categories, and informativeness tasks")])])]),i("li",[e._v(" The "),i("strong",[e._v("low Fleiss' Kappa scores")]),e._v(" for the damage severity, humanitarian categories, and informativeness tasks can "),i("strong",[e._v("potentially be improved")]),e._v(" by converting the abstract "),i("strong",[e._v("definitions")]),e._v(" of different classes "),i("strong",[e._v("into checklists")]),e._v(", understanding common "),i("strong",[e._v("annotator disagreements")]),e._v(", and adding "),i("strong",[e._v("more clarification/examples")]),e._v(" where necessary in the annotation procedure. ")]),i("li",[e._v(" We consider the performance of the Flood Presence model across the datasets to be robust, which we attribute to the task being binary (as opposed to multiclass) and to the task having classes which yield higher agreement between annotators. ")])])]),i("h6",[e._v("Importance of Selecting a Metric based on Crisis Management Priorities and the Nature of the Data")]),i("p",[i("ul",[i("li",[e._v("While the Flood Presence model may be performant, we aimed to understand if this task as well as the other tasks have informative utility to crisis managers during flood crisis.")]),i("li",[e._v("Although "),i("strong",[e._v("weighted F1")]),e._v(" is a popular metric reported in the literature, it is "),i("strong",[e._v("biased towards the model's performance on the majority classes.")])]),i("li",[e._v("Cohen's Kappa provides the level of accuracy achieved that is above the "),i("strong",[e._v("random classifier baseline.")])]),i("li",[e._v("Cohenâ€™s Kappa can be a more appropriate metric to use in the case of multiclass classification tasks when the dataset is imbalanced, but there is likely an even more appropriate metric depending on the task and the priorities of crisis managers.")]),i("li",[e._v("We have determined that selecting a model performance metric should be a process which both considers the "),i("strong",[e._v("nature of the data")]),e._v(" (i.e. class imbalance) & uses "),i("strong",[e._v("insights from crisis managers")]),e._v(".")])])]),i("h6",[e._v("Understanding the Informative Utility of the Image Analysis Module")]),i("p",[e._v(" To broaden the discussion of the efficacy the image analysis module has in mitigating information overload and enhancing situational awareness, we also qualitatively examine the informative utility the image models have in assisting crisis managers during a flood crisis event. We held image annotation workshops with various crisis managers and aimed to understand what type of information they seek to gain from a crowdsourced image during a flood crisis event as well as what their priorities are. We iterate on our methodology by using insights we gained from the crisis managers into the Text Analysis Module, exhibiting the principle of iterative development ou framework intends to promote. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v(" Image Annotation Workshops with EOC & Methodology Iteration ")]),i("p",[e._v(" The Urban Risk Lab"),i("sup",[i("a",{attrs:{href:"#fn4",id:"ref4"}},[e._v("1")])]),e._v(" held image annotation workshops in December 2021 with the following crisis experts in Fukuchiyama: "),i("ul",[i("li",[e._v("Director of the Regional Disaster Management Research Center, Fukuchiyama Public University and Former Crisis Management Supervisor of Fukuchiyama City")]),i("li",[e._v("3 Crisis Managers at an EOC in Fukuchiyama")]),i("li",[e._v("5 Associates (including Fire Department Director & 1 Firefighter) of the Fire Department in Fukuchiyama")])]),e._v(" In January 2022, the same workshop was held with a former Deputy Administrator of the Federal Emergency Management Agency (FEMA) in the US ")]),i("br"),i("p",[e._v(" Crisis experts were "),i("strong",[e._v("presented 25 images")]),e._v(" from past FC flood crises. The images represented various types of crisis impact."),i("sup",[i("a",{attrs:{href:"#fn5",id:"ref5"}},[e._v("2")])]),e._v(" A subset of the images were selected because they had "),i("strong",[e._v("disagreement between annotators")]),e._v(" & were given "),i("strong",[e._v("a wrong prediction by the CNN model")]),e._v(" developed for a task. ")]),i("br"),i("p",[e._v(" The "),i("strong",[e._v("crisis experts were tasked with labeling images with a variety of labels")]),e._v(" & "),i("strong",[e._v("identifying insights")]),e._v(" from an image that are "),i("strong",[e._v("useful for decision making and response")]),e._v(" during crisis events. ")]),i("br"),i("p",[i("sup",{attrs:{id:"fn4"}},[e._v(" 1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who conducted the qualitative focus-group research in the form of interviews, workshops, and general interfacing with our partners in Fukuchiyama and in the US, provided the translations of the results from Japanese to English to enable the analysis of those results as it relates to the work presented in this thesis, and assisted in the synthesis of the main findings from the observations and discourse that occurred during the image annotation workshops. We note that Saeko Baird has formal training in Human-Computer Interaction (HCI) research."),i("a",{attrs:{href:"#ref4",title:"Jump back to footnote 1 in the text."}},[e._v("â†©")])]),i("br"),i("sup",{attrs:{id:"fn5"}},[e._v(" 2. "),i("strong",[e._v("Crisis Impact Types:")]),e._v(" river flood, rock-fall, landslide, residential housing damage, blocked roads, agricultural land damage, submerged residential areas, and damaged infrastructure."),i("a",{attrs:{href:"#ref5",title:"Jump back to footnote 2 in the text."}},[e._v("â†©")])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Image Annotation Workshop & Methodology Iteration - Aims")]),i("p",[e._v(" With these workshops, we aimed to: "),i("ul",[i("li",[e._v("Understand "),i("strong",[e._v("cross-contextual insights")]),e._v(", i.e. information needs and decision-making priorities common to both Fukuchiyama & US crisis management")]),i("li",[i("strong",[e._v("Compare our devised image analysis ML methodology")]),e._v(" for automatic insights to the insights gained from "),i("strong",[e._v("manual assessment")]),e._v(" of crisis images "),i("strong",[e._v("by crisis experts.")])]),i("li",[e._v("Use results to "),i("strong",[e._v("iterate on design of ML methodology")]),e._v(" to better embed information needs and priorities of crisis managers.")])])]),i("p",[e._v(" The following questions were posed for each of the images to actively engage crisis managers in the annotation process. We note, however, that the conversations often expanded beyond these questions, getting to the core of what their main insights would be through manual assessment of the image data and what their main decision priorities would be during a flood crisis event as it related to analyzing the image data: ")]),i("br"),i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12 col-md-8"},[i("img",{staticClass:"img-fluid",attrs:{src:a("835a")}})])]),i("br")])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Image Annotation Workshops with Crisis Experts - Results")]),i("p",[e._v(" We report qualitative summaries describing the insights derived from manual assessment by domain experts of the crisis report images and their expressed information needs. We used these summaries to compare how the insights the Image Analysis Module aims to automatically provide to crisis managers compares to the insights derived from manual assessment by crisis experts and their expressed information needs. This comparison allowed us to qualitatively assess the utility of the Image Analysis Module in attaining situational awareness as it was devised in this work, and helped us in determining ways to iterate on the module in the future to better accommodate the information needs of the crisis managers during a crisis event through new prediction tasks which align with their information needs. ")]),i("h6",[e._v("Cross-contextual Insights")]),i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12"},[i("p",[i("ul",[i("li",[i("strong",[i("u",[e._v("Potential of Human Casualties in Crisis Imagery")])]),i("ul",[i("li",[e._v("Possibility of Human Casualty is "),i("strong",[e._v("Top Priority")])]),i("li",[i("strong",[e._v("Identified physical markers")]),e._v(" suggesting "),i("strong",[e._v("potential for human casualty:")]),i("ul",[i("li",[e._v("Submerged Vehicles")]),i("li",[e._v("Collapsed Buildings")]),i("li",[e._v("Housing in Close Proximity to Rockfall or Landslide")])])]),i("li",[e._v(" Not investigating when there is actually human casualty (False Negative) "),i("strong",[e._v("is more costly")]),e._v(" than investigating when there is not actually human casualty (False Positive) ")])])]),i("li",[i("strong",[i("u",[e._v("Presence of People in Crisis Imagery")])]),i("ul",[i("li",[i("strong",[e._v("People in crisis imagery")]),e._v(" is important and should be "),i("strong",[e._v("assessed with high priority")])]),i("li",[i("strong",[e._v("Insights should be specific,")]),e._v(" e.g. people laying down vs. people standing casually/walking about - can indicate crisis impact severity to personnel ")])])]),i("li",[i("strong",[i("u",[e._v("Insights of Broader Impact derived from Physical Markers in Images:")])]),i("ul",[i("li",[e._v(" Experts "),i("strong",[e._v("identified physical markers")]),e._v(" which suggest "),i("strong",[e._v("potential broader impact to area")]),e._v(", including: "),i("ul",[i("li",[e._v("Muddy Water â†’ potential nearby landslide")]),i("li",[e._v("Fallen Power Pole â†’ potential power outage")]),i("li",[e._v("Road Passability â†’ possibility of emergency vehicle use & isolated residential areas")])])])])])])])])]),i("h6",[e._v("Contextual Insights")]),i("div",{staticClass:"row align-items-center justify-content-around"},[i("div",{staticClass:"col-12"},[i("p",[i("ul",[i("li",[i("strong",[i("u",[e._v("National Standards in Japan for Assessing Impact Severity")])]),i("ul",[i("li",[e._v(" Standards for Flood Impact Severity on Housing used in Fukuchiyama: "),i("ul",[i("li",[e._v("â€œWater reaches up to the first-floor ceilingâ€ â†’ Severe Flooding/Destruction")]),i("li",[e._v("â€œWater reaches 1m above first-floor levelâ€ â†’ Partial Flooding/Destruction")]),i("li",[e._v("â€œWater reaches below flood levelâ€ â†’ Minor Flooding/Destruction")])])])])]),i("li",[i("strong",[i("u",[e._v("Insights Derived from both the Image and Contextual-Knowledge:")])]),i("ul",[i("li",[i("strong",[e._v("FC crisis experts used contextual knowledge")]),e._v(" of the area where the image was taken in gaining insights "),i("ul",[i("li",[e._v("E.g. Image showing flooding in an area that doesnâ€™t typically flood causes more concern for that area")])])])])])])])])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Image Annotation Workshop & Methodology Iteration - Discussion")]),i("p",[i("ul",[i("li",[e._v(" From crisis expert insights and feedback, we have determined that the tasks as presented in this work have classes with interpretations that are either "),i("strong",[e._v("too vague and subjective (damage severity, humanitarian categories, and informativeness)")]),e._v(" or "),i("strong",[e._v("too simplistic (flood presence)")]),e._v(" to be useful for them in gaining situational awareness about an unfolding crisis event. ")]),i("li",[e._v(' The humanitarian categories task has the "Rescue, Volunteering, or Donation Effort" class, which has insights for the '),i("strong",[e._v("recovery phase of a crisis event rather than the emergency phase. Since our ML methodology aims to assist crisis managers during the emergency phase of a crisis event, such classes should be revised or replaced with classes which have insights directly for the emergency phase.")])]),i("li",[e._v(" Although the "),i("strong",[e._v("flood presence task")]),e._v(" has classes with interpretations which are too simple for attaining situational awareness, we note that the "),i("strong",[e._v("relatively high performance, high consistency between independent annotators, and clarity")]),e._v(" in the interpretation of the classes associated with the flood presence task "),i("strong",[e._v("sets precedent for task creation and model performance for the future tasks")]),e._v(" developed from the insights and feedback received from the workshops discussed in this work and future workshops. ")])])]),i("p",[e._v(" The insights and feedback provided by the domain experts enabled us to determine how the Image Analysis Module we have developed in this work is limited in helping to gain insights about the unfolding crisis event. "),i("strong",[e._v("Where our ML methodology falls short in meeting their information needs, their feedback will assist in developing new classification tasks which would be informative enough to assist them during a crisis event and clear enough to yield more consistent labels between annotators, ensuring better quality data to train and evaluate models.")]),e._v(" The development of new image prediction tasks and associated models will be conducted in a future work. However, we were able to apply some of these insights to inform the ML methodology of the "),i("a",{attrs:{href:"#/projects/ml-for-crowdsourced-crisis-data/text-analysis-module"}},[e._v("Text Analysis Module")]),e._v(". ")])])])])}],we={name:"ImageAnalysisCarousel",methods:{scrollUp(){H()}}},be=we,_e=(a("4129"),Object(u["a"])(be,ve,ye,!1,null,"acc745a0",null)),Ce=_e.exports,ke=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},xe=[function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-8"},[i("h3",[e._v("Text Analysis Module")])]),i("div",{staticClass:"col-md-10"},[i("div",{staticClass:"carousel slide",attrs:{id:"ccCarousel","data-ride":"carousel","data-interval":"false"}},[i("ol",{staticClass:"carousel-indicators"},[i("li",{staticClass:"active",attrs:{"data-target":"#ccCarousel","data-slide-to":"0"}})]),i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item active cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("img",{staticClass:"img-fluid",attrs:{id:"pika-gif",src:a("9faf"),alt:"First slide"}})])])])])])])}],Te={name:"TextAnalysisCarousel"},Fe=Te,je=(a("34b4"),Object(u["a"])(Fe,ke,xe,!1,null,"055562fa",null)),Ie=je.exports,Me=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row align-items-center justify-content-center"},[e._m(0),a("div",{staticClass:"col-md-8"},[a("div",{staticClass:"carousel slide",attrs:{id:"NLPIntLitDevCarousel","data-ride":"carousel","data-interval":"false"}},[a("ol",{staticClass:"carousel-indicators"},[a("li",{staticClass:"active",attrs:{"data-target":"#NLPIntLitDevCarousel","data-slide-to":"0"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#NLPIntLitDevCarousel","data-slide-to":"1"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#NLPIntLitDevCarousel","data-slide-to":"2"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#NLPIntLitDevCarousel","data-slide-to":"3"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#NLPIntLitDevCarousel","data-slide-to":"4"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#NLPIntLitDevCarousel","data-slide-to":"5"},on:{click:e.scrollUp}})]),e._m(1)])])])},Pe=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-12 col-md-8"},[a("h3",[e._v("Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature")])])},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item active int-dev-lit-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("269c"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Project Presentation, Report, and Code")]),i("p",[e._v(" This was a final project for 6.864: Advanced Natural Language Processing. This presentation focuses on introducing the project, the specific parts I worked on, and the main results from our analysis. "),i("br"),e._v(" A brief overview of the motivation, methods, and results is available in this "),i("a",{attrs:{href:"./assets/int-dev-gray-lit.pdf",target:"_blank"}},[e._v("presentation PDF.")]),i("br"),e._v(" The more thorough report of our methodology, visualizations, and findings "),i("a",{attrs:{href:"./assets/6_864_Project.pdf",target:"_blank"}},[e._v("here.")]),i("br"),e._v(" The code for this project was written in Python. "),i("a",{attrs:{href:"https://github.com/dyllew/6.864-fp",target:"_blank"}},[e._v("Here's the GitHub Repo.")])]),i("p",[e._v(" For this project, my main contributions were the supervised methodology/Named Entity Recognition stream discussed in the following slides ")])])]),i("div",{staticClass:"carousel-item int-dev-lit-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Abstract")]),i("p",[e._v(" In fields like international development, decision-makers prioritize making evidence-based decisions for funding and implementing future projects. This aim is made difficult because of the plethora of information being published each year, and the nature of the research corpus as unstructured text or grey literature. To make informed decisions and understand the growing corpus of research available, researchers have turned to evidence synthesis - the process of compiling information and knowledge from many sources and disciplines to inform decisions. However, the manual evidence synthesis process takes extensive time (often 18 months to 3 years) and effort, and may soon be impossible at the worldâ€™s increasing rate of research output. To address these problems, we employ natural language processing techniques on a international development literature corpus of 244 documents to extract information from the title and abstract of international development documents, and to automatically cluster documents based on their content. We classify documents by Country of Study using a pretrained transformer Named Entity Recognition model and achieve an accuracy of 91.0%. Using K-Means clustering, we uncover informative and distinctive groupings of the documents which share similar semantic content. These methods reduce the time it takes for manual evidence synthesis for international development grey literature by enabling country of study filtering and clustering documents by semantic similarity. ")])])]),i("div",{staticClass:"carousel-item int-dev-lit-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Named Entity Recognition (NER) for Country of Study (CoS) Classification - Models")]),i("p",[e._v(" The country of study (CoS) associated with each paper is quite pertinent to the international development domain. Our dataset is labeled with the CoS of each paper in our corpus. However, since our dataset is rather small (244 documents), we sought to evaluate whether pretrained NER models which extract a variety of entity types from text, could accurately extract the CoS for the papers in our corpus, which have a variety of text fields. ")]),i("h6",[e._v("Country of Study Extraction and Classification")]),i("p",[e._v(" We create a lower-cased, alphabetically-ordered, list of countries, which we construct using countryinfo "),i("a",{attrs:{href:"https://pypi.org/project/countryinfo/",target:"_blank"}},[e._v("(Link to CountryInfo PyPI page)")]),e._v(", a Python package which contains a large dictionary of countries, their alternative names, and ISO information. We ensure the strings of the countries present in our corpus match their respective string in the alphabetically-sorted list of countries. We note that Myanmar and Kosovo are countries present in our corpus, but are not present in the countryinfo dictionary, so we add them to the final list of alphabetically-sorted countries. Since nationality is a type of named entity that NER models typically extract in addition to countries, using a comprehensive, open-source nationality-country mapping "),i("a",{attrs:{href:"https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv",target:"_blank"}},[e._v("(Demonym-Country Mapping Link)")]),e._v(", we construct a lower-cased, alphabetically-ordered list of nationalities as well as a dictionary mapping nationality to country. We note that we use the words nationality and demonym interchangeably. These lists and dictionary are useful for performing the country of study (CoS) classification using extracted entities from input text or determining if a nationality or country is a substring contained in the input text string. ")]),i("h6",[e._v("Simple Substring Matcher (SSM) Algorithm Baseline & CoS Extraction & Classification")]),i("p",[e._v(" As a baseline to our CoS prediction task, we devise a simple, non-ML, deterministic algorithm, called the Simple Substring Matcher (SSM) Algorithm. This method begins by making the input text lower-cased. To predict a CoS, it then scans through the alphabetically-sorted list of countries and classifies the first country which is a substring in the input text as the CoS. If no country is found as a substring in the text, the method then scans the alphabetically-sorted list of nationalities. If a nationality is found as a substring of the input, the method maps the nationality to the corresponding country and classifies the paper as having that country as the CoS. If neither country nor nationality is found as a substring in the text, the method classifies the paper's CoS as a "),i("i",[e._v("None")]),e._v(" value. We refer to this classification model as the Simple Substring Matcher (SSM) model. ")]),i("h6",[e._v("CoS Extraction & Classification by Pretrained NER Models")]),i("p",[e._v(" Although we utilize different pretrained NER models in our experiments as shown in the next slide, the process for classifying CoS using predicted entities is the same. Each model takes the raw text as input, predicts various non-overlapping entities present in the text into one of several entity categories. For the CoS classification task, we only consider the predicted "),i("b",[e._v("NORP")]),e._v(" (nationalities, or religious, or political groups) entities and the "),i("b",[e._v("GPE")]),e._v(" (countries, cities, states) entities as we assume that these categories are the only ones which would contain the country or relevant demonym associated with the CoS. We now begin our discussion of the classification procedure for the pretrained NER models. First, we make all NORP and GPE entities lower-cased. Next, we map any demonyms present among the NORP entities to their corresponding country. We then combine the resulting unique NORP and GPE entities into an alphabetically-ordered list. We scan this list of NORP and GPE entities checking if any of them exist in the countries list mentioned above, classifying the CoS as the first entity-country match found. If no match is found, we make a final attempt to determine the CoS by providing each of the entities as input to the GeoPy Geocoder "),i("a",{attrs:{href:"https://geopy.readthedocs.io/en/stable/",target:"_blank"}},[e._v("(Link to GeoPy API)")]),e._v(" object, which provides an address-location object if a location is found for the provided entity or no value otherwise. We do this for each entity, and if a location is found for a particular entity, we classify the paper's CoS as the country associated with the found address-location. If no country is found for all the entities, we classify the paper's CoS as "),i("i",[e._v("None")]),e._v(". ")])])]),i("div",{staticClass:"carousel-item int-dev-lit-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("332f"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("NER for Country of Study Classification - Results")]),i("p",[e._v(" In addition to testing different classification models, I experimented with different input strings to see how results change with various text fields and concatenations between them. These various inputs to the models include the title, abstract, intervention description, outcome description, and various concatenations of these text fields. ")]),i("p",[e._v(" All of the pretrained spaCy NER models have 0.0% accuracy when using the just the intervention description, however the SSM model achieves 13.9% accuracy on the intervention description. All models attain an accuracy of 2.9% when using just the outcome description. The title and abstract individually appear to be good input fields for predicting the CoS, however the concatenation of title and abstract appears to be the most informative input, as this is the input that yields the highest performance across all of the models. Overall, we observe that the baseline simple substring checker is a fairly competitive model against the pretrained ML models, outperforming all the ML models on intervention description, performing the same as the ML models on outcome description, and only falling a few percentage points below even the best ML model on the other inputs. With the exception of the title, intervention description, and outcome description, the ML models in increasing order of complexity, do increasingly better on the CoS extraction task, in the following order from least performant to most performant: ESMS, ESMM, ESML, and ESMT. With the exception of the intervention description and output description inputs, we observe that the ESMT model performs best across all other inputs. Furthermore, we see that the concatenated title and abstract input and the ESMT model combination performs the best across all input-model combinations with 91.0% accuracy. We use this top-performing model by accuracy to construct AI-assisted tools which could assist researcher in the evidence in the following slide. ")])])]),i("div",{staticClass:"carousel-item int-dev-lit-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("2981"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Predictions by Pretrained Transformer NER Model for International Development Gray Literature Map and Filter Function")]),i("h6",[e._v("Map of International Development Literature Gray Corpus")]),i("p",[e._v(" Using the CoS predictions from the pretrained NER transformer model on the concatenated title and abstract input, we construct a geographical map of the corpus as shown in the left image. For each paper, which had a non-null prediction by the ESMT model, we place a tooltip at location coordinate associated with the predicted CoS. These location coordinates were pulled using the GeoPy Geocoder object from the GeoPy Python package. We added slight, uniform random jitter to each of the coordinates, so papers with the same predicted CoS don't directly overlap. When a user hovers over the tooltip, they will see the title of the paper associated with that tooltip. The webpage for this map can be downloaded "),i("a",{attrs:{href:"https://drive.google.com/file/d/1Q2P6ouwcDWrnXsq8LMpa4YqCuD7qsbtO/view?usp=sharing",target:"_blank"}},[e._v("here")]),e._v(" for view in a browser. ")]),i("h6",[e._v("Filtering by Predicted CoS")]),i("p",[e._v(" For large corpora of International Development Gray Literature, the utility of the CoS prediction task is most evident by the robust filtering capability it enables. For instance, by concatenating only the title and abstract of papers in the corpus, and using them as input to generate CoS predictions by the pretrained transformer model used in this study, this enables the ability for unlabeled papers in the corpus to be accurately filtered to identify studies which had a specific CoS. This method would greatly reduce the time necessary for manual CoS annotation while also yielding higher accuracy than a simple substring matcher, simplifying a step in the international literature review process with high accuracy. An example of this filtering functionality is shown in the right image for papers in the corpus, which were predicted as having Guatemala as the CoS. The CoS was predicted using the pretrained transformer NER model with the concatenated title and abstract as input. We display the corresponding title and abstract for quick scanning of results for relevancy to research topic. Additionally, we provide the option to filter the corpus for papers which had a predicted CoS as "),i("i",[e._v("None")]),e._v(". ")])])]),i("div",{staticClass:"carousel-item int-dev-lit-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("269c"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Conclusion")]),i("p",[e._v(" The manual evidence synthesis for international development gray literature is a time-consuming process. We have demonstrated that certain components of the evidence synthesis process in international development gray literature such as filtering corpora for papers which have a specific country of study or grouping similar documents together can benefit greatly from the use of methods of information extraction and unsupervised learning. More specifically, we have utilized a pretrained transformer NER model to accurately predict the country of study for the papers present in the corpus used in this study, thus enabling accurate filtering of the corpus for papers with a specific predicted country of study. After tuning to find the optimal number of clusters in K-Means clustering, we uncovered informative and distinctive clusters of documents with similar content in the corpus. The automation of these components in the evidence synthesis process for international development grey literature mitigates the effort and time that is required for manual evidence synthesis. ")])])])])}],Se={name:"NLPIntDevGrayLit",methods:{scrollUp(){H()}}},Ne=Se,Ae=(a("7332"),Object(u["a"])(Ne,Me,Pe,!1,null,"8440b6de",null)),De=Ae.exports,Ee=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},Le=[function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-12"},[i("h3",[e._v("Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction")])]),i("div",{staticClass:"col-md-8"},[i("div",{staticClass:"carousel slide",attrs:{id:"taxiCarousel","data-ride":"carousel","data-interval":"false"}},[i("ol",{staticClass:"carousel-indicators"},[i("li",{staticClass:"active",attrs:{"data-target":"#taxiCarousel","data-slide-to":"0"}}),i("li",{attrs:{"data-target":"#taxiCarousel","data-slide-to":"1"}}),i("li",{attrs:{"data-target":"#taxiCarousel","data-slide-to":"2"}})]),i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item active taxi-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("f124"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Motivation")]),i("p",[e._v(" With the advent of ridesharing apps, the New York City taxi industry must provide accurate predictions for taxi fares and demand surges in order to remain competitive. It is important that it provides riders with accurate estimates for the price of trip fare for quality customer service. It is beneficial for the taxi industry to accurately predict locations that will experience increased demand or surges in taxi demand to effectively allocate drivers and cabs to these areas in a timely manner. In this project, we used a large public dataset provided by the NYC Taxi & Limousine Commission containing yellow taxi trips records for every month from 2015-Present. We specifically utilized data from January 2019-June 2019. ")])])]),i("div",{staticClass:"carousel-item taxi-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("84e6"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Graph Neural Networks for Taxi Fare and Surge Prediction")]),i("p",[e._v(" The Yellow Taxi Trip Records contain data about taxi trips including pickup and dropoff locations, date and time of pickup and dropoff, distance traveled, and the associated fare of the trip. This data has inherent graph structure in which the nodes are the different pickup/dropoff locations and the edges are the directed trips between them. Due to the increasing advances of Graph Neural Networks (GNNs) in recent years and the advent of efficient frameworks like Deep Graph Library (DGL), we evaluated the performance of GNNs against other classical machine learning methods to assess the viability of GNNs as an accurate model which leverages the inherent graph structure in the data used for these tasks. ")])])]),i("div",{staticClass:"carousel-item taxi-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("84e6"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Results")]),i("p",[e._v(" Using the raw yellow taxi trip data from January 2019-June 2019, we contrived two datasets to pose node and regression problems for graphical methods. Using GraphSage GNNs, we explored and benchmarked a new application of GNNs to taxi data against classical ML approaches. For fare prediction, the GNN model performed slightly worse than Linear Regression, Random Forests, and a Fully-Connected Neural Network (FC NN), with the FC NN performing the best, however, it is highly-parameterized. For surge prediction, the GNN performed slightly better than a FC NN. For both taxi prediction tasks, we demonstrated that a less complex GNN model can perform comparably to a highly-parameterized FC NN. ")])])])])])])])}],We={name:"Taxi"},Re=We,Ue=(a("2f27"),Object(u["a"])(Re,Ee,Le,!1,null,"2ba5834e",null)),qe=Ue.exports,Oe=function(){var e=this,t=e.$createElement;e._self._c;return e._m(0)},ze=[function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"row align-items-center justify-content-center"},[i("div",{staticClass:"col-12"},[i("h1",[e._v("Trump Campaign Speech Analysis")])]),i("div",{staticClass:"col-md-8"},[i("div",{staticClass:"carousel slide",attrs:{id:"trumpCarousel","data-ride":"carousel","data-interval":"false"}},[i("ol",{staticClass:"carousel-indicators"},[i("li",{staticClass:"active",attrs:{"data-target":"#trumpCarousel","data-slide-to":"0"}}),i("li",{attrs:{"data-target":"#trumpCarousel","data-slide-to":"1"}}),i("li",{attrs:{"data-target":"#trumpCarousel","data-slide-to":"2"}})]),i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item active trump-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("89e4"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Main Puzzle")]),i("p",[e._v(" There have been concerns that nationalist, right-wing sentiments have gained momentum over the years of the Trump presidency. Our group wanted to investigate how Donald Trumpâ€™s rhetoric may have influenced public sentiment on a regional level. To this end, we analyzed Trump's campaign speeches and the tweets by locals from 4 cities he visited on his campaign and Florida, a swing state. ")])])]),i("div",{staticClass:"carousel-item trump-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("3bc0"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Most Frequent Negative and Positive Words in Trump's Campaign Speeches")]),i("p",[e._v(' Trump\'s positive sentiment words tend to be adjectives with "great" far exceeding the rest. Among words with negative sentiment, there are more meaningful words related to his speech topics such as "investigation", "defense", "deficit", & "press". ')])])]),i("div",{staticClass:"carousel-item trump-carousel-item"},[i("img",{staticClass:"col-12 rounded img-fluid",attrs:{src:a("3ea6")}}),i("img",{staticClass:"col-12 rounded img-fluid pt-4",attrs:{src:a("f061")}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Trump's Most Frequently Used Words Across his Entire Campaign & Across Florida Campaign")]),i("p",[e._v(' In Trump\'s speeches across the entire campaign, his most frequent words, normalized on Romney\'s campaign speeches, include "Hillary", "don\'t" "great", "deal", as well as words related to his election platform such as "border", "wall", "Mexico", "ISIS", "trade", and "China". Words used to thwart Hillary Clinton\'s campaign such as "Hillary", "email", "lies", "corrupt", "crook", and "FBI" in regards to Clinton\'s email scandal appear more frequently in Trump\'s Florida campaign speeches than across all of his campaign speeches, showing that in swing states, Trump strategizes to mention the scandal more frequently to win voters to tip the scale. ')])])])])])])])}],$e={name:"Trump"},He=$e,Be=(a("4162"),Object(u["a"])(He,Oe,ze,!1,null,"3213d2e9",null)),Je=Be.exports,Ge=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row align-items-center justify-content-center"},[e._m(0),a("div",{staticClass:"col-md-8"},[a("div",{staticClass:"carousel slide",attrs:{id:"ccCarousel","data-ride":"carousel","data-interval":"false"}},[a("ol",{staticClass:"carousel-indicators"},[a("li",{staticClass:"active",attrs:{"data-target":"#ccCarousel","data-slide-to":"0"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"1"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"2"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"3"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#ccCarousel","data-slide-to":"4"},on:{click:e.scrollUp}})]),e._m(1)])])])},Ke=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-12"},[a("h3",[e._v("Evolution of the U.S. TV News Narrative on Climate Change")])])},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item active cc-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("d8cb"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Project Code, Poster, and Report")]),i("p",[e._v(" This was a final project for IDS.131: Statistics, Computation, and Applications. This presentation focuses on introducing the project, the specific parts I worked on, and the main findings from our analysis. "),i("br"),e._v(" A brief overview of the methods, visualizations, and results is available in this "),i("a",{attrs:{href:"./assets/IDS131_Poster.pdf",target:"_blank"}},[e._v("poster PDF.")]),i("br"),e._v(" The more thorough report of our findings with all visualizations is "),i("a",{attrs:{href:"./assets/IDS131_Final_Report.pdf",target:"_blank"}},[e._v("here.")]),i("br"),e._v(" The code for this project was written in Python. "),i("a",{attrs:{href:"https://github.com/dyllew/ids.131-fp",target:"_blank"}},[e._v("Here's the GitHub Repo.")])])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("div",{staticClass:"carousel-text"},[i("h5",[i("strong",[e._v("Motivation & Research Question")])]),i("p",[e._v(" Print and televised media reporting on climate change influences the public perception of climate change, which in turn affects support for systemic policies to reduce greenhouse gas emissions and for individual actions to mitigate climate change. Over two thirds of Americans get their news often or sometimes from television. In this analysis, we looked at ten years of data from three television stations: CNN, Fox News, and MSNBC to address the following research question: ")]),i("br"),i("strong",{attrs:{id:"research-question"}},[e._v(" How has the frequency and content of top American English-speaking news media coverage of climate change evolved in the past ten years (July 2009-January 2020)-and what environmental and political factors have influenced the trends? ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("64e1"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Dataset, Exploration, and Preprocessing")]),i("p",[e._v(" The data used for this analysis includes text snippets of 15-seconds of TV news audio transcripts of climate change mentions on CNN, MSNBC, and Fox News from July 2009-January 2020, provided by the "),i("a",{attrs:{href:"https://blog.gdeltproject.org/a-new-dataset-for-exploring-climate-change-narratives-on-television-news-2009-2020/",target:"_blank"}},[e._v("GDELT Project.")]),e._v(" The features of the data points include time of day and date of the mention, the TV news network, the show, and the text snippet of the transcribed audio. This dataset provides the ability to compare the TV networks over time on the subject of climate change in order to answer the research question posed. We followed standard Natural Language Processing (NLP) text preprocessing by removing punctuation and numbers, converting all letters to lower-case (the data was already provided as lowercase), lemmatizing, removing standard English stopwords & corpus-specific stopwords, and tokenizing the data into words. We conducted our analysis with two distinct analysis streams: Frequency Analysis and Content Analysis. Our Frequency Analysis entailed methods of time-series analysis of climate change mentions by the different TV networks over time and dynamic time-warping & STL decomposition. Content Analysis used methods of TF-IDF document embeddings, Cosine Similarity as a proxy for content similarity between documents, and Topic Modeling & Change-Point Detection. The above figure was created as part of an exploratory component of the TF-IDF Embedding analysis, in which we wished to extract the most important words to each of the networks across the entire corpus. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("cad8"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Content Analysis: TF-IDF Document Embedding & Cosine Similarity")]),i("p",[e._v(" My contribution to this project was primarly focused in the Content Analysis stream, specifically TF-IDF document embeddings & Cosine Similarity between documents. To conduct our content analysis, we needed to featurize the news snippets, or collections of snippets, which form the documents in our corpus. I first formed a document for each TV network in our dataset (e.g. all snippets for CNN), and transformed the documents into a L2-normalized unit vector TF-IDF vector embedding. From this featurization, I formed a document-term matrix, where the rows correspond to the TF-IDF embedding of a document and the column represents a unique word in the corpus (~34,000 words). Thus, entry i, j corresponds to the normalized TF-IDF score of word j in document i (i.e. word j's relative importance for the ith document). I also constructed document-term matrices for documents representing each year of our dataset (e.g. all snippets in 2009) as well as for documents constructed from networks in specific years (e.g. all CNN snippets in 2015). Finally, for each of the document-term matrices mentioned above, I calculated the pairwise cosine similarity between the document embeddings to yield a measure of content similarity between the documents. A heatmap constructed from the computed cosine similarities between the network & year documents is shown above. The main findings from my analysis were that the content of climate mentions in the latter years of the dataset, 2016-2020, are most dissimilar to earlier years in the dataset, 2009-2012. Additionally, in the years 2010 & 2018, the content of MSNBC differed greatly from the other news networks in those years and with other networks and itself in other years. This similarly occurred for CNN in 2012 & 2013. Lastly, with the exception of these years, the similarity of content of climate mentions between CNN and MSNBC, the liberal-leaning networks, has been increasing over the years, although the pairwise content similarity between all of the networks is fairly high over time. ")])])]),i("div",{staticClass:"carousel-item cc-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("d8cb"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Results")]),i("p",[e._v(" Climate change TV news media coverage frequency and content appears to be significantly driven by political events more so than environmental factors. The frequency of climate change mentions follow similar patterns by network, with clear influence of political events such as the 2009 UNCCC, 2015 Paris Agreement, and 2019 Democratic primary debates driving climate news coverage. This is reflected in the content of the climate mentions over time as words describing the political events occurring at the time tend to be the most important words for each of the networks in that specific year coupled with the tendency of different networks in the same year to have high content similarity. Topic analysis also finds that the majority of 15 topics found in topic analysis had significant changes in mean on some of the topics at the time of Donald Trump's inauguration. ")])])])])}],Ve={name:"ClimateChangeNews",methods:{scrollUp(){H()}}},Ye=Ve,Qe=(a("4781"),Object(u["a"])(Ye,Ge,Ke,!1,null,"506aa114",null)),Xe=Qe.exports,Ze=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"row align-items-center justify-content-center"},[e._m(0),a("div",{staticClass:"col-md-8"},[a("div",{staticClass:"carousel slide",attrs:{id:"boomerangCarousel","data-ride":"carousel","data-interval":"false"}},[a("ol",{staticClass:"carousel-indicators"},[a("li",{staticClass:"active",attrs:{"data-target":"#boomerangCarousel","data-slide-to":"0"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#boomerangCarousel","data-slide-to":"1"},on:{click:e.scrollUp}}),a("li",{attrs:{"data-target":"#boomerangCarousel","data-slide-to":"2"},on:{click:e.scrollUp}})]),e._m(1)])])])},et=[function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col-12",attrs:{id:"title"}},[a("h1",[e._v("Boomerang")])])},function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("div",{staticClass:"carousel-inner"},[i("div",{staticClass:"carousel-item active boomerang-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("5e93"),alt:"First slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Sign Up/Login Page")]),i("p",[e._v("When my group decided on how we wanted to split up the tasks for Boomerang, I decided to design & create the sign up/login page & the create account flow. I thought this would be a good section of the application to practice and enhance my visual design skills and to create a UI that was intuitive. ")])])]),i("div",{staticClass:"carousel-item boomerang-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("e1a7"),alt:"Second slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Create Account Part I")]),i("p",[e._v(" In the sections of Boomerang I built, I created the front-end using Vue.js which is the same front-end framework I used to build this website. The backend was built using Express.js. The fields above checked for user input to ensure that the account username was not already taken and that the each of the fields was in the correct format, notifying the user instantly on submission if their input was invalid. ")])])]),i("div",{staticClass:"carousel-item boomerang-carousel-item"},[i("img",{staticClass:"rounded img-fluid",attrs:{src:a("a21f"),alt:"Third slide"}}),i("div",{staticClass:"carousel-text"},[i("h5",[e._v("Create Account Part II")]),i("p",[e._v(" Account creation for any app is an important user flow as it lets the user understand both the purpose of an app & how they can engage with it completely. For these reasons, I decided to include descriptions of the main concepts of the application (Communities, Channels, etc.) as this would reduce the time it would take for a user to get immersed in the app. ")])])])])}],tt={name:"Boomerang",methods:{scrollUp(){H()}}},at=tt,it=(a("d490"),Object(u["a"])(at,Ze,et,!1,null,"84374a92",null)),st=it.exports,ot=function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("div",{staticClass:"col pt-5 justify-content-center"},[a("h2",[e._v("Uh Oh! Looks like you went to a page that doesn't exist on dyllew.github.io")]),a("router-link",{staticClass:"router-link",attrs:{to:"/"}},[e._v("Click this link to go home.")])],1)},rt=[],nt={name:"NotFoundComponent"},ct=nt,lt=(a("2db4"),Object(u["a"])(ct,ot,rt,!1,null,"4ffa6c61",null)),dt=lt.exports;i["default"].use(T["a"]);const ht=[{path:"/",component:S},{path:"/about",component:W},{path:"/projects",component:X},{path:"/projects/boomerang",component:st},{path:"/projects/ml-for-crowdsourced-crisis-data",component:ge},{path:"/projects/ml-for-crowdsourced-crisis-data/image-analysis-module",component:Ce},{path:"/projects/ml-for-crowdsourced-crisis-data/text-analysis-module",component:Ie},{path:"/projects/nlp-for-int-dev-gray-lit",component:De},{path:"/projects/trump-speech-analysis",component:Je},{path:"/projects/gnns-taxi-prediction",component:qe},{path:"/projects/climate-change-news",component:Xe},{path:"/artwork",component:se},{path:"/resume",component:de},{path:"/404",component:dt},{path:"*",redirect:"/404"}],ut=new T["a"]({mode:"hash",routes:ht});var mt=ut;a("f9e3"),a("2dd8");i["default"].config.productionTip=!1,i["default"].use(s["a"]),i["default"].use(o["a"]),new i["default"]({router:mt,render:e=>e(x)}).$mount("#app")},5984:function(e,t,a){e.exports=a.p+"img/iaa.ec440fef.png"},"5db7":function(e,t,a){},"5e93":function(e,t,a){e.exports=a.p+"img/boomerang-home.2b52b305.jpg"},"621b":function(e,t,a){e.exports=a.p+"img/test-set-eval.d9e2b629.png"},"64e1":function(e,t,a){e.exports=a.p+"img/network_tfidf_wordclouds.295ee901.png"},6837:function(e,t,a){},"6a64":function(e,t,a){e.exports=a.p+"img/in-fc.af1ca061.png"},"6e61":function(e,t,a){e.exports=a.p+"img/flood_confusion_matrix.7d011586.png"},"70f6":function(e,t,a){e.exports=a.p+"img/informativeness_confusion_matrix.e14fee3e.png"},7332:function(e,t,a){"use strict";var i=a("6837"),s=a.n(i);s.a},"746f":function(e,t,a){},"766d":function(e,t,a){},"7b9c":function(e,t,a){e.exports=a.p+"img/image-analysis-module-modified.2f3d5731.png"},"7d1a":function(e,t,a){e.exports=a.p+"img/NER.042f94f2.png"},8050:function(e,t,a){e.exports=a.p+"01fbdfc68fb18b120d93f81bebddbbe3.pdf"},"835a":function(e,t,a){e.exports=a.p+"img/workshop-preface-questions.fe9b1412.png"},"84e6":function(e,t,a){e.exports=a.p+"img/fare-surge-graph-pred.16940831.png"},"85ec":function(e,t,a){},"878d":function(e,t,a){e.exports=a.p+"img/years_cosine_similarity.1e4e7357.png"},"89e4":function(e,t,a){e.exports=a.p+"img/trump-campaign.81aaea0e.png"},"8b08":function(e,t,a){e.exports=a.p+"img/project-collage.6966ef19.png"},9271:function(e,t,a){e.exports=a.p+"img/tfidf-networks.c48a6bce.png"},9409:function(e,t,a){e.exports=a.p+"img/hc-fc.f248f6e2.png"},9469:function(e,t,a){e.exports=a.p+"img/damage_severity_per_class_metric.b0e58f3a.png"},"964e":function(e,t,a){e.exports=a.p+"img/ds-train.fff83a4a.png"},"9faf":function(e,t,a){e.exports=a.p+"img/pika-gif.6999dd5c.gif"},a21f:function(e,t,a){e.exports=a.p+"img/join-communities.392bb659.png"},a297:function(e,t,a){e.exports=a.p+"img/resume.851be6c2.png"},a2c3:function(e,t,a){e.exports=a.p+"img/reptile.b479d166.png"},ab31:function(e,t,a){e.exports=a.p+"img/masters-thesis-overview.b92e2d31.png"},aec1:function(e,t,a){e.exports=a.p+"img/damage_severity_confusion_matrix.baa9b21a.png"},af03:function(e,t,a){},afd1:function(e,t,a){e.exports=a.p+"img/network_cosine_similarity.0a83a38c.png"},b720:function(e,t,a){e.exports=a.p+"57c143e9704cb3120e7e05eaf2a3a1a6.pdf"},bb54:function(e,t,a){e.exports=a.p+"img/in-train.cce4b41a.png"},be19:function(e,t,a){e.exports=a.p+"img/hc-train.314e090e.png"},c207:function(e,t,a){e.exports=a.p+"img/feelings.7216f530.jpg"},c7dc:function(e,t,a){"use strict";var i=a("af03"),s=a.n(i);s.a},c9e4:function(e,t,a){e.exports=a.p+"img/dylan-n-leo.621aa4fc.jpg"},cad8:function(e,t,a){e.exports=a.p+"img/networks_and_years_cosine_similarity.3eaf2c24.png"},cbeb:function(e,t,a){e.exports=a.p+"img/linkedin-profpic.00cde042.jpg"},d472:function(e,t,a){"use strict";var i=a("e31f"),s=a.n(i);s.a},d490:function(e,t,a){"use strict";var i=a("766d"),s=a.n(i);s.a},d54d:function(e,t,a){e.exports=a.p+"img/humanitarian_categories_confusion_matrix.cd985937.png"},d67d:function(e,t,a){},d730:function(e,t,a){e.exports=a.p+"img/image-analysis-module.c12d2c01.png"},d8cb:function(e,t,a){e.exports=a.p+"img/final-project-overview.13f71d33.png"},e1a7:function(e,t,a){e.exports=a.p+"img/create-account.b25be796.png"},e31f:function(e,t,a){},e75b:function(e,t,a){e.exports=a.p+"img/taxi-proj-thumbnail.6dd3b85f.png"},ee29:function(e,t,a){e.exports=a.p+"img/leo_n_me.2868644e.jpg"},f061:function(e,t,a){e.exports=a.p+"img/RelativeWordFreqDiffFlorida.80584f3b.png"},f124:function(e,t,a){e.exports=a.p+"img/taxi-fare-and-surge-pred.064e07ad.png"},f1bb:function(e,t,a){},f2e7:function(e,t,a){},f3bd:function(e,t,a){"use strict";var i=a("f2e7"),s=a.n(i);s.a},ffa5:function(e,t,a){"use strict";var i=a("25d6"),s=a.n(i);s.a}});
//# sourceMappingURL=app.a7d688b9.js.map