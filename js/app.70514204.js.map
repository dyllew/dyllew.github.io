{"version":3,"file":"js/app.70514204.js","mappings":"4DAAA,aAAe,IAA0B,sC,2CCAzC,aAAe,IAA0B,sC,2CCAzC,aAAe,IAA0B,sC,2CCAzC,aAAe,IAA0B,sC,2CCAzC,aAAe,IAA0B,sC,2CCAzC,aAAe,IAA0B,sC,qECArCA,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iCAAiCC,MAAM,CAAC,GAAK,QAAQ,CAACH,EAAG,UAAiC,MAAtBD,KAAKK,OAAOC,MAAsC,SAAtBN,KAAKK,OAAOC,KAAiBL,EAAG,UAAUF,EAAIQ,KAAKN,EAAG,gBAAgB,EAC/P,EACIO,EAAkB,GCFlBV,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,2BAA2B,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,KAAK,CAACG,MAAM,CAAC,GAAK,4BAA4BK,GAAG,CAAC,MAAQV,EAAIW,SAAS,CAACX,EAAIY,GAAG,qBAAqBZ,EAAIa,GAAG,IACnT,EACIJ,EAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,mBAAmBC,MAAM,CAAC,GAAK,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,iEAAiE,CAACF,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gCAAgC,CAACH,EAAG,IAAI,CAACE,YAAY,2BAA2BF,EAAG,IAAI,CAACE,YAAY,6BAA6BF,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,sCAAsC,OAAS,WAAW,CAACH,EAAG,IAAI,CAACE,YAAY,gCAAgCF,EAAG,IAAI,CAACE,YAAY,kCAAkCF,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACH,EAAG,IAAI,CAACE,YAAY,uBAAuBF,EAAG,IAAI,CAACE,YAAY,4BAC5rB,GCsBA,G,QAAA,CAEAU,KAAAA,SACAC,QAAAA,CACAJ,SACA,sBACA,KC/BsP,I,UCQlPK,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAeA,EAAiB,QCnB5BjB,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,YAAY,CAACH,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,WAAW,CAACL,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,OAAOV,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,cAAc,CAACL,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,OAAOV,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,aAAa,CAACL,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,OAAOV,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,YAAY,CAACL,EAAIY,GAAG,aAAa,IAC1kB,EACIH,EAAkB,GCUtB,GACAK,KAAAA,UCbsP,ICQlP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCNhC,GACAA,KAAAA,MACAG,WAAAA,CAEAC,OAAAA,EACAC,OAAAA,IClB0O,ICQtO,GAAY,OACd,EACApB,EACAU,GACA,EACA,KACA,KACA,MAIF,EAAe,EAAiB,Q,UCnB5BV,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,wDAAwD,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,UAAU,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,aAAaM,GAAG,CAAC,MAAQV,EAAIoB,YAAY,CAAClB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAMgB,EAAQ,eAAgDnB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,WAAW,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,YAAYV,EAAG,MAAM,CAACE,YAAY,aAAaM,GAAG,CAAC,MAAQV,EAAIsB,aAAa,CAACpB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAMgB,EAAQ,eAA2CnB,EAAG,MAAM,CAACE,YAAY,WAAWC,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,aAAaM,GAAG,CAAC,MAAQV,EAAIuB,eAAe,CAACrB,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAMgB,EAAQ,aAAkDnB,EAAG,MAAM,CAACE,YAAY,sBAAsB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACE,YAAY,aAAaM,GAAG,CAAC,MAAQV,EAAIwB,cAAc,CAACtB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAMgB,EAAQ,eAA6CnB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,YAAY,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACE,YAAY,aAAaM,GAAG,CAAC,MAAQV,EAAIwB,cAAc,CAACtB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAMgB,EAAQ,eAA6CnB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,GAAK,aAAa,CAACH,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,aAAaM,GAAG,CAAC,MAAQV,EAAIuB,eAAe,CAACrB,EAAG,MAAM,CAACE,YAAY,8BAA8BC,MAAM,CAAC,IAAMgB,EAAQ,gBAC38D,EACIZ,EAAkB,GCqEtB,GAEAK,KAAAA,OACAW,OACA,OACAC,YAAAA,OAAAA,WAEA,EACAX,QAAAA,CACAK,YACA,2BACA,EACAG,eACA,8BACA,EACAC,cACA,6BACA,EACAF,aACA,4BACA,IC3FoP,ICQhP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5BvB,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,6DAA6DC,MAAM,CAAC,GAAK,mBAAmB,CAACL,EAAIa,GAAG,GAAGX,EAAG,MAAM,CAACE,YAAY,oCAAoCC,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,oBAAoB,CAACL,EAAIY,GAAG,2PAA2PV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8CK,GAAG,CAAC,MAAQV,EAAI2B,WAAW,CAAC3B,EAAIY,GAAG,WAAWZ,EAAIY,GAAG,6CAA6CZ,EAAIa,GAAG,GAAGX,EAAG,IAAI,CAACF,EAAIY,GAAG,0PACvwB,EACIH,EAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,4CAA4C,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,GAAK,oBAAoB,IAAMgB,EAAQ,MAAoC,IAAM,oBAC5Q,EAAE,WAAY,IAAIrB,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,IAAI,CAACF,EAAIY,GAAG,6HAA6HV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gCAAgC,CAACL,EAAIY,GAAG,YAAYZ,EAAIY,GAAG,aAAaV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,sCAAsC,OAAS,WAAW,CAACL,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,kBAAkBV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACL,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,wFAC/f,G,UCFO,MAAMgB,EAAgB,CACzB,CACIC,GAAI,kCACJC,KAAM,4CACNC,IAAK,CAACC,YAAa,+BACnBC,MAAO,2GACPC,KAAM,+LACNC,eAAgB,CACZN,GAAI,gBACJO,QAAS,sBACTC,IAAK,gDAGb,CACIR,GAAI,2BACJC,KAAM,qCACNC,IAAK,CAACC,YAAa,uBACnBC,MAAO,mIACPC,KAAM,iQACNC,eAAgB,CACZN,GAAI,gBACJO,QAAS,mBACTC,IAAK,kCAGb,CACIR,GAAI,sBACJC,KAAM,gCACNC,IAAK,CAACC,YAAa,8BACnBC,MAAO,4DACPC,KAAM,kNACNC,eAAgB,CACZN,GAAI,gBACJO,QAAS,aACTC,IAAK,+BAGb,CACIR,GAAI,gBACJC,KAAM,iCACNC,IAAK,CAACC,YAAa,6BACnBC,MAAO,oEACPC,KAAM,kJAEV,CACIL,GAAI,YACJC,KAAM,kCACNC,IAAK,CAACC,YAAa,oBAAqBM,UAAW,QACnDL,MAAO,iCACPC,KAAM,mJACNC,eAAgB,CACZN,GAAI,gBACJO,QAAS,aACTC,IAAK,+BAGb,CACIR,GAAI,gBACJC,KAAM,sBACNC,IAAK,CAACC,YAAa,sBACnBC,MAAO,YACPC,KAAM,wIACNC,eAAgB,CACZN,GAAI,gBACJO,QAAS,8BACTC,IAAK,sDAKJE,EAAa,CACtB,CACIV,GAAI,mBACJC,KAAM,kEACNC,IAAK,CAACC,YAAa,sCACnBC,MAAO,wBACPC,KAAM,+XACNM,eAAgB,qBAChBL,eAAgB,CACZN,GAAI,gBACJO,QAAS,mBACTC,IAAK,gIAGb,CACIR,GAAI,kBACJC,KAAM,iEACNC,IAAK,CAACC,YAAa,4BACnBC,MAAO,uBACPC,KAAM,sTACNM,eAAgB,qBAChBL,eAAgB,CACZN,GAAI,gBACJO,QAAS,mBACTC,IAAK,gIAKV,SAASI,EAAaC,EAAW,UACpCC,OAAOC,SAAS,CACZC,IAAK,EACLC,KAAM,EACNJ,SAAUA,GAElB,CAEO,SAASK,EAAsBC,GAClC,MAAMC,EAAeC,SAASC,eAAeH,GACvCI,EAAW,IAAIC,EAAAA,GAAU,IAAGL,KAClCC,EAAaK,iBAAiB,oBAAoB,KAC9Cb,GAAc,IAElBQ,EAAaK,iBAAiB,cAAc,SAASC,GACjD,MAAMC,EAASD,EAAME,QAAQ,GAAGC,MAC1BC,EAAmBC,EAAqBR,EAAUI,GACxDP,EAAaK,iBAAiB,YAAaK,EAAkB,CAACE,MAAM,IACpEZ,EAAaK,iBAAiB,YAAY,WACtCL,EAAaa,oBAAoB,YAAaH,EAAkB,CAACE,MAAM,GAC3E,GACF,GACN,CAEA,SAASD,EAAqBR,EAAUI,GACpC,MAAMG,EAAmBJ,IACrB,MAAMQ,EAAQR,EAAME,QAAQ,GAAGC,MACzBM,EAAkB,GACnBC,KAAKC,MAAMV,EAASO,GAASC,EAC9BZ,EAASe,OAEHF,KAAKC,MAAMV,EAASO,IAAUC,GACpCZ,EAASgB,MACb,EAEJ,OAAOT,CACX,CCzGA,OAEA7C,KAAAA,QACAC,QAAAA,CACAY,WACAc,GACA,ICtCqP,ICQjP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5B1C,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiDJ,EAAIqE,GAAIrE,EAAIsE,UAAU,SAASC,GAAS,OAAOrE,EAAG,MAAM,CAACsE,IAAID,EAAQzC,KAAK1B,YAAY,iCAAiC,CAACF,EAAG,cAAc,CAACG,MAAM,CAAC,QAAUkE,MAAY,EAAE,IAAG,EACtT,EACI9D,EAAkB,GCFlBV,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,KAAK,CAACE,YAAY,eAAe,CAACJ,EAAIY,GAAGZ,EAAIyE,GAAGzE,EAAIuE,QAAQtC,UAAU/B,EAAG,MAAM,CAACE,YAAY,eAAesE,MAAM1E,EAAIuE,QAAQxC,IAAIO,UAAYtC,EAAIuE,QAAQxC,IAAIO,UAAY,GAAGjC,MAAM,CAAC,GAAKL,EAAIuE,QAAQ1C,GAAG,IAAM7B,EAAI2E,UAAU3E,EAAIuE,QAAQxC,IAAIC,aAAa,IAAM,mBAAmB9B,EAAG,MAAM,CAACE,YAAY,aAAa,CAACF,EAAG,IAAI,CAACE,YAAY,aAAa,CAACJ,EAAIY,GAAGZ,EAAIyE,GAAGzE,EAAIuE,QAAQrC,SAAUlC,EAAIuE,QAAQpC,eAAgBjC,EAAG,MAAM,CAACG,MAAM,CAAC,GAAKL,EAAIuE,QAAQpC,eAAeN,KAAK,CAAC3B,EAAG,IAAI,CAACE,YAAY,sBAAsBC,MAAM,CAAC,KAAOL,EAAIuE,QAAQpC,eAAeE,IAAI,OAAS,WAAW,CAACrC,EAAIY,GAAGZ,EAAIyE,GAAGzE,EAAIuE,QAAQpC,eAAeC,YAAYlC,EAAG,QAAQA,EAAG,IAAI,CAACE,YAAY,mCAAmCM,GAAG,CAAC,MAAQ,SAASkE,GAAQ,OAAO5E,EAAI6E,gBAAgB7E,EAAIuE,QAAQzC,KAAK,IAAI,CAAC9B,EAAIY,GAAGZ,EAAIyE,GAAGzE,EAAIwC,qBAAqBtC,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,IAAI,CAACE,YAAY,6BAA6BM,GAAG,CAAC,MAAQ,SAASkE,GAAQ,OAAO5E,EAAI6E,gBAAgB7E,EAAIuE,QAAQzC,KAAK,IAAI,CAAC9B,EAAIY,GAAG,8BACxmC,EACIH,EAAkB,GCsBtB,GACAK,KAAAA,cACAgE,MAAAA,CAAAA,WACA/D,QAAAA,CACA8D,gBAAAA,GACA,oBACA,EACAF,UAAAA,GACA,sBACA,GAEAI,SAAAA,CACAvC,iBACA,oFACA,ICtC2P,ICQvP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCPhC,GAEA1B,KAAAA,WACAG,WAAAA,CACA+D,YAAAA,GAEAvD,OACA,OACA6C,SAAAA,EAEA,GCtBwP,ICOpP,IAAY,OACd,EACA,EACA,GACA,EACA,KACA,KACA,MAIF,GAAe,GAAiB,QClB5BvE,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,mFAAmF,CAACF,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,UAAU,CAACJ,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAMgB,EAAQ,WAAyCnB,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,UAAU,CAACJ,EAAIY,GAAG,oBAAoBV,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,IAAMgB,EAAQ,YACvjB,GCyBA,IAEAP,KAAAA,WC9BuP,MCQnP,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,KACA,MAIF,GAAe,GAAiB,QCnB5Bf,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,wDAAwD,CAACF,EAAG,MAAM,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,eAAe,CAACH,EAAG,IAAI,CAACE,YAAY,6BAA6BC,MAAM,CAAC,OAAS,SAAS,KAAO,oCAAoC,CAACL,EAAIY,GAAG,uBAAuBV,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,QAAQ,CAACE,YAAY,MAAMC,MAAM,CAAC,IAAM,wCAC3b,GCUA,IAEAS,KAAAA,UCfsP,MCQlP,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5Bf,GAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACJ,EAAIa,GAAG,GAAGX,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,wCAAwC,CAACL,EAAIa,GAAG,GAAGX,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACJ,EAAIa,GAAG,GAAGb,EAAIa,GAAG,GAAGb,EAAIa,GAAG,GAAGX,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACJ,EAAIa,GAAG,GAAGb,EAAIqE,GAAIrE,EAAIiF,SAAS,SAASC,GAAQ,OAAOhF,EAAG,MAAM,CAACsE,IAAIU,EAAOpD,KAAK1B,YAAY,iCAAiC,CAACF,EAAG,cAAc,CAACG,MAAM,CAAC,QAAU6E,MAAW,EAAE,IAAGlF,EAAIa,GAAG,IAAI,KAAKb,EAAIa,GAAG,GAAGb,EAAIa,GAAG,GAAGb,EAAIa,GAAG,GAAGb,EAAIa,GAAG,IAAIb,EAAIa,GAAG,IAAIb,EAAIa,GAAG,WACxuB,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+GACjI,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uCAAuC,mBAAmB,QACjkC,EAAE,WAAY,IAAIL,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,GAAK,eAAe,IAAMgB,EAAQ,MAAyD,IAAM,mBAAmBnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oEAAoEV,EAAG,IAAI,CAACF,EAAIY,GAAG,sxBAAsxBV,EAAG,MAAMA,EAAG,MAAMF,EAAIY,GAAG,gDAAgDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8C,OAAS,WAAW,CAACL,EAAIY,GAAG,WAAWV,EAAG,MAAMA,EAAG,MAAMF,EAAIY,GAAG,2GAA2GV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0FAA0F,OAAS,WAAW,CAACL,EAAIY,GAAG,6BAA6BV,EAAG,MAAMF,EAAIY,GAAG,qQAAqQV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oDAAoD,OAAS,WAAW,CAACL,EAAIY,GAAG,sBAAsBZ,EAAIY,GAAG,qMAAqMV,EAAG,KAAK,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,kDAAkD,OAAS,WAAW,CAACL,EAAIY,GAAG,qBAAqBZ,EAAIY,GAAG,sLAAsLV,EAAG,MAAMF,EAAIY,GAAG,6FAC5sF,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,gBAAgBV,EAAG,IAAI,CAACF,EAAIY,GAAG,uOAAuOV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,qCAAqC,OAAS,WAAW,CAACL,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,SAASV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,+BAA+B,OAAS,WAAW,CAACL,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,sfAAsfV,EAAG,IAAI,CAACF,EAAIY,GAAG,w0CAAw0CV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,SAAS,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACL,EAAIY,GAAG,qQACxpF,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,oDAAoDZ,EAAIY,GAAG,0IAA0IV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,oGAAoGZ,EAAIY,GAAG,wCAAwCV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,mEAAmEZ,EAAIY,GAAG,mDAAmDV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,yEAAyEZ,EAAIY,GAAG,0FAA0FZ,EAAIY,GAAG,wSAAwSV,EAAG,KAAK,CAACF,EAAIY,GAAG,qBAAqBV,EAAG,IAAI,CAACF,EAAIY,GAAG,wIAAwIV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,oCAAoCZ,EAAIY,GAAG,sIAAsIV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,yKAAyKV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,0DAA0DZ,EAAIY,GAAG,qQAAqQV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,wEAAwEZ,EAAIY,GAAG,2HAA2HV,EAAG,SAAS,CAACF,EAAIY,GAAG,2EAA2EZ,EAAIY,GAAG,iUAAiUV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,mEAAmEZ,EAAIY,GAAG,oZAAoZZ,EAAIY,GAAG,6BAA6BV,EAAG,IAAI,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,iIAAiIV,EAAG,IAAI,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,6DAC9wH,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,kCAAkCZ,EAAIY,GAAG,iHAAiHV,EAAG,MAAMF,EAAIY,GAAG,iDAAiDV,EAAG,MAAMA,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,kRAAkRV,EAAG,IAAI,CAACF,EAAIY,GAAG,oIACzoB,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,4JAChH,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,KAAK,CAACF,EAAIY,GAAG,qFAAqFV,EAAG,IAAI,CAACF,EAAIY,GAAG,4GAA4GV,EAAG,SAAS,CAACF,EAAIY,GAAG,6HAA6HZ,EAAIY,GAAG,0MAA0MV,EAAG,KAAK,CAACF,EAAIY,GAAG,+CAA+CV,EAAG,IAAI,CAACF,EAAIY,GAAG,0GAA0GV,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,6DAA6DZ,EAAIY,GAAG,6DAA6DV,EAAG,IAAI,CAACF,EAAIY,GAAG,4CAA4CV,EAAG,KAAK,CAACF,EAAIY,GAAG,mFAAmFV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,6LAA6LZ,EAAIY,GAAG,2ZAC/iD,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,kCAAkCV,EAAG,IAAI,CAACF,EAAIY,GAAG,wPAAwPV,EAAG,SAAS,CAACF,EAAIY,GAAG,4SAA4SZ,EAAIY,GAAG,kCAAkCV,EAAG,SAAS,CAACF,EAAIY,GAAG,sDAAsDZ,EAAIY,GAAG,uBAAuBV,EAAG,SAAS,CAACF,EAAIY,GAAG,8GAA8GV,EAAG,IAAI,CAACF,EAAIY,GAAG,0UAA0UV,EAAG,SAAS,CAACF,EAAIY,GAAG,oQAAoQV,EAAG,IAAI,CAACF,EAAIY,GAAG,2GAC9uD,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,KAAK,CAACF,EAAIY,GAAG,sCAAsCV,EAAG,IAAI,CAACF,EAAIY,GAAG,yHAAyHV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oDAAoD,OAAS,WAAW,CAACL,EAAIY,GAAG,2BAA2BZ,EAAIY,GAAG,2BAA2BV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,kDAAkD,OAAS,WAAW,CAACL,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,ocAAocV,EAAG,IAAI,CAACF,EAAIY,GAAG,kDAAkDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0FAA0F,OAAS,WAAW,CAACL,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,mMAAmMV,EAAG,KAAK,CAACF,EAAIY,GAAG,oFAAoFV,EAAG,IAAI,CAACF,EAAIY,GAAG,ouBAAouBV,EAAG,KAAK,CAACF,EAAIY,GAAG,wEAAwEV,EAAG,IAAI,CAACF,EAAIY,GAAG,4gBAA4gBV,EAAG,IAAI,CAACF,EAAIY,GAAG,siBAC/jG,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,KAAK,CAACF,EAAIY,GAAG,wEAAwEV,EAAG,IAAI,CAACF,EAAIY,GAAG,8uBAA8uBV,EAAG,KAAK,CAACF,EAAIY,GAAG,sFAAsFV,EAAG,IAAI,CAACF,EAAIY,GAAG,qoBAAqoBV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACF,EAAIY,GAAG,+DAA+DV,EAAG,KAAK,CAACF,EAAIY,GAAG,iHAAiHZ,EAAIY,GAAG,6LAA6LV,EAAG,KAAK,CAACF,EAAIY,GAAG,mEAAmEV,EAAG,IAAI,CAACF,EAAIY,GAAG,kxBAAkxBV,EAAG,KAAK,CAACF,EAAIY,GAAG,8EAA8EV,EAAG,IAAI,CAACF,EAAIY,GAAG,muBAAmuBV,EAAG,IAAI,CAACF,EAAIY,GAAG,yqBACv/H,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,KAAK,CAACF,EAAIY,GAAG,8EAA8EV,EAAG,IAAI,CAACF,EAAIY,GAAG,iRAAiRV,EAAG,SAAS,CAACF,EAAIY,GAAG,mTAAmTZ,EAAIY,GAAG,+PAA+PV,EAAG,IAAI,CAACF,EAAIY,GAAG,kpCACnuC,EAAE,WAAY,IAAIZ,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,MAAM,CAACE,YAAY,oBAAoBC,MAAM,CAAC,GAAK,eAAe,IAAMgB,EAAQ,MAAmD,IAAM,iBAAiBnB,EAAG,IAAI,CAACF,EAAIY,GAAG,gNAAgNV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,iEAAiEV,EAAG,IAAI,CAACF,EAAIY,GAAG,qcAAqcV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,IAAI,CAACF,EAAIY,GAAG,sPAAsPV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,4GAA4GV,EAAG,IAAI,CAACF,EAAIY,GAAG,+UAA+UV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,2GAA2GV,EAAG,IAAI,CAACF,EAAIY,GAAG,maACxqE,GCkZA,IACAE,KAAAA,8BACAG,WAAAA,CACA+D,YAAAA,GAEAvD,OACA,OACAwD,QAAAA,EAEA,EACAE,UACA1C,IACAM,EAAAA,sCACA,GC9a6R,MCQzR,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,SAAS,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,6BAA6BV,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,0BAA0B,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,yBAAyB,mBAAmB,UAAUH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAMgB,EAAQ,MAAuD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oCAAoCV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,IAAI,CAACF,EAAIY,GAAG,wQAAwQV,EAAG,SAAS,CAACF,EAAIY,GAAG,8EAA8EZ,EAAIY,GAAG,UAAUV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,+FAA+FV,EAAG,SAAS,CAACF,EAAIY,GAAG,qMAAqMZ,EAAIY,GAAG,yLAAyLV,EAAG,KAAK,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,IAAI,CAACF,EAAIY,GAAG,+UAA+UV,EAAG,SAAS,CAACF,EAAIY,GAAG,kGAAkGZ,EAAIY,GAAG,sHAAsHV,EAAG,SAAS,CAACF,EAAIY,GAAG,oKAAoKZ,EAAIY,GAAG,MAAMV,EAAG,SAAS,CAACF,EAAIY,GAAG,qEAAqEZ,EAAIY,GAAG,qBAAqBV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,oBAAoBV,EAAG,SAAS,CAACF,EAAIY,GAAG,+DAA+DZ,EAAIY,GAAG,qHAAqHV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,wCAAwCZ,EAAIY,GAAG,qJAAqJV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,KAAK,CAACF,EAAIY,GAAG,iGAAiGV,EAAG,IAAI,CAACF,EAAIY,GAAG,+ZAA+ZV,EAAG,SAAS,CAACF,EAAIY,GAAG,+DAA+DZ,EAAIY,GAAG,oFAAoFV,EAAG,KAAK,CAACF,EAAIY,GAAG,kGAAkGV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,sFAAsFZ,EAAIY,GAAG,0FAA0FV,EAAG,SAAS,CAACF,EAAIY,GAAG,wMAAwMV,EAAG,IAAI,CAACF,EAAIY,GAAG,mQAAmQV,EAAG,IAAI,CAACF,EAAIY,GAAG,gGAAgGV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,yDAAyDV,EAAG,SAAS,CAACF,EAAIY,GAAG,kLAAkLZ,EAAIY,GAAG,uNAAuNV,EAAG,IAAI,CAACF,EAAIY,GAAG,yQAAyQV,EAAG,KAAK,CAACF,EAAIY,GAAG,uHAAuHV,EAAG,IAAI,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,SAAS,CAACF,EAAIY,GAAG,0IAA0IZ,EAAIY,GAAG,wcAAwcV,EAAG,IAAI,CAACF,EAAIY,GAAG,uDAAuDV,EAAG,SAAS,CAACF,EAAIY,GAAG,4EAA4EZ,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,oBAAoBV,EAAG,SAAS,CAACF,EAAIY,GAAG,8FAAkGZ,EAAIY,GAAG,qCAAqCV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,qGAAqGV,EAAG,SAAS,CAACF,EAAIY,GAAG,sHAAsHV,EAAG,KAAK,CAACF,EAAIY,GAAG,kEAAkEV,EAAG,IAAI,CAACF,EAAIY,GAAG,oMAAoMV,EAAG,SAAS,CAACF,EAAIY,GAAG,4IAA4IZ,EAAIY,GAAG,kPAAoPV,EAAG,SAAS,CAACF,EAAIY,GAAG,gHAAgHZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,mHAAmHV,EAAG,SAAS,CAACF,EAAIY,GAAG,8CAA8CZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,mHAAmHZ,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,0NAA0NZ,EAAIY,GAAG,wIAAwIV,EAAG,SAAS,CAACF,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,wOAAwOV,EAAG,IAAI,CAACF,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,6DAA6DZ,EAAIY,GAAG,uDAAuDV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,wTAAwTV,EAAG,IAAI,CAACF,EAAIY,GAAG,+CAA+CV,EAAG,SAAS,CAACF,EAAIY,GAAG,uNAAuNZ,EAAIY,GAAG,sMAAsMV,EAAG,WAAWA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+EAA+EV,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,gDAAgDV,EAAG,SAAS,CAACF,EAAIY,GAAG,yDAAyDZ,EAAIY,GAAG,+BAA+BV,EAAG,SAAS,CAACF,EAAIY,GAAG,qDAAqDZ,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,gBAAgBZ,EAAIY,GAAG,4JAA4JV,EAAG,KAAK,CAACF,EAAIY,GAAG,oDAAoDV,EAAG,IAAI,CAACF,EAAIY,GAAG,0GAA0GV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,4DAA4D,OAAS,WAAW,CAACL,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,qJAAqJV,EAAG,KAAK,CAACF,EAAIY,GAAG,wDAAwDV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,kDAAkDZ,EAAIY,GAAG,2GAA2GV,EAAG,SAAS,CAACF,EAAIY,GAAG,2CAA+CZ,EAAIY,GAAG,kCAAkCV,EAAG,SAAS,CAACF,EAAIY,GAAG,kEAAkEZ,EAAIY,GAAG,sIAAsIV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,KAAK,CAACF,EAAIY,GAAG,2HAA2HV,EAAG,QAAQ,CAACG,MAAM,CAAC,GAAK,aAAa,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,aAAaV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCV,EAAG,MAAMF,EAAIY,GAAG,MAAMV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,sBAAsBZ,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,WAAWV,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,kCAAkCV,EAAG,MAAMF,EAAIY,GAAG,MAAMV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,sBAAsBZ,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,YAAYV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BV,EAAG,MAAMF,EAAIY,GAAG,MAAMV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,sBAAsBZ,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,WAAWV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BV,EAAG,MAAMF,EAAIY,GAAG,MAAMV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,sBAAsBZ,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,YAAYV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,WAAWV,EAAG,KAAK,CAACF,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,WAAWV,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,UAA+CnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA+CnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,UAA+CnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA+CnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,gJAAgJV,EAAG,SAAS,CAACF,EAAIY,GAAG,8EAA8EZ,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,oCAAoCZ,EAAIY,GAAG,iCAAiCV,EAAG,SAAS,CAACF,EAAIY,GAAG,oFAAoFZ,EAAIY,GAAG,+BAA+BV,EAAG,SAAS,CAACF,EAAIY,GAAG,wDAA4DZ,EAAIY,GAAG,UAAUV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAA2CZ,EAAIY,GAAG,wBAAwBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8EAA8EV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACE,YAAY,QAAQ,CAACJ,EAAIY,GAAG,oJAAoJV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,WAAWV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,gBAAgB,IAAMgB,EAAQ,WAAoDnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,kCAAkCV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,6CAA6CV,EAAG,SAAS,CAACF,EAAIY,GAAG,wEAAwEZ,EAAIY,GAAG,+MAA+MV,EAAG,SAAS,CAACF,EAAIY,GAAG,8FAA8FZ,EAAIY,GAAG,uYAAuYV,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,0EAA0EV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,uGAAuGZ,EAAIY,GAAG,kHAAkHV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,0CAA0CV,EAAG,IAAI,CAACF,EAAIY,GAAG,0HAA0HV,EAAG,SAAS,CAACF,EAAIY,GAAG,qEAAqEZ,EAAIY,GAAG,UAAUV,EAAG,SAAS,CAACF,EAAIY,GAAG,8BAA8BZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,kCAAkCZ,EAAIY,GAAG,+HAA+HV,EAAG,SAAS,CAACF,EAAIY,GAAG,6EAA6EZ,EAAIY,GAAG,+FAA+FV,EAAG,SAAS,CAACF,EAAIY,GAAG,wDAAwDZ,EAAIY,GAAG,wEAAwEV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,wEAAwEV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,4GAA4GV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,uKAAuK,OAAS,WAAW,CAACL,EAAIY,GAAG,gBAAgBZ,EAAIY,GAAG,iCAAiCV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,QAAQV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,OAAOV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,sFAAsFV,EAAG,IAAI,CAACF,EAAIY,GAAG,kFAAkFV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,gCAAgCV,EAAG,SAAS,CAACF,EAAIY,GAAG,wBAAwBZ,EAAIY,GAAG,6DAA6DV,EAAG,SAAS,CAACF,EAAIY,GAAG,wDAAwDZ,EAAIY,GAAG,qKAAqKV,EAAG,SAAS,CAACF,EAAIY,GAAG,sDAAsDZ,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wEAAwEV,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iJAAkJV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,SAAS,CAACF,EAAIY,GAAG,qBAAqBZ,EAAIY,GAAG,sBAAsBV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,8CAA8CV,EAAG,SAAS,CAACF,EAAIY,GAAG,2CAA2CZ,EAAIY,GAAG,mCAAmCV,EAAG,SAAS,CAACF,EAAIY,GAAG,qCAAqCZ,EAAIY,GAAG,MAAMV,EAAG,MAAMA,EAAG,MAAMF,EAAIY,GAAG,+DAA+DV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,+CAA+CV,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,0CAA0CV,EAAG,KAAK,CAACF,EAAIY,GAAG,+DAA+DV,EAAG,KAAK,CAACF,EAAIY,GAAG,0DAA0DZ,EAAIY,GAAG,kDAAkDV,EAAG,SAAS,CAACF,EAAIY,GAAG,kCAAkCZ,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,wJAAwJV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAMA,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,kFAAkFV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,2GAA2GZ,EAAIY,GAAG,qCAAqCV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,0EAA0EV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+DAA+DV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA0CnB,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,YAAYH,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,gIAAgIV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BZ,EAAIY,GAAG,8MAAgNV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,oNAAoNV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,oIAAoIZ,EAAIY,GAAG,+GAA+GV,EAAG,SAAS,CAACF,EAAIY,GAAG,8DAA8DZ,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,eAAeV,EAAG,SAAS,CAACF,EAAIY,GAAG,+EAA+EZ,EAAIY,GAAG,wGAAwGV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,2CAA2CV,EAAG,MAAM,CAACE,YAAY,kDAAkD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA4CnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,UAA4CnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA4CnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,eAAgDnB,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,gIAAgIV,EAAG,SAAS,CAACF,EAAIY,GAAG,iDAAiDZ,EAAIY,GAAG,0DAA0DV,EAAG,SAAS,CAACF,EAAIY,GAAG,4DAA4DZ,EAAIY,GAAG,sEAAsEV,EAAG,SAAS,CAACF,EAAIY,GAAG,2FAA2FZ,EAAIY,GAAG,4CAA4CV,EAAG,SAAS,CAACF,EAAIY,GAAG,sBAAsBZ,EAAIY,GAAG,iBAAiBV,EAAG,SAAS,CAACF,EAAIY,GAAG,qDAAqDZ,EAAIY,GAAG,eAAeV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAAuEnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,UAAuEnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,sdAA4eV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA+EnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA+EnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,gEAAgEV,EAAG,SAAS,CAACF,EAAIY,GAAG,+FAA+FZ,EAAIY,GAAG,8BAA8BV,EAAG,SAAS,CAACF,EAAIY,GAAG,uFAAyFZ,EAAIY,GAAG,iFAAiFV,EAAG,SAAS,CAACF,EAAIY,GAAG,8FAA8FZ,EAAIY,GAAG,gUAAkUV,EAAG,SAAS,CAACF,EAAIY,GAAG,qKAAqKZ,EAAIY,GAAG,iFAAiFV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,MAAM,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAAuEnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAAuEnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,oOAA0OV,EAAG,SAAS,CAACF,EAAIY,GAAG,iEAAiEV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,6JAA6JZ,EAAIY,GAAG,kCAAkCV,EAAG,SAAS,CAACF,EAAIY,GAAG,oLAAsLZ,EAAIY,GAAG,6TAAqUV,EAAG,SAAS,CAACF,EAAIY,GAAG,sJAA0JV,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,0EAA0EV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,uGAAuGZ,EAAIY,GAAG,kHAAkHV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,mBAAmBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,sBAAsBV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAA6DnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,WAAsEnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,mHAAmHV,EAAG,SAAS,CAACF,EAAIY,GAAG,iHAAiHZ,EAAIY,GAAG,6FAA6FV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,gEAAgEV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oFAAoFV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,cAAc,IAAMgB,EAAQ,WAAqDnB,EAAG,MAAM,CAACE,YAAY,WAAWF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACF,EAAIY,GAAG,6CAA6CV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,gCAAgCV,EAAG,SAAS,CAACF,EAAIY,GAAG,kDAAkDZ,EAAIY,GAAG,+GAA+GV,EAAG,SAAS,CAACF,EAAIY,GAAG,0FAA0FZ,EAAIY,GAAG,wBAAwBV,EAAG,IAAI,CAACF,EAAIY,GAAG,gdAAgdV,EAAG,SAAS,CAACF,EAAIY,GAAG,kIAAkIZ,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uDAAuDV,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,qCAAqCV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,SAAS,CAACF,EAAIY,GAAG,2BAA2BZ,EAAIY,GAAG,2DAA2DV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,uNAA6NV,EAAG,KAAK,CAACF,EAAIY,GAAG,0NAA0NV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,0BAA0BV,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,sCAAsCV,EAAG,SAAS,CAACF,EAAIY,GAAG,gBAAgBZ,EAAIY,GAAG,oDAAoDV,EAAG,SAAS,CAACF,EAAIY,GAAG,oBAAoBV,EAAG,KAAK,CAACF,EAAIY,GAAG,8CAA8CV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,sCAAsCV,EAAG,SAAS,CAACF,EAAIY,GAAG,oCAAoCZ,EAAIY,GAAG,sFAAsFV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,8BAA8BZ,EAAIY,GAAG,qFAAqFV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BZ,EAAIY,GAAG,gCAAgCV,EAAG,SAAS,CAACF,EAAIY,GAAG,iBAAiBZ,EAAIY,GAAG,0BAA0BV,EAAG,SAAS,CAACF,EAAIY,GAAG,qBAAqBZ,EAAIY,GAAG,2BAA2BV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BZ,EAAIY,GAAG,oBAAoBV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,gDAAgDV,EAAG,KAAK,CAACF,EAAIY,GAAG,QAAQV,EAAG,SAAS,CAACF,EAAIY,GAAG,2FAA2FZ,EAAIY,GAAG,uJAAuJV,EAAG,KAAK,CAACF,EAAIY,GAAG,uGAAuGV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,0HAA0HV,EAAG,SAAS,CAACF,EAAIY,GAAG,gEAAgEZ,EAAIY,GAAG,OAAOV,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,iBAAiBZ,EAAIY,GAAG,2DAA2DV,EAAG,SAAS,CAACF,EAAIY,GAAG,uEAAuEV,EAAG,KAAK,CAACF,EAAIY,GAAG,4EAA4EV,EAAG,SAAS,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,KAAK,CAACF,EAAIY,GAAG,6IAA6IV,EAAG,SAAS,CAACF,EAAIY,GAAG,iHAAiHZ,EAAIY,GAAG,OAAOV,EAAG,KAAK,CAACF,EAAIY,GAAG,8GAA8GV,EAAG,SAAS,CAACF,EAAIY,GAAG,wBAAwBZ,EAAIY,GAAG,mCAAmCV,EAAG,SAAS,CAACF,EAAIY,GAAG,mCAAmCZ,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,2DAA2D,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wEAAwEV,EAAG,IAAI,CAACF,EAAIY,GAAG,6JAA6JV,EAAG,SAAS,CAACF,EAAIY,GAAG,kIAAkIZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,0DAA0DV,EAAG,SAAS,CAACF,EAAIY,GAAG,0EAA0EZ,EAAIY,GAAG,4CAA4CV,EAAG,SAAS,CAACF,EAAIY,GAAG,+BAA+BZ,EAAIY,GAAG,wJAAwJV,EAAG,SAAS,CAACF,EAAIY,GAAG,yEAAyEZ,EAAIY,GAAG,UAAUV,EAAG,KAAK,CAACF,EAAIY,GAAG,mEAAmEV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,wGAAwGV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,kEAAkEZ,EAAIY,GAAG,iGAAiGV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,uBAAuBZ,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,0DAA0DV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,uBAAuBZ,EAAIY,GAAG,wDAAwDV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,gDAAgDV,EAAG,SAAS,CAACF,EAAIY,GAAG,YAAYZ,EAAIY,GAAG,kBAAkBV,EAAG,IAAI,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,sFAAsFV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,2DAA2DV,EAAG,SAAS,CAACF,EAAIY,GAAG,qCAAqCZ,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,6BAA6BV,EAAG,IAAI,CAACF,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,8EAA8EZ,EAAIY,GAAG,OAAOV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,4BAA4BV,EAAG,SAAS,CAACF,EAAIY,GAAG,6CAA6CZ,EAAIY,GAAG,6BAA6BV,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,qmBAAqmBV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,SAASV,EAAG,MAAMA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,QAAQV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,qKAAqKV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,IAAI,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,eAAeV,EAAG,SAAS,CAACF,EAAIY,GAAG,+BAA+BZ,EAAIY,GAAG,+GAA+GV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,uDAAuDZ,EAAIY,GAAG,wDAAwDV,EAAG,SAAS,CAACF,EAAIY,GAAG,uBAAuBZ,EAAIY,GAAG,sBAAsBV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,+EAA+EV,EAAG,IAAI,CAACF,EAAIY,GAAG,8aAA8aV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,IAAMgB,EAAQ,cAAqEnB,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,IAAI,CAACF,EAAIY,GAAG,6yBAA6yBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,uDAAuDV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,qCAAqCV,EAAG,SAAS,CAACF,EAAIY,GAAG,oBAAoBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,KAAK,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,KAAK,CAACF,EAAIY,GAAG,6DAA6DV,EAAG,KAAK,CAACF,EAAIY,GAAG,8EAA8EV,EAAG,SAAS,CAACF,EAAIY,GAAG,oBAAoBZ,EAAIY,GAAG,yFAAyFV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,4CAA4CV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,8BAA8BZ,EAAIY,GAAG,gCAAgCV,EAAG,SAAS,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,kCAAkCZ,EAAIY,GAAG,iIAAiIV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,2EAA2EV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,mBAAmBV,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,iBAAiBV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,8CAA8CV,EAAG,KAAK,CAACF,EAAIY,GAAG,gDAAgDV,EAAG,KAAK,CAACF,EAAIY,GAAG,wGAAwGV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,iEAAiEV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,yEAAyEV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,iFAAiFV,EAAG,KAAK,CAACF,EAAIY,GAAG,+EAA+EV,EAAG,KAAK,CAACF,EAAIY,GAAG,4EAA4EV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,sEAAsEV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,iDAAiDZ,EAAIY,GAAG,+DAA+DV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,6HAA6HV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oEAAoEV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,6JAA6JV,EAAG,SAAS,CAACF,EAAIY,GAAG,8FAA8FZ,EAAIY,GAAG,QAAQV,EAAG,SAAS,CAACF,EAAIY,GAAG,qCAAqCZ,EAAIY,GAAG,+FAA+FV,EAAG,KAAK,CAACF,EAAIY,GAAG,2HAA6HV,EAAG,SAAS,CAACF,EAAIY,GAAG,6RAA6RV,EAAG,KAAK,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,iHAAiHV,EAAG,SAAS,CAACF,EAAIY,GAAG,+FAA+FZ,EAAIY,GAAG,kFAAkFV,EAAG,SAAS,CAACF,EAAIY,GAAG,iFAAiFZ,EAAIY,GAAG,6HAA6HV,EAAG,IAAI,CAACF,EAAIY,GAAG,wNAAwNV,EAAG,SAAS,CAACF,EAAIY,GAAG,uVAAuVZ,EAAIY,GAAG,uMAAuMV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oEAAoE,CAACL,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,qBACz4nD,GC2yBA,IACAE,KAAAA,wBACAqE,UACA1C,IACAM,EAAAA,wBACA,GCnzBuR,MCQnR,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4BAA4BV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,yBAAyB,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,QAAQH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,UAAUH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,MAAsD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,IAAI,CAACF,EAAIY,GAAG,koBAAkoBV,EAAG,KAAK,CAACF,EAAIY,GAAG,gGAAgGV,EAAG,IAAI,CAACF,EAAIY,GAAG,kLAAkLV,EAAG,SAAS,CAACF,EAAIY,GAAG,sGAAsGZ,EAAIY,GAAG,eAAeV,EAAG,SAAS,CAACF,EAAIY,GAAG,qGAAqGZ,EAAIY,GAAG,0GAA0GV,EAAG,KAAK,CAACF,EAAIY,GAAG,gDAAgDV,EAAG,IAAI,CAACF,EAAIY,GAAG,8LAA8LV,EAAG,KAAK,CAACF,EAAIY,GAAG,oDAAoDV,EAAG,IAAI,CAACF,EAAIY,GAAG,0DAA0DV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,2eAA2eV,EAAG,KAAK,CAACF,EAAIY,GAAG,+DAA+DV,EAAG,IAAI,CAACF,EAAIY,GAAG,iVAAiVV,EAAG,KAAK,CAACF,EAAIY,GAAG,gBAAgBV,EAAG,IAAI,CAACF,EAAIY,GAAG,2zBAA2zBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,KAAK,CAACF,EAAIY,GAAG,mEAAmEV,EAAG,IAAI,CAACF,EAAIY,GAAG,8eAA8eV,EAAG,KAAK,CAACF,EAAIY,GAAG,oCAAoCV,EAAG,IAAI,CAACF,EAAIY,GAAG,sLAAsLV,EAAG,SAAS,CAACF,EAAIY,GAAG,sTAAsTZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,uQAAuQZ,EAAIY,GAAG,gEAAgEV,EAAG,SAAS,CAACF,EAAIY,GAAG,kIAAkIV,EAAG,KAAK,CAACF,EAAIY,GAAG,sCAAsCV,EAAG,IAAI,CAACF,EAAIY,GAAG,kXAAkXV,EAAG,SAAS,CAACF,EAAIY,GAAG,+HAA+HZ,EAAIY,GAAG,uQAAuQV,EAAG,SAAS,CAACF,EAAIY,GAAG,4GAA4GZ,EAAIY,GAAG,stBAA4tBV,EAAG,KAAK,CAACF,EAAIY,GAAG,mDAAmDV,EAAG,IAAI,CAACF,EAAIY,GAAG,oFAAoFV,EAAG,SAAS,CAACF,EAAIY,GAAG,+LAA+LZ,EAAIY,GAAG,8LAA8LV,EAAG,IAAI,CAACF,EAAIY,GAAG,sHAAsHV,EAAG,SAAS,CAACF,EAAIY,GAAG,6SAAqTZ,EAAIY,GAAG,0CAA0CV,EAAG,SAAS,CAACF,EAAIY,GAAG,8IAAoJZ,EAAIY,GAAG,sDAAsDV,EAAG,SAAS,CAACF,EAAIY,GAAG,qCAAqCZ,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,wFAA4FZ,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oDAAoDV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,MAAqD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,sEAAsEV,EAAG,SAAS,CAACF,EAAIY,GAAG,wCAAwCZ,EAAIY,GAAG,kCAAkCV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,iEAAiEV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,2BAA2BV,EAAG,KAAK,CAACF,EAAIY,GAAG,qCAAqCV,EAAG,KAAK,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,KAAK,CAACF,EAAIY,GAAG,uCAAuCV,EAAG,IAAI,CAACF,EAAIY,GAAG,kHAAkHV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4DAA4DV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,QAAQ,CAACG,MAAM,CAAC,GAAK,+BAA+B,CAACH,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,oFAAoFV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,4CAA4CV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,SAASV,EAAG,KAAK,CAACF,EAAIY,GAAG,eAAeV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,8LAA8LV,EAAG,MAAMA,EAAG,MAAMF,EAAIY,GAAG,oZAAoZV,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,qBAAqB,IAAMgB,EAAQ,MAA+D,IAAM,mBAAmBnB,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4CAA4CV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,qBAAqBV,EAAG,KAAK,CAACF,EAAIY,GAAG,kCAAkCV,EAAG,MAAM,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,gDAAgDV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACE,YAAY,gBAAgBC,MAAM,CAAC,GAAK,oBAAoB,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,oBAAoBV,EAAG,KAAK,CAACF,EAAIY,GAAG,oCAAoCV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,+DAA+DV,EAAG,SAAS,CAACF,EAAIY,GAAG,oBAAoBZ,EAAIY,GAAG,cAAcV,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,yHAAyHV,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,oBAAoBV,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,4EAA4EV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,oBAAoBV,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,sCAAsCV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8NAA+NV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,qKAAqKV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,gFAAgFV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oGAAoG,OAAS,WAAW,CAACL,EAAIY,GAAG,uGAAuGZ,EAAIY,GAAG,iBAAiBV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,WAAWV,EAAG,YAAYA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iDAAiDV,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,gBAAgB,IAAMgB,EAAQ,MAAoD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,2SAA2SV,EAAG,SAAS,CAACF,EAAIY,GAAG,2DAA2DZ,EAAIY,GAAG,cAAcV,EAAG,SAAS,CAACF,EAAIY,GAAG,wBAAwBZ,EAAIY,GAAG,gDAAgDV,EAAG,SAAS,CAACF,EAAIY,GAAG,wDAAwDZ,EAAIY,GAAG,mCAAmCV,EAAG,SAAS,CAACF,EAAIY,GAAG,8EAA8EZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,4XAA4XV,EAAG,IAAI,CAACF,EAAIY,GAAG,yQAAyQV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iDAAiDV,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,kDAAkDV,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,KAAsD,IAAM,4BAA4BnB,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,qBAAqBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,yLAAyLV,EAAG,SAAS,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,mDAAmDV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,SAAS,CAACF,EAAIY,GAAG,iBAAiBZ,EAAIY,GAAG,qBAAqBV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,iFAAiFV,EAAG,KAAK,CAACF,EAAIY,GAAG,uGAAuGV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,UAAUV,EAAG,SAAS,CAACF,EAAIY,GAAG,WAAWZ,EAAIY,GAAG,cAAcV,EAAG,SAAS,CAACF,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,mBAAmBV,EAAG,SAAS,CAACF,EAAIY,GAAG,SAASZ,EAAIY,GAAG,mCAAmCV,EAAG,KAAK,CAACF,EAAIY,GAAG,QAAQV,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,sEAAsEV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,iBAAiBZ,EAAIY,GAAG,iBAAiBV,EAAG,SAAS,CAACF,EAAIY,GAAG,gBAAgBZ,EAAIY,GAAG,kBAAkBV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,6IAA6IV,EAAG,SAAS,CAACF,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,YAAYZ,EAAIY,GAAG,ghBAA0hBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,iCAAiCV,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,qIAAyIV,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAMgB,EAAQ,MAAsD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,8GAA8GV,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,kDAAkDV,EAAG,SAAS,CAACF,EAAIY,GAAG,iDAAiDZ,EAAIY,GAAG,mTAAuTV,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,mBAAmB,IAAMgB,EAAQ,MAAgD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,0CAA0CV,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,SAAS,CAACF,EAAIY,GAAG,wBAAwBZ,EAAIY,GAAG,kBAAkBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,uBAAuBZ,EAAIY,GAAG,uEAAuEV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,8BAA8BZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,0FAA0FV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,ytBAAytBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,oDAAoDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0CAA0C,OAAS,WAAW,CAACL,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,yDAAyDV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,+EAA+EV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8C,OAAS,WAAW,CAACL,EAAIY,GAAG,iBAAiBZ,EAAIY,GAAG,kHAAkHV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0HAA0H,OAAS,WAAW,CAACL,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,KAAKV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iDAAiDV,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,mFAAmFV,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,8BAA8B,IAAMgB,EAAQ,MAAoD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,qBAAqBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,2DAA2DV,EAAG,SAAS,CAACF,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,yCAAyCV,EAAG,IAAI,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,SAAS,CAACF,EAAIY,GAAG,2BAA2BZ,EAAIY,GAAG,sCAAsCV,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,YAAYV,EAAG,SAAS,CAACF,EAAIY,GAAG,mCAAmCZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,mIAAmIV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,qCAAqCV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,aAAaV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,uFAAuFV,EAAG,SAAS,CAACF,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,6EAA6EV,EAAG,SAAS,CAACF,EAAIY,GAAG,6CAA6CZ,EAAIY,GAAG,4DAA4DV,EAAG,SAAS,CAACF,EAAIY,GAAG,wBAAwBZ,EAAIY,GAAG,0CAA0CV,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,2HAA2HV,EAAG,SAAS,CAACF,EAAIY,GAAG,qBAAqBZ,EAAIY,GAAG,gEAAgEV,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,mBAAmB,IAAMgB,EAAQ,MAA+C,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,aAAaV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,2CAA2CV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,qBAAqBZ,EAAIY,GAAG,+EAA+EV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,oCAAoCZ,EAAIY,GAAG,oGAAoGV,EAAG,SAAS,CAACF,EAAIY,GAAG,8BAA8BV,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,SAAS,CAACF,EAAIY,GAAG,gBAAgBZ,EAAIY,GAAG,yDAAyDV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,aAAaV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,sBAAsBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,oEAAoEV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,4SAA4SV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,OAAOV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,yDAAyD,OAAS,WAAW,CAACL,EAAIY,GAAG,gDAAgDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,WAAWV,EAAG,YAAYA,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oCAAoCV,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,mFAAmFV,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,sKAAsKV,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,0JAA0JV,EAAG,KAAK,CAACF,EAAIY,GAAG,sBAAsBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,6CAA6CV,EAAG,SAAS,CAACF,EAAIY,GAAG,mHAAmHZ,EAAIY,GAAG,iPAAiPV,EAAG,IAAI,CAACF,EAAIY,GAAG,yZAAyZV,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,gBAAgBZ,EAAIY,GAAG,wBAAwBV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,0DAA0DV,EAAG,KAAK,CAACF,EAAIY,GAAG,oDAAoDV,EAAG,KAAK,CAACF,EAAIY,GAAG,8FAA8FV,EAAG,KAAK,CAACF,EAAIY,GAAG,4GAA4GV,EAAG,KAAK,CAACF,EAAIY,GAAG,kEAAkEV,EAAG,KAAK,CAACF,EAAIY,GAAG,sFAAsFV,EAAG,KAAK,CAACF,EAAIY,GAAG,oDAAoDV,EAAG,KAAK,CAACF,EAAIY,GAAG,yFAAyFV,EAAG,KAAK,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,KAAK,CAACF,EAAIY,GAAG,4BAA4BV,EAAG,KAAK,CAACF,EAAIY,GAAG,0EAA0EV,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACF,EAAIY,GAAG,mFAAmFV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,mBAAmBZ,EAAIY,GAAG,wBAAwBV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,KAAK,CAACF,EAAIY,GAAG,gCAAgCV,EAAG,KAAK,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,KAAK,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,KAAK,CAACF,EAAIY,GAAG,uCAAuCV,EAAG,KAAK,CAACF,EAAIY,GAAG,6CAA6CV,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,4BAA4B,IAAMgB,EAAQ,MAAoD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,8KAA8KV,EAAG,SAAS,CAACF,EAAIY,GAAG,+FAA+FZ,EAAIY,GAAG,0CAA0CV,EAAG,SAAS,CAACF,EAAIY,GAAG,6DAA6DZ,EAAIY,GAAG,iDAAiDV,EAAG,SAAS,CAACF,EAAIY,GAAG,uCAAuCZ,EAAIY,GAAG,oDAAoDV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,iCAAiCV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BZ,EAAIY,GAAG,kHAAkHV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,6CAA6CZ,EAAIY,GAAG,UAAUV,EAAG,SAAS,CAACF,EAAIY,GAAG,8CAA8CV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,mJAAmJV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,oFAAoFV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,6PAA6PV,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,gCAAgC,IAAMgB,EAAQ,MAA+D,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iCAAiCV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,0KAA8KV,EAAG,SAAS,CAACF,EAAIY,GAAG,8CAAgDZ,EAAIY,GAAG,sBAAsBV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,8DAAgEV,EAAG,SAAS,CAACF,EAAIY,GAAG,WAAWZ,EAAIY,GAAG,qEAAuEV,EAAG,KAAK,CAACF,EAAIY,GAAG,4MAA4MV,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,0BAA0B,IAAMgB,EAAQ,MAAmD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8CAA8CV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,uHAAuHV,EAAG,SAAS,CAACF,EAAIY,GAAG,kHAAkHZ,EAAIY,GAAG,mBAAmBV,EAAG,SAAS,CAACF,EAAIY,GAAG,YAAYZ,EAAIY,GAAG,sBAAsBV,EAAG,SAAS,CAACF,EAAIY,GAAG,yFAAyFZ,EAAIY,GAAG,OAAOV,EAAG,SAAS,CAACF,EAAIY,GAAG,oDAAsDZ,EAAIY,GAAG,QAAQV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,yDAAyDV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAoBZ,EAAIY,GAAG,WAAWV,EAAG,SAAS,CAACF,EAAIY,GAAG,wCAAwCZ,EAAIY,GAAG,OAAOV,EAAG,SAAS,CAACF,EAAIY,GAAG,gCAAgCZ,EAAIY,GAAG,0FAA4FV,EAAG,KAAK,CAACF,EAAIY,GAAG,2EAA2EV,EAAG,SAAS,CAACF,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,4EAA4EV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,cAAcZ,EAAIY,GAAG,yDAAyDV,EAAG,SAAS,CAACF,EAAIY,GAAG,wEAAwEV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,8BAA8BV,EAAG,SAAS,CAACF,EAAIY,GAAG,gDAAgDZ,EAAIY,GAAG,wGAAwGV,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,mEAAmEV,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uBAAuBV,EAAG,IAAI,CAACF,EAAIY,GAAG,gNAAgNV,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAMgB,EAAQ,KAAwD,IAAM,sBAAsBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,qDAAqDV,EAAG,IAAI,CAACF,EAAIY,GAAG,mlBAAmlBV,EAAG,IAAI,CAACF,EAAIY,GAAG,4OAA4OV,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,udAAudV,EAAG,IAAI,CAACF,EAAIY,GAAG,yGAAyGV,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACE,YAAY,iBAAiB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,KAAK,CAACF,EAAIY,GAAG,4BAA4BV,EAAG,KAAK,CAACF,EAAIY,GAAG,6BAA6BV,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,uNAAuNV,EAAG,IAAI,CAACF,EAAIY,GAAG,0PAA0PV,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,YAAY,IAAMgB,EAAQ,MAA2C,IAAM,wBAAwBnB,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACF,EAAIY,GAAG,mCAAmCV,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,MAAiD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,MAAiD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,6SAA6SV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,WAAWV,EAAG,IAAI,CAACF,EAAIY,GAAG,iEAAiEV,EAAG,SAAS,CAACF,EAAIY,GAAG,kCAAkCZ,EAAIY,GAAG,0JAA0JV,EAAG,SAAS,CAACF,EAAIY,GAAG,oJAAoJV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,OAAOV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,wJAAwJ,OAAS,WAAW,CAACL,EAAIY,GAAG,oDAAoDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACF,EAAIY,GAAG,8BAA8BV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,0BAA0BV,EAAG,IAAI,CAACF,EAAIY,GAAG,uVAAuVV,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,sBAAsB,IAAMgB,EAAQ,MAAqD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,uIAAuIV,EAAG,SAAS,CAACF,EAAIY,GAAG,4DAA4DZ,EAAIY,GAAG,qEAAqEV,EAAG,SAAS,CAACF,EAAIY,GAAG,qBAAqBZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,eAAeV,EAAG,SAAS,CAACF,EAAIY,GAAG,iCAAiCZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,+IAA+IV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BZ,EAAIY,GAAG,uBAAuBV,EAAG,SAAS,CAACF,EAAIY,GAAG,+DAA+DZ,EAAIY,GAAG,gZAAgZV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,0BAA0B,IAAMgB,EAAQ,MAA6D,IAAM,wBAAwBnB,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,KAAK,CAACF,EAAIY,GAAG,wCAAwCV,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,yDAAyDV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,+CAA+CZ,EAAIY,GAAG,KAAKV,EAAG,SAAS,CAACF,EAAIY,GAAG,0FAA4FZ,EAAIY,GAAG,aAAaV,EAAG,SAAS,CAACF,EAAIY,GAAG,kFAAkFZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,4oBAA4oBV,EAAG,MAAM,CAACE,YAAY,WAAWF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,cAAc,IAAMgB,EAAQ,MAAkD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,yIAAyIV,EAAG,SAAS,CAACF,EAAIY,GAAG,oHAAoHZ,EAAIY,GAAG,gDAAgDV,EAAG,SAAS,CAACF,EAAIY,GAAG,WAAWZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCZ,EAAIY,GAAG,sFAAsFV,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4CAA4CV,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,8BAA8B,IAAMgB,EAAQ,IAAmD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,MAA6D,IAAM,sBAAsBnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,oSAA0SV,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,wJAA8JV,EAAG,SAAS,CAACF,EAAIY,GAAG,uBAAuBZ,EAAIY,GAAG,gFAAgFV,EAAG,SAAS,CAACF,EAAIY,GAAG,iBAAiBZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,wSAA4SV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iDAAiDV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,gHAAgHV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,+EAA+EV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,0GAA0GV,EAAG,IAAI,CAACF,EAAIY,GAAG,oKAAoKV,EAAG,SAAS,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,wEAAwEV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,iGAAiGV,EAAG,SAAS,CAACF,EAAIY,GAAG,0HAA0HV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,SAAS,CAACF,EAAIY,GAAG,sDAAsDZ,EAAIY,GAAG,mBAAmBV,EAAG,SAAS,CAACF,EAAIY,GAAG,+FAA+FZ,EAAIY,GAAG,kIAAkIV,EAAG,KAAK,CAACF,EAAIY,GAAG,2HAA2HV,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,4NAA4NV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,wKAAwKV,EAAG,IAAI,CAACF,EAAIY,GAAG,mTAAmTV,EAAG,IAAI,CAACF,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,gEAAgEZ,EAAIY,GAAG,wBAAwBV,EAAG,SAAS,CAACF,EAAIY,GAAG,oGAAoGZ,EAAIY,GAAG,0PAA0PV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,0DAA0DV,EAAG,KAAK,CAACF,EAAIY,GAAG,gBAAgBV,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,4JAA4JV,EAAG,SAAS,CAACF,EAAIY,GAAG,mEAAmEZ,EAAIY,GAAG,YAAYV,EAAG,SAAS,CAACF,EAAIY,GAAG,8FAA8FZ,EAAIY,GAAG,uFAAuFV,EAAG,IAAI,CAACF,EAAIY,GAAG,ySAAySV,EAAG,IAAI,CAACF,EAAIY,GAAG,yDAAyDV,EAAG,SAAS,CAACF,EAAIY,GAAG,2CAA2CZ,EAAIY,GAAG,0CAA0CV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,qOAAqOV,EAAG,MAAM,CAACE,YAAY,oBAAoB,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,sBAAsB,IAAMgB,EAAQ,MAA8D,IAAM,sBAAsBnB,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,2BAA2BV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,IAAI,CAACF,EAAIY,GAAG,gFAAgFV,EAAG,SAAS,CAACF,EAAIY,GAAG,6CAA6CZ,EAAIY,GAAG,gBAAgBV,EAAG,SAAS,CAACF,EAAIY,GAAG,0CAA0CZ,EAAIY,GAAG,sOAAwOV,EAAG,SAAS,CAACF,EAAIY,GAAG,oBAAoBZ,EAAIY,GAAG,wDAAwDV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,mEAAmEV,EAAG,SAAS,CAACF,EAAIY,GAAG,0EAA0EZ,EAAIY,GAAG,yEAAyEV,EAAG,SAAS,CAACF,EAAIY,GAAG,2CAA2CZ,EAAIY,GAAG,4BAA4BV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BZ,EAAIY,GAAG,gHAAgHV,EAAG,SAAS,CAACF,EAAIY,GAAG,4EAA4EZ,EAAIY,GAAG,+BAA+BV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,yCAAyCV,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,sBAAsB,IAAMgB,EAAQ,MAAqD,IAAM,+BAA+BnB,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,+CAA+CV,EAAG,IAAI,CAACF,EAAIY,GAAG,+GAA+GV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,YAAYV,EAAG,SAAS,CAACF,EAAIY,GAAG,wCAAwCZ,EAAIY,GAAG,MAAMV,EAAG,SAAS,CAACF,EAAIY,GAAG,yDAAyDZ,EAAIY,GAAG,YAAYV,EAAG,SAAS,CAACF,EAAIY,GAAG,2CAA2CZ,EAAIY,GAAG,8DAA8DV,EAAG,SAAS,CAACF,EAAIY,GAAG,aAAaZ,EAAIY,GAAG,QAAQV,EAAG,SAAS,CAACF,EAAIY,GAAG,eAAeZ,EAAIY,GAAG,sJAAsJV,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,6BAA6B,IAAMgB,EAAQ,MAA4D,IAAM,sCAAsCnB,EAAG,KAAK,CAACF,EAAIY,GAAG,oCAAoCV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,yEAAyEZ,EAAIY,GAAG,+FAA+FV,EAAG,SAAS,CAACF,EAAIY,GAAG,mFAAmFZ,EAAIY,GAAG,6UAA6UV,EAAG,MAAM,CAACE,YAAY,wBAAwB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,2DAA2DV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,8IAA8IV,EAAG,SAAS,CAACF,EAAIY,GAAG,0BAA0BZ,EAAIY,GAAG,SAASV,EAAG,SAAS,CAACF,EAAIY,GAAG,6BAA6BV,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,6PAA6PV,EAAG,MAAM,CAACE,YAAY,QAAQ,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,aAAa,IAAMgB,EAAQ,MAA4C,IAAM,oBAAoBnB,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACL,EAAIY,GAAG,qDAAqDV,EAAG,SAAS,CAACF,EAAIY,GAAG,kCAAkCZ,EAAIY,GAAG,6BAA6BV,EAAG,SAAS,CAACF,EAAIY,GAAG,yCAAyCZ,EAAIY,GAAG,4DAA4DV,EAAG,SAAS,CAACF,EAAIY,GAAG,kBAAkBZ,EAAIY,GAAG,kBAAkBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iDAAiDV,EAAG,KAAK,CAACF,EAAIY,GAAG,kFAAkFV,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,yBAAyB,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,qCAAqCV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,kCAAkC,IAAMgB,EAAQ,MAAwD,IAAM,kBAAkBnB,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,eAAe,IAAMgB,EAAQ,MAA8C,IAAM,oDAAoDnB,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACF,EAAIY,GAAG,2DAA2DV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAMgB,EAAQ,MAAuD,IAAM,sEAAsEnB,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,aAAa,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,qCAAqCV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,kCAAkC,IAAMgB,EAAQ,MAAwD,IAAM,oBAAoBnB,EAAG,MAAM,CAACE,YAAY,gCAAgC,CAACF,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,iEAAiEZ,EAAIY,GAAG,oGAAoGV,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,mBAAmB,IAAMgB,EAAQ,MAAkD,IAAM,kBAAkBnB,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACH,EAAG,IAAI,CAACF,EAAIY,GAAG,0CAA0CV,EAAG,SAAS,CAACF,EAAIY,GAAG,8EAA8EZ,EAAIY,GAAG,2BAA2BV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,cAAcV,EAAG,SAAS,CAACF,EAAIY,GAAG,4EAA4EV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,WAAWV,EAAG,IAAI,CAACF,EAAIY,GAAG,sGAAsGV,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,SAAS,CAACF,EAAIY,GAAG,8IAA8IV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACH,EAAG,IAAI,CAACF,EAAIY,GAAG,wFAAwFV,EAAG,SAAS,CAACF,EAAIY,GAAG,uDAAuDZ,EAAIY,GAAG,wJAAwJV,EAAG,IAAI,CAACF,EAAIY,GAAG,iUAAiUV,EAAG,SAAS,CAACF,EAAIY,GAAG,oEAAoEZ,EAAIY,GAAG,2BAA2BV,EAAG,SAAS,CAACF,EAAIY,GAAG,YAAYZ,EAAIY,GAAG,gBAAgBV,EAAG,MAAM,CAACE,YAAY,mBAAmBC,MAAM,CAAC,GAAK,YAAY,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,kBAAkBV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,eAAe,IAAMgB,EAAQ,MAA8C,IAAM,oDAAoDnB,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACF,EAAIY,GAAG,2DAA2DV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAMgB,EAAQ,MAAuD,IAAM,sEAAsEnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,OAAOV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,sCAAsC,OAAS,WAAW,CAACL,EAAIY,GAAG,oBAAoBZ,EAAIY,GAAG,kHAAkHV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iDAAiDV,EAAG,KAAK,CAACF,EAAIY,GAAG,4IAA4IV,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,0BAA0B,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wCAAwC,IAAMgB,EAAQ,MAAkD,IAAM,2CAA2CnB,EAAG,MAAMA,EAAG,MAAMA,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wBAAwB,IAAMgB,EAAQ,MAAoD,IAAM,0BAA0BnB,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,GAAK,8BAA8B,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,+BAA+BV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,wCAAwC,IAAMgB,EAAQ,MAAkD,IAAM,6CAA6CnB,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,GAAK,kCAAkC,CAACH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,wHAAwHV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,oDAAoDV,EAAG,SAAS,CAACF,EAAIY,GAAG,oGAAoGZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,wGAAwGV,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,yBAAyB,IAAMgB,EAAQ,MAAoD,IAAM,0BAA0BnB,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACG,MAAM,CAAC,MAAQ,MAAM,CAACH,EAAG,IAAI,CAACF,EAAIY,GAAG,+FAA+FV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,4EAA4EV,EAAG,IAAI,CAACF,EAAIY,GAAG,8FAA8FV,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAMgB,EAAQ,MAAgD,IAAM,wBAAwBnB,EAAG,MAAM,CAACE,YAAY,kBAAkBC,MAAM,CAAC,GAAK,qCAAqC,CAACH,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,wHAAwHV,EAAG,SAAS,CAACF,EAAIY,GAAG,4BAA4BZ,EAAIY,GAAG,oDAAoDV,EAAG,SAAS,CAACF,EAAIY,GAAG,oGAAoGZ,EAAIY,GAAG,QAAQV,EAAG,IAAI,CAACF,EAAIY,GAAG,oGAAoGV,EAAG,KAAK,CAACA,EAAG,IAAI,CAACF,EAAIY,GAAG,+FAA+FV,EAAG,MAAM,CAACA,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,OAAO,GAAK,SAAS,CAACL,EAAIY,GAAG,SAASZ,EAAIY,GAAG,4EAA4EV,EAAG,IAAI,CAACF,EAAIY,GAAG,8FAA8FV,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,iBAAiB,IAAMgB,EAAQ,MAAgD,IAAM,wBAAwBnB,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,GAAK,qBAAqB,CAACH,EAAG,IAAI,CAACG,MAAM,CAAC,GAAK,kBAAkB,CAACL,EAAIY,GAAG,OAAOV,EAAG,KAAK,CAACF,EAAIY,GAAG,sBAAsBV,EAAG,MAAM,CAACE,YAAY,YAAYC,MAAM,CAAC,GAAK,uBAAuB,IAAMgB,EAAQ,MAAkD,IAAM,wBAAwBnB,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,MAAMA,EAAG,IAAI,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,GAAK,QAAQ,CAACL,EAAIY,GAAG,mHAAmHV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,QAAQ,MAAQ,yCAAyC,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,IAAI,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACF,EAAIY,GAAG,2HAA2HV,EAAG,KAAK,CAACF,EAAIY,GAAG,4GAA4GV,EAAG,SAAS,CAACF,EAAIY,GAAG,kGAAkGZ,EAAIY,GAAG,uDAAuDV,EAAG,KAAK,CAACF,EAAIY,GAAG,wJAAwJV,EAAG,KAAK,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,SAAS,CAACF,EAAIY,GAAG,oFAAoFZ,EAAIY,GAAG,yJAC9upE,GC2qCA,IACAE,KAAAA,uBACAqE,UACA1C,IACAM,EAAAA,uBACA,GCnrCsR,MCQlR,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wIAAwIV,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,yBAAyB,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,wBAAwB,mBAAmB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,kDAAkD,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA8C,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4CAA4CV,EAAG,IAAI,CAACF,EAAIY,GAAG,6MAA6MV,EAAG,MAAMF,EAAIY,GAAG,mFAAmFV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gCAAgC,OAAS,WAAW,CAACL,EAAIY,GAAG,uBAAuBV,EAAG,MAAMF,EAAIY,GAAG,kFAAkFV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACL,EAAIY,GAAG,WAAWV,EAAG,MAAMF,EAAIY,GAAG,sDAAsDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,qCAAqC,OAAS,WAAW,CAACL,EAAIY,GAAG,+BAA+BV,EAAG,IAAI,CAACF,EAAIY,GAAG,wJAAwJV,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,cAAcV,EAAG,IAAI,CAACF,EAAIY,GAAG,8+CAA8+CV,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,yBAAyBV,EAAG,SAAS,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACL,EAAIY,GAAG,yKAAyKV,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4FAA4FV,EAAG,IAAI,CAACF,EAAIY,GAAG,icAAicV,EAAG,KAAK,CAACF,EAAIY,GAAG,oDAAoDV,EAAG,IAAI,CAACF,EAAIY,GAAG,8GAA8GV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,wCAAwC,OAAS,WAAW,CAACL,EAAIY,GAAG,qCAAqCZ,EAAIY,GAAG,omBAAomBV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,2HAA2H,OAAS,WAAW,CAACL,EAAIY,GAAG,oCAAoCZ,EAAIY,GAAG,sbAAsbV,EAAG,KAAK,CAACF,EAAIY,GAAG,yFAAyFV,EAAG,IAAI,CAACF,EAAIY,GAAG,uxBAAuxBV,EAAG,IAAI,CAACF,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,iGAAiGV,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,IAAI,CAACF,EAAIY,GAAG,2XAA2XV,EAAG,IAAI,CAACF,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,sEAAsEV,EAAG,IAAI,CAACF,EAAIY,GAAG,SAASZ,EAAIY,GAAG,2yBAA2yBV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,0CAA0C,OAAS,WAAW,CAACL,EAAIY,GAAG,yBAAyBZ,EAAIY,GAAG,gXAAgXV,EAAG,IAAI,CAACF,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA0C,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,uDAAuDV,EAAG,IAAI,CAACF,EAAIY,GAAG,+VAA+VV,EAAG,IAAI,CAACF,EAAIY,GAAG,knDAAknDV,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA4C,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,2HAA2HV,EAAG,KAAK,CAACF,EAAIY,GAAG,6DAA6DV,EAAG,IAAI,CAACF,EAAIY,GAAG,+sBAA+sBV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,qFAAqF,OAAS,WAAW,CAACL,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,8BAA8BV,EAAG,KAAK,CAACF,EAAIY,GAAG,gCAAgCV,EAAG,IAAI,CAACF,EAAIY,GAAG,+eAA+eV,EAAG,SAAS,CAACF,EAAIY,GAAG,8OAA8OZ,EAAIY,GAAG,+dAA+dV,EAAG,IAAI,CAACF,EAAIY,GAAG,UAAUZ,EAAIY,GAAG,cAAcV,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA8C,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,gBAAgBV,EAAG,IAAI,CAACF,EAAIY,GAAG,6kCAC55d,GCiMA,IACAE,KAAAA,mBACAqE,UACA1C,IACAM,EAAAA,uBACA,GCzMyQ,MCQrQ,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,yEAAyEV,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,iBAAiB,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,gBAAgB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,gBAAgB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,gBAAgB,mBAAmB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAuD,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,gBAAgBV,EAAG,IAAI,CAACF,EAAIY,GAAG,muBAAmuBV,EAAG,MAAM,CAACE,YAAY,oCAAoC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAoD,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,8DAA8DV,EAAG,IAAI,CAACF,EAAIY,GAAG,otBAAotBV,EAAG,MAAM,CAACE,YAAY,oCAAoC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAoD,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,IAAI,CAACF,EAAIY,GAAG,+wBACpsG,GCgEA,IAEAE,KAAAA,OACAqE,UACA1C,IACAM,EAAAA,eACA,GCzE2Q,MCQvQ,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,sCAAsCV,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,kBAAkB,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,iBAAiB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,iBAAiB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,iBAAiB,mBAAmB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,4CAA4C,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA6C,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,IAAI,CAACF,EAAIY,GAAG,kZAAkZV,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAgD,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4EAA4EV,EAAG,IAAI,CAACF,EAAIY,GAAG,sQAA+QV,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,oCAAoCC,MAAM,CAAC,IAAMgB,EAAQ,OAA2DnB,EAAG,MAAM,CAACE,YAAY,iDAAiDC,MAAM,CAAC,IAAMgB,EAAQ,UAA+DnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,6FAA6FV,EAAG,IAAI,CAACF,EAAIY,GAAG,4hBAAsjBV,EAAG,SAAS,CAACF,EAAIY,GAAG,2IAC5vG,GC+DA,IAEAE,KAAAA,QACAqE,UACA1C,IACAM,EAAAA,gBACA,GCxE4Q,MCQxQ,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iEAAiEV,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,wBAAwB,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,uBAAuB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uBAAuB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uBAAuB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uBAAuB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,uBAAuB,mBAAmB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAqD,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,sCAAsCV,EAAG,IAAI,CAACF,EAAIY,GAAG,qNAAqNV,EAAG,MAAMF,EAAIY,GAAG,uFAAuFV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,6BAA6B,OAAS,WAAW,CAACL,EAAIY,GAAG,iBAAiBV,EAAG,MAAMF,EAAIY,GAAG,yEAAyEV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,mCAAmC,OAAS,WAAW,CAACL,EAAIY,GAAG,WAAWV,EAAG,MAAMF,EAAIY,GAAG,sDAAsDV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,uCAAuC,OAAS,WAAW,CAACL,EAAIY,GAAG,qCAAqCV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACA,EAAG,SAAS,CAACF,EAAIY,GAAG,sCAAsCV,EAAG,IAAI,CAACF,EAAIY,GAAG,qeAAqeV,EAAG,MAAMA,EAAG,SAAS,CAACG,MAAM,CAAC,GAAK,sBAAsB,CAACL,EAAIY,GAAG,sPAAsPV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAuD,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,6CAA6CV,EAAG,IAAI,CAACF,EAAIY,GAAG,0MAA0MV,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,oHAAoH,OAAS,WAAW,CAACL,EAAIY,GAAG,oBAAoBZ,EAAIY,GAAG,i3CAAm3CV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAmE,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,qEAAqEV,EAAG,KAAK,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,IAAI,CAACF,EAAIY,GAAG,41CAA41CV,EAAG,KAAK,CAACF,EAAIY,GAAG,mBAAmBV,EAAG,IAAI,CAACF,EAAIY,GAAG,sDAAsDV,EAAG,SAAS,CAACF,EAAIY,GAAG,8RAA8RZ,EAAIY,GAAG,6QAA6QV,EAAG,SAAS,CAACF,EAAIY,GAAG,mPAAmPV,EAAG,MAAM,CAACE,YAAY,kCAAkC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAAqD,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,aAAaV,EAAG,IAAI,CAACF,EAAIY,GAAG,i5BACxrQ,GC2IA,IACAE,KAAAA,oBACAqE,UACA1C,IACAM,EAAAA,sBACA,GCnJ0Q,MCQtQ,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIa,GAAG,EAC1E,EACIJ,GAAkB,CAAC,WAAY,IAAIT,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,SAASC,MAAM,CAAC,GAAK,UAAU,CAACH,EAAG,KAAK,CAACF,EAAIY,GAAG,iBAAiBV,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,sBAAsB,CAACH,EAAG,KAAK,CAACE,YAAY,uBAAuB,CAACF,EAAG,KAAK,CAACE,YAAY,SAASC,MAAM,CAAC,iBAAiB,qBAAqB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,qBAAqB,mBAAmB,OAAOH,EAAG,KAAK,CAACG,MAAM,CAAC,iBAAiB,qBAAqB,mBAAmB,SAASH,EAAG,MAAM,CAACE,YAAY,kBAAkB,CAACF,EAAG,MAAM,CAACE,YAAY,gDAAgD,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,KAA6C,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,wBAAwBV,EAAG,IAAI,CAACF,EAAIY,GAAG,sUAAsUV,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA6C,IAAM,kBAAkBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,2BAA2BV,EAAG,IAAI,CAACF,EAAIY,GAAG,waAAwaV,EAAG,MAAM,CAACE,YAAY,yCAAyC,CAACF,EAAG,MAAM,CAACE,YAAY,yBAAyBC,MAAM,CAAC,IAAMgB,EAAQ,MAA+C,IAAM,iBAAiBnB,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,MAAM,CAACE,YAAY,mBAAmB,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,4BAA4BV,EAAG,IAAI,CAACF,EAAIY,GAAG,6XACx9E,GC4DA,IAEAE,KAAAA,YACAqE,UACA1C,IACAM,EAAAA,oBACA,GCrEkQ,MCQ9P,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QCnB5BhD,GAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,YAAY,mCAAmC,CAACF,EAAG,KAAK,CAACF,EAAIY,GAAG,iFAAiFV,EAAG,cAAc,CAACE,YAAY,cAAcC,MAAM,CAAC,GAAK,MAAM,CAACL,EAAIY,GAAG,kCAAkC,EACtU,EACIH,GAAkB,GCMtB,IACAK,KAAAA,qBCTiQ,MCQ7P,IAAY,OACd,GACA,GACA,IACA,EACA,KACA,WACA,MAIF,GAAe,GAAiB,QChBhCsE,EAAAA,WAAAA,IAAQC,EAAAA,IAmBR,MAAMC,GAAS,CACX,CACI/E,KAAM,IACNS,UAAWuE,GAEf,CACIhF,KAAM,SACNS,UAAWwE,GAEf,CACIjF,KAAM,YACNS,UAAWyE,IAEf,CACIlF,KAAM,sBACNS,UAAW0E,IAEf,CACInF,KAAM,4CACNS,UAAW2E,IAEf,CACIpF,KAAM,kEACNS,UAAW4E,IAEf,CACIrF,KAAM,iEACNS,UAAW6E,IAEf,CACItF,KAAM,qCACNS,UAAW8E,IAEf,CACIvF,KAAM,kCACNS,UAAW+E,IAEf,CACIxF,KAAM,iCACNS,UAAWgF,IAEf,CACIzF,KAAM,gCACNS,UAAWiF,IAEf,CACI1F,KAAM,WACNS,UAAWkF,IAEf,CACI3F,KAAM,UACNS,UAAWmF,IAEf,CACI5F,KAAM,OACNS,UAAWoF,IAEf,CACI7F,KAAM,IACN8F,SAAU,SAKZC,GAAY,IAAIjB,EAAAA,GAAU,CAC5BkB,KAAM,OACNC,OAAQlB,KAGZ,U,gBCjFAF,EAAAA,WAAAA,OAAAA,eAA2B,EAC3BA,EAAAA,WAAAA,IAAQqB,EAAAA,KACRrB,EAAAA,WAAAA,IAAQsB,EAAAA,IAER,IAAItB,EAAAA,WAAI,CACNE,OAAM,GACNvF,OAAQ4G,GAAKA,EAAEC,KACdC,OAAO,O,uBCjBV,IAAIC,EAAM,CACT,sBAAuB,KACvB,sBAAuB,KACvB,2BAA4B,KAC5B,sBAAuB,KACvB,4BAA6B,KAC7B,sBAAuB,KACvB,YAAa,KACb,oCAAqC,IACrC,kCAAmC,GACnC,0BAA2B,KAC3B,uBAAwB,KACxB,6BAA8B,KAC9B,6BAA8B,KAC9B,sBAAuB,KACvB,2BAA4B,KAC5B,uBAAwB,IACxB,sCAAuC,KACvC,uBAAwB,KACxB,mCAAoC,KACpC,4BAA6B,KAC7B,uBAAwB,KACxB,yCAA0C,KAC1C,yCAA0C,IAC1C,cAAe,KACf,iBAAkB,IAClB,oBAAqB,KACrB,mBAAoB,KACpB,8BAA+B,KAC/B,iBAAkB,KAClB,yBAA0B,KAC1B,+BAAgC,KAChC,+BAAgC,KAChC,wCAAyC,KACzC,cAAe,KACf,iBAAkB,KAClB,cAAe,IACf,iBAAkB,KAClB,yBAA0B,KAC1B,0BAA2B,GAC3B,+BAAgC,IAChC,2BAA4B,KAC5B,sCAAuC,KACvC,oCAAqC,KACrC,oCAAqC,KACrC,iDAAkD,KAClD,iDAAkD,KAClD,YAAa,KACb,yBAA0B,KAC1B,uCAAwC,KACxC,8BAA+B,KAC/B,cAAe,KACf,iBAAkB,IAClB,yCAA0C,KAC1C,yCAA0C,KAC1C,yBAA0B,KAC1B,wBAAyB,KACzB,yBAA0B,KAC1B,yBAA0B,KAC1B,iBAAkB,KAClB,yBAA0B,KAC1B,gCAAiC,KACjC,6BAA8B,IAC9B,0BAA2B,KAC3B,sBAAuB,KACvB,oBAAqB,KACrB,wBAAyB,KACzB,wBAAyB,KACzB,kBAAmB,KACnB,kCAAmC,KACnC,iCAAkC,KAClC,6CAA8C,KAC9C,iBAAkB,KAClB,iBAAkB,KAClB,+BAAgC,KAChC,wBAAyB,KACzB,8BAA+B,KAC/B,qBAAsB,KACtB,gBAAiB,KACjB,eAAgB,KAChB,4BAA6B,KAC7B,iCAAkC,KAClC,4BAA6B,KAC7B,sBAAuB,KACvB,6BAA8B,KAC9B,qCAAsC,KACtC,2BAA4B,KAC5B,wBAAyB,KACzB,uBAAwB,KACxB,uBAAwB,KACxB,uBAAwB,KACxB,4BAA6B,KAC7B,2BAA4B,KAC5B,0BAA2B,KAC3B,mCAAoC,IACpC,gCAAiC,MAIlC,SAASC,EAAeC,GACvB,IAAInF,EAAKoF,EAAsBD,GAC/B,OAAOE,EAAoBrF,EAC5B,CACA,SAASoF,EAAsBD,GAC9B,IAAIE,EAAoBC,EAAEL,EAAKE,GAAM,CACpC,IAAII,EAAI,IAAIC,MAAM,uBAAyBL,EAAM,KAEjD,MADAI,EAAEE,KAAO,mBACHF,CACP,CACA,OAAON,EAAIE,EACZ,CACAD,EAAeQ,KAAO,WACrB,OAAOC,OAAOD,KAAKT,EACpB,EACAC,EAAeU,QAAUR,EACzB/B,EAAOwC,QAAUX,EACjBA,EAAelF,GAAK,I,2iPCnHhB8F,EAA2B,CAAC,EAGhC,SAAST,EAAoBU,GAE5B,IAAIC,EAAeF,EAAyBC,GAC5C,QAAqBE,IAAjBD,EACH,OAAOA,EAAaH,QAGrB,IAAIxC,EAASyC,EAAyBC,GAAY,CAGjDF,QAAS,CAAC,GAOX,OAHAK,EAAoBH,GAAUI,KAAK9C,EAAOwC,QAASxC,EAAQA,EAAOwC,QAASR,GAGpEhC,EAAOwC,OACf,CAGAR,EAAoBe,EAAIF,E,WCzBxB,IAAIG,EAAW,GACfhB,EAAoBiB,EAAI,SAASC,EAAQC,EAAUC,EAAIC,GACtD,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASC,EAAI,EAAGA,EAAIR,EAASS,OAAQD,IAAK,CACrCL,EAAWH,EAASQ,GAAG,GACvBJ,EAAKJ,EAASQ,GAAG,GACjBH,EAAWL,EAASQ,GAAG,GAE3B,IAJA,IAGIE,GAAY,EACPC,EAAI,EAAGA,EAAIR,EAASM,OAAQE,MACpB,EAAXN,GAAsBC,GAAgBD,IAAaf,OAAOD,KAAKL,EAAoBiB,GAAGW,OAAM,SAAStE,GAAO,OAAO0C,EAAoBiB,EAAE3D,GAAK6D,EAASQ,GAAK,IAChKR,EAASU,OAAOF,IAAK,IAErBD,GAAY,EACTL,EAAWC,IAAcA,EAAeD,IAG7C,GAAGK,EAAW,CACbV,EAASa,OAAOL,IAAK,GACrB,IAAIM,EAAIV,SACER,IAANkB,IAAiBZ,EAASY,EAC/B,CACD,CACA,OAAOZ,CArBP,CAJCG,EAAWA,GAAY,EACvB,IAAI,IAAIG,EAAIR,EAASS,OAAQD,EAAI,GAAKR,EAASQ,EAAI,GAAG,GAAKH,EAAUG,IAAKR,EAASQ,GAAKR,EAASQ,EAAI,GACrGR,EAASQ,GAAK,CAACL,EAAUC,EAAIC,EAwB/B,C,eC5BArB,EAAoB+B,EAAI,SAASvB,EAASwB,GACzC,IAAI,IAAI1E,KAAO0E,EACXhC,EAAoBC,EAAE+B,EAAY1E,KAAS0C,EAAoBC,EAAEO,EAASlD,IAC5EgD,OAAO2B,eAAezB,EAASlD,EAAK,CAAE4E,YAAY,EAAMC,IAAKH,EAAW1E,IAG3E,C,eCPA0C,EAAoBoC,EAAI,WACvB,GAA0B,kBAAfC,WAAyB,OAAOA,WAC3C,IACC,OAAOtJ,MAAQ,IAAIuJ,SAAS,cAAb,EAGhB,CAFE,MAAOpC,GACR,GAAsB,kBAAXzE,OAAqB,OAAOA,MACxC,CACA,CAPuB,E,eCAxBuE,EAAoBC,EAAI,SAASsC,EAAKC,GAAQ,OAAOlC,OAAOmC,UAAUC,eAAe5B,KAAKyB,EAAKC,EAAO,C,eCCtGxC,EAAoB8B,EAAI,SAAStB,GACX,qBAAXmC,QAA0BA,OAAOC,aAC1CtC,OAAO2B,eAAezB,EAASmC,OAAOC,YAAa,CAAEC,MAAO,WAE7DvC,OAAO2B,eAAezB,EAAS,aAAc,CAAEqC,OAAO,GACvD,C,eCNA7C,EAAoB8C,EAAI,G,eCKxB,IAAIC,EAAkB,CACrB,IAAK,GAaN/C,EAAoBiB,EAAEU,EAAI,SAASqB,GAAW,OAAoC,IAA7BD,EAAgBC,EAAgB,EAGrF,IAAIC,EAAuB,SAASC,EAA4B3I,GAC/D,IAKImG,EAAUsC,EALV7B,EAAW5G,EAAK,GAChB4I,EAAc5I,EAAK,GACnB6I,EAAU7I,EAAK,GAGIiH,EAAI,EAC3B,GAAGL,EAASkC,MAAK,SAAS1I,GAAM,OAA+B,IAAxBoI,EAAgBpI,EAAW,IAAI,CACrE,IAAI+F,KAAYyC,EACZnD,EAAoBC,EAAEkD,EAAazC,KACrCV,EAAoBe,EAAEL,GAAYyC,EAAYzC,IAGhD,GAAG0C,EAAS,IAAIlC,EAASkC,EAAQpD,EAClC,CAEA,IADGkD,GAA4BA,EAA2B3I,GACrDiH,EAAIL,EAASM,OAAQD,IACzBwB,EAAU7B,EAASK,GAChBxB,EAAoBC,EAAE8C,EAAiBC,IAAYD,EAAgBC,IACrED,EAAgBC,GAAS,KAE1BD,EAAgBC,GAAW,EAE5B,OAAOhD,EAAoBiB,EAAEC,EAC9B,EAEIoC,EAAqBC,KAAK,sBAAwBA,KAAK,uBAAyB,GACpFD,EAAmBE,QAAQP,EAAqBQ,KAAK,KAAM,IAC3DH,EAAmBI,KAAOT,EAAqBQ,KAAK,KAAMH,EAAmBI,KAAKD,KAAKH,G,IC/CvF,IAAIK,EAAsB3D,EAAoBiB,OAAEL,EAAW,CAAC,MAAM,WAAa,OAAOZ,EAAoB,KAAO,IACjH2D,EAAsB3D,EAAoBiB,EAAE0C,E","sources":["webpack://dyllew/./public/assets/17_835_Poster.pdf","webpack://dyllew/./public/assets/6_864_Project.pdf","webpack://dyllew/./public/assets/Dylan_Lewis_Resume.pdf","webpack://dyllew/./public/assets/IDS131_Final_Report.pdf","webpack://dyllew/./public/assets/IDS131_Poster.pdf","webpack://dyllew/./public/assets/int-dev-gray-lit.pdf","webpack://dyllew/./src/App.vue","webpack://dyllew/./src/components/Header.vue","webpack://dyllew/src/components/Header.vue","webpack://dyllew/./src/components/Header.vue?2279","webpack://dyllew/./src/components/Header.vue?03e2","webpack://dyllew/./src/components/NavBar.vue","webpack://dyllew/src/components/NavBar.vue","webpack://dyllew/./src/components/NavBar.vue?dd1b","webpack://dyllew/./src/components/NavBar.vue?3457","webpack://dyllew/src/App.vue","webpack://dyllew/./src/App.vue?51dd","webpack://dyllew/./src/App.vue?0e40","webpack://dyllew/./src/components/Home.vue","webpack://dyllew/src/components/Home.vue","webpack://dyllew/./src/components/Home.vue?5ebb","webpack://dyllew/./src/components/Home.vue?f1b4","webpack://dyllew/./src/components/About.vue","webpack://dyllew/./src/constants.js","webpack://dyllew/src/components/About.vue","webpack://dyllew/./src/components/About.vue?878d","webpack://dyllew/./src/components/About.vue?5a46","webpack://dyllew/./src/components/Projects.vue","webpack://dyllew/./src/components/ProjectCard.vue","webpack://dyllew/src/components/ProjectCard.vue","webpack://dyllew/./src/components/ProjectCard.vue?72e0","webpack://dyllew/./src/components/ProjectCard.vue?31ae","webpack://dyllew/src/components/Projects.vue","webpack://dyllew/./src/components/Projects.vue?db9f","webpack://dyllew/./src/components/Projects.vue?3130","webpack://dyllew/./src/components/Artwork.vue","webpack://dyllew/src/components/Artwork.vue","webpack://dyllew/./src/components/Artwork.vue?8a8c","webpack://dyllew/./src/components/Artwork.vue?bd4f","webpack://dyllew/./src/components/Resume.vue","webpack://dyllew/src/components/Resume.vue","webpack://dyllew/./src/components/Resume.vue?da58","webpack://dyllew/./src/components/Resume.vue?fdad","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue","webpack://dyllew/src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue?9515","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData.vue?236c","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue","webpack://dyllew/src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue?192c","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel.vue?7b8e","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue","webpack://dyllew/src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue?45f6","webpack://dyllew/./src/components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel.vue?3c97","webpack://dyllew/./src/components/project-pages/NLPIntDevGrayLit.vue","webpack://dyllew/src/components/project-pages/NLPIntDevGrayLit.vue","webpack://dyllew/./src/components/project-pages/NLPIntDevGrayLit.vue?173d","webpack://dyllew/./src/components/project-pages/NLPIntDevGrayLit.vue?c9d5","webpack://dyllew/./src/components/project-pages/GNNsTaxiPrediction.vue","webpack://dyllew/src/components/project-pages/GNNsTaxiPrediction.vue","webpack://dyllew/./src/components/project-pages/GNNsTaxiPrediction.vue?fbbd","webpack://dyllew/./src/components/project-pages/GNNsTaxiPrediction.vue?c70c","webpack://dyllew/./src/components/project-pages/TrumpSpeechAnalysis.vue","webpack://dyllew/src/components/project-pages/TrumpSpeechAnalysis.vue","webpack://dyllew/./src/components/project-pages/TrumpSpeechAnalysis.vue?a2e0","webpack://dyllew/./src/components/project-pages/TrumpSpeechAnalysis.vue?7873","webpack://dyllew/./src/components/project-pages/ClimateChangeNews.vue","webpack://dyllew/src/components/project-pages/ClimateChangeNews.vue","webpack://dyllew/./src/components/project-pages/ClimateChangeNews.vue?a510","webpack://dyllew/./src/components/project-pages/ClimateChangeNews.vue?de5c","webpack://dyllew/./src/components/project-pages/Boomerang.vue","webpack://dyllew/src/components/project-pages/Boomerang.vue","webpack://dyllew/./src/components/project-pages/Boomerang.vue?75c0","webpack://dyllew/./src/components/project-pages/Boomerang.vue?300e","webpack://dyllew/./src/components/NotFoundComponent.vue","webpack://dyllew/src/components/NotFoundComponent.vue","webpack://dyllew/./src/components/NotFoundComponent.vue?ca97","webpack://dyllew/./src/components/NotFoundComponent.vue?48c1","webpack://dyllew/./src/router.js","webpack://dyllew/./src/main.js","webpack://dyllew/./public/assets/ sync ^\\.\\/.*$","webpack://dyllew/webpack/bootstrap","webpack://dyllew/webpack/runtime/chunk loaded","webpack://dyllew/webpack/runtime/define property getters","webpack://dyllew/webpack/runtime/global","webpack://dyllew/webpack/runtime/hasOwnProperty shorthand","webpack://dyllew/webpack/runtime/make namespace object","webpack://dyllew/webpack/runtime/publicPath","webpack://dyllew/webpack/runtime/jsonp chunk loading","webpack://dyllew/webpack/startup"],"sourcesContent":["export default __webpack_public_path__ + \"6eb743779661419ed3ede6d7193d7100.pdf\";","export default __webpack_public_path__ + \"7e9aaf3483d23bc06619cc3709d2c27b.pdf\";","export default __webpack_public_path__ + \"66d74b41579bff85b953a99ace15dcfb.pdf\";","export default __webpack_public_path__ + \"025d895ee0dabebc6014119228790280.pdf\";","export default __webpack_public_path__ + \"dab0ebf17c4c46cff583b36d55f0da85.pdf\";","export default __webpack_public_path__ + \"b1023c72c5afaee8e04a5cd4dd6439a7.pdf\";","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"container-fluid p-3 min-vh-100\",attrs:{\"id\":\"app\"}},[_c('Header'),(this.$route.path !== `/` && this.$route.path !== `/404`)?_c('NavBar'):_vm._e(),_c('router-view')],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row justify-content-end\"},[_c('div',{staticClass:\"col-12 col-md-4 col-lg-4\",attrs:{\"id\":\"website-title\"}},[_c('h1',{attrs:{\"id\":\"name animate__fadeInDown\"},on:{\"click\":_vm.goHome}},[_vm._v(\" Dylan Lewis \")])]),_vm._m(0)])\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"col-md-4 my-auto\",attrs:{\"id\":\"logos-col\"}},[_c('div',{staticClass:\"d-flex flex-row justify-content-center justify-content-md-end\"},[_c('a',{attrs:{\"href\":\"mailto: dylanrl97@gmail.com\"}},[_c('i',{staticClass:\"fa fa-envelope-o fa-3x\"}),_c('i',{staticClass:\"fa fa-envelope-o fa-2x\"})]),_c('a',{attrs:{\"href\":\"https://www.linkedin.com/in/dyllew/\",\"target\":\"_blank\"}},[_c('i',{staticClass:\"fa fa-linkedin-square fa-3x\"}),_c('i',{staticClass:\"fa fa-linkedin-square fa-2x\"})]),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/\",\"target\":\"_blank\"}},[_c('i',{staticClass:\"fa fa-github fa-3x\"}),_c('i',{staticClass:\"fa fa-github fa-2x\"})])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row justify-content-end\">\n        <div id=\"website-title\" class=\"col-12 col-md-4 col-lg-4\">\n            <h1 v-on:click=\"goHome\" id=\"name animate__fadeInDown\"> Dylan Lewis </h1>\n        </div>\n        <div id=\"logos-col\" class=\"col-md-4 my-auto\">\n            <div class=\"d-flex flex-row justify-content-center justify-content-md-end\">\n                <a href=\"mailto: dylanrl97@gmail.com\">\n                    <i class=\"fa fa-envelope-o fa-3x\"></i>\n                    <i class=\"fa fa-envelope-o fa-2x\"></i>\n                </a> \n                <a href=\"https://www.linkedin.com/in/dyllew/\" target=\"_blank\">\n                    <i class=\"fa fa-linkedin-square fa-3x\"></i>\n                    <i class=\"fa fa-linkedin-square fa-2x\"></i>\n                </a>\n                <a href=\"https://github.com/dyllew/\" target=\"_blank\">\n                    <i class=\"fa fa-github fa-3x\"></i>\n                    <i class=\"fa fa-github fa-2x\"></i>\n                </a>\n            </div>\n        </div>\n    </div>    \n</template>\n\n<script>\nexport default {\n  // eslint-disable-next-line\n  name: 'Header',\n  methods: {\n      goHome() {\n          this.$router.push('/');\n      }\n  }\n}\n</script>\n\n<style scoped>\nh1 {\n    font-size: 50px;\n}\n\nh1:hover {\n    cursor: pointer;\n}\n\na {\n    color: #61DAFB;\n    margin-left: 15px;\n    margin-right: 15px;\n}\n\na:hover {\n    color: white;\n}\n\n@media (max-width: 761px) {\n    .fa-3x {\n        display: none;\n    }\n}\n@media (min-width: 761px) {\n    .fa-2x {\n        display: none;\n    }\n}\n\n@media (min-width: 768px) {\n    #logos-col {\n        justify-content: flex-end; \n    }\n}\n\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Header.vue?vue&type=template&id=540ae276&scoped=true&\"\nimport script from \"./Header.vue?vue&type=script&lang=js&\"\nexport * from \"./Header.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Header.vue?vue&type=style&index=0&id=540ae276&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"540ae276\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row pt-2 justify-content-center\"},[_c('div',{staticClass:\"col-md-12\",attrs:{\"id\":\"nav-bar\"}},[_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/about\"}},[_vm._v(\"About Me\")]),_vm._v(\" | \"),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/projects\"}},[_vm._v(\"Projects\")]),_vm._v(\" | \"),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/artwork\"}},[_vm._v(\"Artwork\")]),_vm._v(\" | \"),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/resume\"}},[_vm._v(\"Resume\")])],1)])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row pt-2 justify-content-center\">\n      <div id=\"nav-bar\" class=\"col-md-12\">\n          <router-link class=\"router-link\" to=\"/about\">About Me</router-link> |\n          <router-link class=\"router-link\" to=\"/projects\">Projects</router-link> |\n          <router-link class=\"router-link\" to=\"/artwork\">Artwork</router-link> |\n          <router-link class=\"router-link\" to=\"/resume\">Resume</router-link>\n      </div>\n    </div>  \n</template>\n\n<script>\nexport default {\n  name: 'NavBar'\n}\n</script>\n\n<style scoped>\n\n#nav-bar {\n  font-size: 30px;\n}\n\n@media (max-width: 500px) {\n    #nav-bar {\n      font-size: 20px;\n    }\n}\n\n.router-link {\n  color: #61DAFB;\n}\n\n.router-link:hover {\n  color: whitesmoke;\n}\n\n.router-link-active {\n  color: white;\n  text-decoration: underline;\n}\n\n\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./NavBar.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./NavBar.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./NavBar.vue?vue&type=template&id=bbe68fba&scoped=true&\"\nimport script from \"./NavBar.vue?vue&type=script&lang=js&\"\nexport * from \"./NavBar.vue?vue&type=script&lang=js&\"\nimport style0 from \"./NavBar.vue?vue&type=style&index=0&id=bbe68fba&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"bbe68fba\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div id=\"app\" class=\"container-fluid p-3 min-vh-100\">\n    <Header />\n    <NavBar v-if=\"this.$route.path !== `/` && this.$route.path !== `/404`\" />\n    <router-view></router-view>\n  </div>\n</template>\n\n<script>\n// eslint-disable-next-line\nimport Header from './components/Header';\nimport NavBar from './components/NavBar';\n\nexport default {\n  name: 'App',\n  components: {\n    // eslint-disable-next-line\n    Header,\n    NavBar\n  },\n}\n\n</script>\n\n<style>\n\nhtml {\n  height: 100vh;\n  scroll-behavior: smooth;\n}\n\nbody {\n  min-height: 100vh;\n  margin: 0;\n  padding: 0;\n}\n\n#app {\n  font-family: Courier, Helvetica, Arial, sans-serif;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  text-align: center;\n  color: #61DAFB;\n  background-color: #080707;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.tooltip {\n  position: relative;\n  display: inline-block;\n  border-bottom: 1px dotted black;\n}\n\n.tooltip .tooltiptext {\n  visibility: hidden;\n  width: 120px;\n  background-color: black;\n  color: #fff;\n  text-align: center;\n  border-radius: 6px;\n  padding: 5px 0;\n  \n  /* Position the tooltip */\n  position: absolute;\n  z-index: 1;\n  bottom: 100%;\n  left: 50%;\n  margin-left: -60px;\n}\n\n.tooltip:hover .tooltiptext {\n  visibility: visible;\n}\n\n@media (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n\n</style>\n","import mod from \"-!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=27e01e1b&\"\nimport script from \"./App.vue?vue&type=script&lang=js&\"\nexport * from \"./App.vue?vue&type=script&lang=js&\"\nimport style0 from \"./App.vue?vue&type=style&index=0&id=27e01e1b&prod&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row pt-4 align-items-md-start justify-content-around\"},[_c('div',{staticClass:\"col-6 pt-4 col-md-3 pt-md-5\",attrs:{\"id\":\"about\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"About Me\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToAbout}},[_c('img',{staticClass:\"rounded img-fluid upper-img\",attrs:{\"src\":require(\"../../public/assets/dylan-n-leo.jpg\")}})])])]),_c('div',{staticClass:\"col-6 pt-4 col-md-3 pt-md-5\",attrs:{\"id\":\"resume\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Resume\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToResume}},[_c('img',{staticClass:\"rounded img-fluid upper-img\",attrs:{\"src\":require(\"../../public/assets/resume.png\")}})])])]),_c('div',{staticClass:\"col-md-4\",attrs:{\"id\":\"artwork-and-projs\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Projects\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToProjects}},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../public/assets/project-collage.png\")}})])]),_c('div',{staticClass:\"img-container mt-4\"},[_c('h4',[_vm._v(\"Artwork\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToArtwork}},[_c('img',{staticClass:\"rounded img-fluid lower-img\",attrs:{\"src\":require(\"../../public/assets/portrait.jpg\")}})])])]),_c('div',{staticClass:\"col-5 pt-5 pt-md-0 col-md-2\",attrs:{\"id\":\"artwork\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Artwork\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToArtwork}},[_c('img',{staticClass:\"rounded img-fluid lower-img\",attrs:{\"src\":require(\"../../public/assets/portrait.jpg\")}})])])]),_c('div',{staticClass:\"col-6 pt-5 pt-md-0 col-md-4\",attrs:{\"id\":\"projects\"}},[_c('div',{staticClass:\"img-container\"},[_c('h4',[_vm._v(\"Projects\")]),_c('div',{staticClass:\"img-holder\",on:{\"click\":_vm.goToProjects}},[_c('img',{staticClass:\"rounded img-fluid lower-img\",attrs:{\"src\":require(\"../../public/assets/project-collage.png\")}})])])])])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"row pt-4 align-items-md-start justify-content-around\">\n    <div id=\"about\" class=\"col-6 pt-4 col-md-3 pt-md-5\">\n      <div class=\"img-container\">\n        <h4>About Me</h4>\n        <div v-on:click=\"goToAbout\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid upper-img\"\n            src=\"../../public/assets/dylan-n-leo.jpg\"\n          />\n        </div>\n      </div>\n    </div>\n\n    <div id=\"resume\" class=\"col-6 pt-4 col-md-3 pt-md-5\">\n      <div class=\"img-container\">\n        <h4>Resume</h4>\n        <div v-on:click=\"goToResume\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid upper-img\"\n            src=\"../../public/assets/resume.png\"\n          />\n        </div>\n      </div>\n    </div>\n    <div id=\"artwork-and-projs\" class=\"col-md-4\">\n        <div class=\"img-container\">\n          <h4>Projects</h4>\n          <div v-on:click=\"goToProjects\" class=\"img-holder\">\n            <img\n              class=\"rounded img-fluid\"\n              src=\"../../public/assets/project-collage.png\"\n            />\n          </div>\n        </div>\n        <div class=\"img-container mt-4\">\n          <h4>Artwork</h4>\n          <div v-on:click=\"goToArtwork\" class=\"img-holder\">\n            <img\n              class=\"rounded img-fluid lower-img\"\n              src=\"../../public/assets/portrait.jpg\"\n            />\n          </div>\n        </div>\n    </div>\n    <div id=\"artwork\" class=\"col-5 pt-5 pt-md-0 col-md-2\">\n      <div class=\"img-container\">\n        <h4>Artwork</h4>\n        <div v-on:click=\"goToArtwork\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid lower-img\"\n            src=\"../../public/assets/portrait.jpg\"\n          />\n        </div>\n      </div>\n    </div>\n    <div id=\"projects\" class=\"col-6 pt-5 pt-md-0 col-md-4\">\n      <div class=\"img-container\">\n        <h4>Projects</h4>\n        <div v-on:click=\"goToProjects\" class=\"img-holder\">\n          <img\n            class=\"rounded img-fluid lower-img\"\n            src=\"../../public/assets/project-collage.png\"\n          />\n        </div>\n      </div>\n    </div>\n  </div>\n</template>\n\n<script>\nexport default {\n  // eslint-disable-next-line\n  name: \"Home\",\n  data() {\n    return {\n      windowWidth: window.innerWidth,\n    };\n  },\n  methods: {\n    goToAbout() {\n      this.$router.push(\"/about\");\n    },\n    goToProjects() {\n      this.$router.push(\"/projects\");\n    },\n    goToArtwork() {\n      this.$router.push(\"/artwork\");\n    },\n    goToResume() {\n      this.$router.push(\"/resume\");\n    },\n  },\n};\n</script>\n\n<style scoped>\n\nh4 {\n  font-size: 4vw;\n}\n\n#artwork-and-projs {\n    display: none;\n}\n\n.img-container:hover {\n  border: 1px solid #61dafb;\n  border-radius: 2px;\n  cursor: pointer;\n  color: white;\n}\n\n@media (max-width: 750px) {\n  .upper-img {\n    height: 40vw;\n    width: auto;\n  }\n\n  .lower-img {\n    height: 30vw;\n    width: auto;\n  }\n}\n\n@media (min-width: 768px) {\n  h4 {\n  font-size: 2vw;\n  }\n\n  #about {\n    order: 1;\n  }\n\n  #resume {\n    order: 3;\n  }\n\n  #artwork-and-projs {\n      display: inline;\n      order: 2;\n  }\n\n  #projects {\n    display: none;\n  }\n\n  #artwork {\n    display: none;\n  }\n\n    @media (min-width: 1150px) { \n        .upper-img {\n          height: 30vw;\n          width: auto;\n      }\n        .lower-img {\n            height: 15vw;\n        }\n    }\n}\n</style>\n","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Home.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Home.vue?vue&type=template&id=06072424&scoped=true&\"\nimport script from \"./Home.vue?vue&type=script&lang=js&\"\nexport * from \"./Home.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Home.vue?vue&type=style&index=0&id=06072424&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"06072424\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row pt-3 pt-lg-5 justify-content-center align-items-center\",attrs:{\"id\":\"home-container\"}},[_vm._m(0),_c('div',{staticClass:\"col-11 col-md-6 col-lg-5 col-xl-5\",attrs:{\"id\":\"about-description\"}},[_c('p',{attrs:{\"id\":\"intro-paragraph\"}},[_vm._v(\" Hey there! I'm a Masters grad from MIT where I studied Computer Science, specifically Artificial Intelligence. My research focused on machine learning tools for assisting crisis managers during climate crises. You can learn more about my research\"),_c('a',{attrs:{\"href\":\"#/projects/ml-for-crowdsourced-crisis-data\"},on:{\"click\":_vm.scrollUp}},[_vm._v(\"here!\")]),_vm._v(\"I'm also a proud dad to my son, Leo  \")]),_vm._m(1),_c('p',[_vm._v(\" In my freetime I love to explore nature, play video games (Zelda/DMC/Soulsborne/Skyrim/Kingdom Hearts/Fallout), cook/bake something new, make and experiment with different forms of art, develop new skills, and go on adventures with my pup! \")])])])\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"col-6 col-md-5 col-lg-5 col-xl-3 mr-xl-5\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"id\":\"leo-and-dylan-pic\",\"src\":require(\"../../public/assets/leo_n_me.jpg\"),\"alt\":\"Leo and Dylan\"}})])\n},function (){var _vm=this,_c=_vm._self._c;return _c('p',[_vm._v(\" I'm interested in Software Engineering, Data Science & Machine Learning, and Web Development! You can contact me through\"),_c('a',{attrs:{\"href\":\"mailto: dylanrl97@gmail.com\"}},[_vm._v(\"email,\")]),_vm._v(\"add me on\"),_c('a',{attrs:{\"href\":\"https://www.linkedin.com/in/dyllew/\",\"target\":\"_blank\"}},[_vm._v(\"LinkedIn,\")]),_vm._v(\"or checkout my\"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/\",\"target\":\"_blank\"}},[_vm._v(\"GitHub!\")]),_vm._v(\" I'm currently looking for remote Data Science and Software Engineering positions! \")])\n}]\n\nexport { render, staticRenderFns }","import { Carousel } from 'bootstrap';\n\nexport const MAIN_PROJECTS = [\n    {   \n        id: \"ml-for-crowdsourced-crisis-data\",\n        link: \"/projects/ml-for-crowdsourced-crisis-data\", \n        src: {imgFilename: \"masters-thesis-overview.png\"},\n        title: \"Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response\", \n        desc: \"Research project focused on developing machine learning methodologies to assist crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"See Thesis Document\",\n            url: \"https://dspace.mit.edu/handle/1721.1/144911\"\n        }\n    },\n    {   \n        id: \"nlp-for-int-dev-gray-lit\",\n        link: \"/projects/nlp-for-int-dev-gray-lit\", \n        src: {imgFilename: \"int-dev-results.png\"},\n        title: \"Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature\", \n        desc: \"NLP project which investigates Named Entity Recognition (NER) and K-means Clustering on a corpus of 244 documents of International Development literature papers for streamlining the evidence synthesis process on international development gray literature.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Presentation PDF\",\n            url: \"./assets/int-dev-gray-lit.pdf\"\n        }\n    },\n    {   \n        id: \"climate-change-news\",\n        link: \"/projects/climate-change-news\", \n        src: {imgFilename: \"final-project-overview.png\"},\n        title: \"Evolution of the U.S. TV News Narrative on Climate Change\", \n        desc: \"Data Science & NLP project in Python that investigated the evolution of climate change coverage frequency & content between popular U.S. TV News Networks CNN, Fox News, and MSNBC over July 2009-January 2020.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Poster PDF\",\n            url: \"./assets/IDS131_Poster.pdf\"\n        }\n    },\n    {   \n        id: \"taxi-pred-img\",\n        link: \"/projects/gnns-taxi-prediction\", \n        src: {imgFilename: \"fare-surge-graph-pred.png\"},\n        title: \"Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction\", \n        desc: \"Machine Learning project in Python which investigated graph neural networks (GNNs) for the tasks of NYC taxi fare and demand surge prediction.\"\n    },\n    {   \n        id: \"trump-img\",\n        link: \"/projects/trump-speech-analysis\", \n        src: {imgFilename: \"FrequencyPlot.png\", sizeClass: 'w-75'},\n        title: \"Trump Campaign Speech Analysis\", \n        desc: \"Data Science project in R which investigated how Donald Trump's 2016 campaign speeches may have influenced public sentiment on a regional level.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Poster PDF\",\n            url: \"./assets/17_835_Poster.pdf\"\n        }\n    },\n    {   \n        id: \"boomerang-img\",\n        link: \"/projects/boomerang\", \n        src: {imgFilename: \"boomerang-home.jpg\"},\n        title: \"Boomerang\",\n        desc: \"Boomerang is a full-stack web application where users can efficiently and reliably borrow items from others within their communities.\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"Go to the Boomerang website\",\n            url: \"https://team-aesthetech-boomerang.herokuapp.com/\"\n        }\n    },\n]\n\nexport const ML_MODULES = [\n    {   \n        id: \"image-module-img\",\n        link: \"/projects/ml-for-crowdsourced-crisis-data/image-analysis-module\", \n        src: {imgFilename: \"image-analysis-module-modified.png\"},\n        title: \"Image Analysis Module\", \n        desc: \"ML Module focused on constructing human-annotated image datasets and assessing the quality of the annotations, developing Convolutional Neural Network (CNN) models to classify the crowdsourced crisis image data for various classification tasks, and conducting image annotation workshops with crisis managers from various contexts for focus-group research / qualitative evaluation.\",\n        projectBtnText: \"See Module Details\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"See Related Code\",\n            url: \"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Image%20Analysis%20Module\"\n        }\n    },\n    {   \n        id: \"text-module-img\",\n        link: \"/projects/ml-for-crowdsourced-crisis-data/text-analysis-module\", \n        src: {imgFilename: \"text-analysis-module.png\"},\n        title: \"Text Analysis Module\", \n        desc: \"ML Module focused on the classification of crowdsourced crisis text (in Japanese (JA)) for assessing Human Risk informed from the identified information needs and priorities of crisis managers and Clustering of crisis text data for uncovering semantic categories in the data for future classification tasks.\",\n        projectBtnText: \"See Module Details\",\n        projectWebsite: {\n            id: \"button-holder\",\n            btnText: \"See Related Code\",\n            url: \"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module\"\n        }\n    }\n]\n\nexport function scrollUpFunc(behavior = 'smooth') {\n    window.scrollTo({\n        top: 0,\n        left: 0,\n        behavior: behavior\n    })\n}\n\nexport function enableSwipeOnCarousel(carouselID) {\n    const carouselElem = document.getElementById(carouselID);\n    const carousel = new Carousel(`#${carouselID}`);\n    carouselElem.addEventListener('slid.bs.carousel', () => {\n        scrollUpFunc();\n    });\n    carouselElem.addEventListener('touchstart', function(event){\n        const xClick = event.touches[0].pageX;\n        const touchMoveHandler = makeTouchMoveHandler(carousel, xClick);\n        carouselElem.addEventListener('touchmove', touchMoveHandler, {once: true});\n        carouselElem.addEventListener('touchend', function(){\n            carouselElem.removeEventListener('touchmove', touchMoveHandler, {once: true});\n        });\n      });\n}\n\nfunction makeTouchMoveHandler(carousel, xClick) {\n    const touchMoveHandler = event => {\n        const xMove = event.touches[0].pageX;\n        const sensitivityInPx = 10;\n        if ( Math.floor(xClick - xMove) > sensitivityInPx ) {\n            carousel.next();\n        }\n        else if ( Math.floor(xClick - xMove) < -sensitivityInPx ) {\n            carousel.prev();\n        }\n    }\n    return touchMoveHandler;\n}\n","<template>\n    <div id=\"home-container\" class=\"row pt-3 pt-lg-5 justify-content-center align-items-center\">\n        <div class=\"col-6 col-md-5 col-lg-5 col-xl-3 mr-xl-5\">\n            <img\n              id=\"leo-and-dylan-pic\"\n              src=\"../../public/assets/leo_n_me.jpg\" \n              class=\"rounded img-fluid\"\n              alt=\"Leo and Dylan\" />\n        </div>\n        <div id=\"about-description\" class=\"col-11 col-md-6 col-lg-5 col-xl-5\">\n            <p id=\"intro-paragraph\">\n              Hey there! I'm a Masters grad from MIT where I studied\n              Computer Science, specifically Artificial Intelligence. My research focused on machine learning tools\n              for assisting crisis managers during climate crises. You can learn more\n              about my research<a href=\"#/projects/ml-for-crowdsourced-crisis-data\" @click=\"scrollUp\">here!</a>I'm also a proud dad to my son, Leo \n            </p>\n            <p>\n              I'm interested in Software Engineering, Data Science & Machine Learning, and Web Development! \n              You can contact me through<a href=\"mailto: dylanrl97@gmail.com\">email,</a>add me on<a href=\"https://www.linkedin.com/in/dyllew/\" target=\"_blank\">LinkedIn,</a>or checkout my<a href=\"https://github.com/dyllew/\" target=\"_blank\">GitHub!</a>\n              I'm currently looking for remote Data Science and Software Engineering positions!\n            </p>\n            <p>\n              In my freetime I love to explore nature, play video games (Zelda/DMC/Soulsborne/Skyrim/Kingdom Hearts/Fallout), cook/bake something new, make and experiment with different forms of art,\n              develop new skills, and go on adventures with my pup!\n            </p> \n        </div>\n    </div>    \n</template>\n\n<script>\nimport { scrollUpFunc } from '../constants';\n\nexport default {\n  // eslint-disable-next-line\n  name: 'About',\n  methods: {\n    scrollUp() {\n      scrollUpFunc();\n    }\n  }\n}\n</script>\n\n<style scoped>\na {\n    color: hotpink;\n    margin-left: 1vw;\n    margin-right: 1vw;\n}\n\na:hover {\n    color: white;\n}\n\n#leo-and-dylan-pic  {\n  max-height: 40vw;\n  border: 0.25vw solid #61DAFB;\n}\n\n#about-description {\n  font-size: 1.75vw;\n  text-align: left;\n}\n\n@media (min-width: 0px) and (max-width: 768px) {\n\n    #leo-and-dylan-pic {\n      max-height: 60vw;\n    }\n\n    #about-description p {\n      font-size: 4vw;\n    }\n\n    #intro-paragraph {\n      padding-top: 10vw;\n    }\n\n}\n\n@media (min-width: 768px) and (max-width: 900px) {\n\n    #leo-and-dylan-pic {\n      max-height: 50vw;\n    }\n\n    #about-description p {\n      font-size: 2vw;\n    }\n\n    #intro-paragraph {\n      padding-top: 5vw;\n    }\n\n}\n\n@media (min-width: 900px) and (max-width: 1200px) {\n\n    #leo-and-dylan-pic {\n      max-height: 40vw;\n    }\n\n    #about-description p {\n      font-size: 1.75vw;\n    }\n}\n\n@media (min-width: 1200px) {\n\n    #about-description p {\n      font-size: 1.35vw;\n    }\n}\n\n</style>\n","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./About.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./About.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./About.vue?vue&type=template&id=29779ff4&scoped=true&\"\nimport script from \"./About.vue?vue&type=script&lang=js&\"\nexport * from \"./About.vue?vue&type=script&lang=js&\"\nimport style0 from \"./About.vue?vue&type=style&index=0&id=29779ff4&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"29779ff4\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},_vm._l((_vm.projects),function(project){return _c('div',{key:project.link,staticClass:\"col-12 col-md-4 col-lg-4 pt-3\"},[_c('ProjectCard',{attrs:{\"project\":project}})],1)}),0)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"card border-info bg-transparent align-items-center\"},[_c('h5',{staticClass:\"card-header\"},[_vm._v(_vm._s(_vm.project.title))]),_c('img',{staticClass:\"card-img-top\",class:_vm.project.src.sizeClass ? _vm.project.src.sizeClass : '',attrs:{\"id\":_vm.project.id,\"src\":_vm.getImgURL(_vm.project.src.imgFilename),\"alt\":\"Project Image\"}}),_c('div',{staticClass:\"card-body\"},[_c('p',{staticClass:\"card-text\"},[_vm._v(_vm._s(_vm.project.desc))]),(_vm.project.projectWebsite)?_c('div',{attrs:{\"id\":_vm.project.projectWebsite.id}},[_c('a',{staticClass:\"btn btn-light col-5\",attrs:{\"href\":_vm.project.projectWebsite.url,\"target\":\"_blank\"}},[_vm._v(_vm._s(_vm.project.projectWebsite.btnText))]),_c('span'),_c('a',{staticClass:\"btn btn-primary text-light col-5\",on:{\"click\":function($event){return _vm.goToProjectPage(_vm.project.link)}}},[_vm._v(_vm._s(_vm.projectBtnText))])]):_c('div',{attrs:{\"id\":\"button-holder\"}},[_c('a',{staticClass:\"btn btn-primary text-light\",on:{\"click\":function($event){return _vm.goToProjectPage(_vm.project.link)}}},[_vm._v(\"See Project Details\")])])])])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"card border-info bg-transparent align-items-center\">\n        <h5 class=\"card-header\">{{project.title}}</h5>\n        <img :id=\"project.id\" \n            class=\"card-img-top\" \n            :class=\"project.src.sizeClass ? project.src.sizeClass : ''\" \n            :src=\"getImgURL(project.src.imgFilename)\" \n            alt=\"Project Image\">\n        <div class=\"card-body\">\n            <p class=\"card-text\">{{project.desc}}</p>\n            <div v-if=\"project.projectWebsite\" :id=\"project.projectWebsite.id\">\n                <a :href=\"project.projectWebsite.url\" target=\"_blank\" class=\"btn btn-light col-5\">{{project.projectWebsite.btnText}}</a>\n                <span></span>\n                <a v-on:click=\"goToProjectPage(project.link)\" class=\"btn btn-primary text-light col-5\">{{projectBtnText}}</a>\n            </div>\n            <div id=\"button-holder\" v-else>\n                <a v-on:click=\"goToProjectPage(project.link)\" class=\"btn btn-primary text-light\">See Project Details</a>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\n\nexport default {\n    name: 'ProjectCard',\n    props: ['project'],\n    methods: {\n        goToProjectPage(projectURL) {\n            this.$router.push(projectURL);\n        },\n        getImgURL(imgFilename) {\n            return require('../../public/assets/' + imgFilename)\n        },\n    },\n    computed: {\n        projectBtnText() {\n           return this.project.projectBtnText ? this.project.projectBtnText : \"See Project Details\"\n        }\n    }\n}\n</script>\n\n<style scoped>\np {\n    text-align: left;\n}\n\nh5 {\n    color: white;\n}\n\n#button-holder {\n    display: flex;\n    justify-content: space-around;\n    align-items: center;\n}\n\n#image-module-img {\n    width: 40vh;\n    height: 40vh;\n}\n\n#text-module-img {\n    height: 40vh;\n}\n\n@media (max-width: 500px) {\n    #text-module-img {\n        max-height: 50vw;\n    }\n}\n\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./ProjectCard.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./ProjectCard.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./ProjectCard.vue?vue&type=template&id=a1454394&scoped=true&\"\nimport script from \"./ProjectCard.vue?vue&type=script&lang=js&\"\nexport * from \"./ProjectCard.vue?vue&type=script&lang=js&\"\nimport style0 from \"./ProjectCard.vue?vue&type=style&index=0&id=a1454394&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"a1454394\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div class=\"row align-items-center justify-content-center\">\n    <div class=\"col-12 col-md-4 col-lg-4 pt-3\" v-for=\"project in projects\" :key=\"project.link\">\n        <ProjectCard :project=\"project\"></ProjectCard>\n    </div>\n  </div>    \n</template>\n\n<script>\nimport ProjectCard from './ProjectCard'\nimport { MAIN_PROJECTS } from '../constants';\n\nexport default {\n  // eslint-disable-next-line\n  name: 'Projects',\n  components: {\n    ProjectCard,\n  },\n  data() {\n      return { \n          projects: MAIN_PROJECTS\n      }\n  },\n}\n</script>\n\n","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Projects.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Projects.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Projects.vue?vue&type=template&id=2b8499df&\"\nimport script from \"./Projects.vue?vue&type=script&lang=js&\"\nexport * from \"./Projects.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"artwork row align-items-center justify-content-center justify-content-xl-around\"},[_c('div',{staticClass:\"col-8 pt-3 pt-md-0 col-md-4\"},[_c('h4',{staticClass:\"header\"},[_vm._v(\"Ethereality\")]),_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../public/assets/portrait.jpg\")}})]),_c('div',{staticClass:\"col-8 pt-4 pt-md-2 col-md-4\"},[_c('h4',{staticClass:\"header\"},[_vm._v(\"Phantom Dragon\")]),_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"src\":require(\"../../public/assets/reptile.png\")}})])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"artwork row align-items-center justify-content-center justify-content-xl-around\">\n    <div class=\"col-8 pt-3 pt-md-0 col-md-4\">\n      <h4 class=\"header\">Ethereality</h4>\n      <img\n        class=\"rounded img-fluid\"\n        src=\"../../public/assets/portrait.jpg\"\n      />\n    </div>\n    <div class=\"col-8 pt-4 pt-md-2 col-md-4\">\n        <h4 class=\"header\">Phantom Dragon</h4>\n        <img\n          class=\"rounded img-fluid\"\n          src=\"../../public/assets/reptile.png\"\n        />\n    </div>\n    <!-- <div class=\"col-8 pt-4 pt-md-2 col-md-3\">\n        <h4></h4>\n        <img\n          class=\"rounded img-fluid\"\n          src=\"../../public/assets/feelings.jpg\"\n        />\n    </div> -->\n  </div>    \n</template>\n\n<script>\n\nexport default {\n  // eslint-disable-next-line\n  name: 'Artwork'\n}\n</script>\n\n<style>\n\n.header {\n  color: white;\n}\n\n</style>\n","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Artwork.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Artwork.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Artwork.vue?vue&type=template&id=2297dd18&\"\nimport script from \"./Artwork.vue?vue&type=script&lang=js&\"\nexport * from \"./Artwork.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Artwork.vue?vue&type=style&index=0&id=2297dd18&prod&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"resume row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 mt-4\",attrs:{\"id\":\"sticky-btn\"}},[_c('a',{staticClass:\"btn btn-primary text-light\",attrs:{\"target\":\"_blank\",\"href\":\"./assets/Dylan_Lewis_Resume.pdf\"}},[_vm._v(\"View PDF in Tab\")])]),_c('div',{staticClass:\"col-12 mt-2\"},[_c('embed',{staticClass:\"pdf\",attrs:{\"src\":\"./assets/Dylan_Lewis_Resume.pdf\"}})])])\n}]\n\nexport { render, staticRenderFns }","<template>\n  <div class=\"resume row align-items-center justify-content-center\">\n    <div id=\"sticky-btn\" class=\"col-12 mt-4\">\n      <a target=\"_blank\" class=\"btn btn-primary text-light\" href=\"./assets/Dylan_Lewis_Resume.pdf\">View PDF in Tab</a>\n    </div>\n    <div class=\"col-12 mt-2\">\n      <embed class=\"pdf\" src=\"./assets/Dylan_Lewis_Resume.pdf\"/>\n    </div>\n  </div>    \n</template>\n\n<script>\n\nexport default {\n  // eslint-disable-next-line\n  name: 'Resume'\n}\n</script>\n\n<style scoped>\n#sticky-btn {\n  position: -webkit-sticky;\n  position: sticky;\n  top: 0;\n}\n\na {\n  color: #61DAFB;\n  font-size: 30px;\n}\n\na:hover {\n  color: #61DAFB;\n}\n\n@media (max-width: 700px) {\n  .pdf {\n    display: none;\n  }\n}\n\n@media  (min-width: 700px) and (max-width: 1200px) {\n  .pdf {\n    width: 600px;\n    height: 600px;\n  }\n}\n\n@media (min-width: 1200px) {\n  .pdf {\n    width: 800px;\n    height: 800px;\n  }\n}\n\n</style>\n","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Resume.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Resume.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Resume.vue?vue&type=template&id=35df9bb0&scoped=true&\"\nimport script from \"./Resume.vue?vue&type=script&lang=js&\"\nexport * from \"./Resume.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Resume.vue?vue&type=style&index=0&id=35df9bb0&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"35df9bb0\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_vm._m(0),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"MLForCrowdsourcedCrisisDataCarousel\"}},[_vm._m(1),_c('div',{staticClass:\"carousel-inner\"},[_vm._m(2),_vm._m(3),_vm._m(4),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_vm._m(5),_vm._l((_vm.modules),function(module){return _c('div',{key:module.link,staticClass:\"col-12 col-md-6 col-lg-6 pt-3\"},[_c('ProjectCard',{attrs:{\"project\":module}})],1)}),_vm._m(6)],2)]),_vm._m(7),_vm._m(8),_vm._m(9),_vm._m(10),_vm._m(11),_vm._m(12)])])])])\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"col-12 col-md-8\"},[_c('h3',[_vm._v(\"Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response\")])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"5\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"6\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"7\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"8\"}}),_c('li',{attrs:{\"data-bs-target\":\"#MLForCrowdsourcedCrisisDataCarousel\",\"data-bs-slide-to\":\"9\"}})])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item active cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12\"},[_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"id\":\"overview-pic\",\"src\":require(\"../../../../public/assets/masters-thesis-overview.png\"),\"alt\":\"First slide\"}})]),_c('div',{staticClass:\"col-12 col-md-9 pb-4\"},[_c('h5',[_vm._v(\"Thesis Document & Code and Related Open-source Python Packages\")]),_c('p',[_vm._v(\" This was a project I worked on as part of my thesis that I wrote as a research assistant at the Urban Risk Lab (URL) at MIT for my Master of Engineering in Electrical Engineering and Computer Science degree. This research investigated the development of machine learning models for assisting crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response. This presentation briefly introduces the motivation behind this work, explains how the investigation unfolded (i.e. methodology), and discusses the main findings and implications from the research for the broader crisis informatics community. Added bonus at the end featuring an overview of adjacent work done conducted by undergrads that I supervised during my time on this project. \"),_c('br'),_c('br'),_vm._v(\" The published thesis document can be found \"),_c('a',{attrs:{\"href\":\"https://dspace.mit.edu/handle/1721.1/144911\",\"target\":\"_blank\"}},[_vm._v(\"here.\")]),_c('br'),_c('br'),_vm._v(\" The code for this thesis was written in Python as packages we developed as well as Jupyter Notebooks. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\",\"target\":\"_blank\"}},[_vm._v(\"Here's the GitHub Repo.\")]),_c('br'),_vm._v(\" With this thesis, we produced two open-source Python packages to better enable readability, reusability, and reproducibility of the computational utilities derived for performing various experiments and analysis on crowdsourced crisis image and text data: \"),_c('ul',[_c('li',[_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-image-module/0.27.0/\",\"target\":\"_blank\"}},[_vm._v(\"URL Image Module\")]),_vm._v(\" - Utilities for training, testing, and predicting with pretrained Convolutional Neural Networks for classifying crowdsourced crisis image data and constructing & analyzing annotated datasets\")]),_c('li',[_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-text-module/0.6.1/\",\"target\":\"_blank\"}},[_vm._v(\"URL Text Module\")]),_vm._v(\" - Utilities for featurizing crisis text data, training and testing with a variety of classification machine learning models, and visualizing clusters of featurized text data\")])]),_c('br'),_vm._v(\" This research was supported by a grant from Google.org and the Tides Foundation. \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-5\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_c('strong',[_vm._v(\"Abstract\")])]),_c('p',[_vm._v(\" The availability of information during a climate crisis event is critical for crisis managers to assess and respond to crisis impact. During crisis events, affected residents post real-time crisis updates on platforms such as \"),_c('a',{attrs:{\"href\":\"https://riskmap.mit.edu/japan.html\",\"target\":\"_blank\"}},[_vm._v(\"RiskMap\")]),_vm._v(\" and \"),_c('a',{attrs:{\"href\":\"https://twitter.com/?lang=en\",\"target\":\"_blank\"}},[_vm._v(\"Twitter\")]),_vm._v(\". These updates provide localized information, which has the potential to enhance crisis awareness and response. However, with limited resources, crisis managers may endure information overload from the inundation of these updates. Prior work has demonstrated the potential of machine learning (ML) methodologies to mitigate this problem. We have identified limitations in the prior work including the lack of involvement of crisis managers in the development and evaluation of a ML methodology. \")]),_c('p',[_vm._v(\" To address these limitations, we propose a novel framework and ML methodology which investigate the efficacy of various ML methods in enhancing crisis awareness and response beyond model performance metrics. This framework aims to iteratively embed the information needs and priorities of crisis managers during crisis into the design of the ML methodology. We cooperated with crisis managers in Fukuchiyama City (FC), a city in Japan which is susceptible to flood events, and analyzed crowdsourced crisis image and text data from past FC flood events. We devised the Flood Presence image classification task, constructed Train/Dev/Test splits, and annotated images from FC. We report a weighted F1 score of 92.1% on the test split and 82.5% on the FC images. Using the results of our image analysis ML methodology and the insights we gained from crisis managers, we iterated on the design of our text analysis ML methodology. This led to the creation of the Human Risk text classification task which is tailored to a subset of the identified information needs of the crisis managers. To align with the priorities of crisis managers for this task, we determined the model evaluation metric to be the F2 score. We report an F2 score of 92.8% on an FC crisis text test dataset, which is a significant improvement over the baseline score of 43.4%. \")]),_c('h5',[_c('strong',[_vm._v(\"Research Question\")])]),_c('strong',{attrs:{\"id\":\"research-question\"}},[_vm._v(\" In collaborating with crisis managers, how can machine learning methods be utilized and evaluated to effectively reduce the information overload of crowdsourced data on crisis managers during flood events for enhanced crisis awareness and response? \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-5\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Main Research Aims\")]),_c('p',{attrs:{\"id\":\"research-aims\"}},[_c('ol',[_c('li',[_c('strong',[_vm._v(\"To reduce information overload during a crisis\")]),_vm._v(\" using accurate, efficient, automated image and text classification of crisis reports by machine learning (ML) models during crisis.\")]),_c('li',[_c('strong',[_vm._v(\"To embed tacit knowledge, information needs, and decision-making priorities of crisis managers\")]),_vm._v(\" into the designed ML methodology.\")]),_c('li',[_c('strong',[_vm._v(\"To evaluate methodology in collaboration with crisis managers\")]),_vm._v(\", e.g. crisis managers in Fukuchiyama, Japan.\")]),_c('li',[_c('strong',[_vm._v(\"To incorporate evaluation results and iterate on the ML methodology\")]),_vm._v(\" in order to better reach aim of using AI to assist crisis managers during crisis.\")])]),_vm._v(\" We investigate various ML techniques that can be applied to mitigate information overload for crisis managers during crisis events while also assessing if those techniques can satisfy the information needs and priorities of crisis managers through qualitative and quantitative evaluation. \")]),_c('h5',[_vm._v(\"Novel Framework\")]),_c('p',[_vm._v(\" To achieve the aims above we develop a novel framework in the crisis informatics community consisting of the following components: \"),_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"Classification Task Creation\")])]),_vm._v(\" - We develop new classification tasks using labels provided to us by crisis managers and labels present in open-source datasets\")]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Data Annotation Procedure\")])]),_vm._v(\" - We open-souce the annotation guide we develop for greater transparency into the procurement of human-annotated data that is used to train and evaluate ML models\")]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Interannotator Agreement/Data Reliability Analysis\")])]),_vm._v(\" - After performing an annotation effort on data provided by crisis managers, we analyze the quality of the annotations through interannotator agreement analysis to demonstrate the importance of understanding data quality prior to using it for ML purposes\")]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Model Development and Evaluation; Per-Class Performance Analysis\")])]),_vm._v(\" - The metrics we used to evaluate models are derived either from metrics reported in the literature or, more notably, \"),_c('strong',[_vm._v(\"metrics determined from insights provided by crisis managers directly\")]),_vm._v(\". Additionally, we consider issues of class imbalance and report per-class performance and confusion matrices to provide more granular insight into model performance. Finally, we establish baselines to compare against the models we develop to assess the degree to which the models we develop outperform the baseline\")]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Qualitative Analysis through Workshops with Crisis Managers\")])]),_vm._v(\" - We sought to cooperate with crisis managers with the intent of using this framework to iteratively design ML systems, such as the specific classification tasks performed by the models and their associated performance metric(s), by iteratively incorporating feedback and insights from crisis managers, so that the system better aligns with their information needs and decision-making priorities\")])]),_vm._v(\" This framework situates \"),_c('i',[_vm._v(\"Model Development and Evaluation\")]),_vm._v(\", which is commonplace in prior work, as part of a larger, contextualized analysis. On that note, we expand on the notion of \"),_c('i',[_vm._v(\"Model Development and Evaluation\")]),_vm._v(\" beyond what is typically seen in the prior work. \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Machine Learning Methodology\")]),_vm._v(\" With crisis management partners in Fukuchiyama (FC), Japan, we present our framework through two ML modules:\"),_c('br'),_vm._v(\" Image Analysis Module & Text Analysis Module\"),_c('br'),_c('br'),_c('p',[_vm._v(\" We note that our findings from the Image Analysis Module influenced the design of the Text Analysis Module in order to meet our aim of developing machine learning systems for crisis management which iteratively incorporate the feedback received from crisis managers. \")]),_c('p',[_vm._v(\" The presentations below describe details of the ML methodologies we developed and their associated results and discussions. \")])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"col-12 col-md-9 pt-4\"},[_c('p',[_vm._v(\" In the next slides, we discuss the important implications of this study overall and summarize our contributions to the field of Crisis Informatics. \")])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h4',[_vm._v(\"Discussion & Implications of Study\")]),_c('h5',[_vm._v(\"Involvement of Crisis Managers in the development & iteration of ML methodology\")]),_c('p',[_vm._v(\" While crisis classification tasks have been published in the literature, to the best of our knowledge, \"),_c('strong',[_vm._v(\"this work is the first of its kind to both involve & incorporate the insights of crisis managers into an ML methodology\")]),_vm._v(\" to reduce information overload during crisis. This is best seen from the image annotation workshops we held with EOC and the incorporation of those results into our text analysis ML methodology. \")]),_c('h5',[_vm._v(\"Investigation of Non-English Crisis Text \")]),_c('p',[_vm._v(\" While we have developed the infrastructure to develop machine learning text models for any language, \"),_c('strong',[_c('u',[_vm._v(\"we underscore our investigation of Japanese crisis text\")]),_vm._v(\" as prior work has focused on applying NLP techniques to \"),_c('u',[_vm._v(\"predominantly English crisis text.\")])])]),_c('h5',[_vm._v(\"Contextualized Framework towards better human-centered design of an AI system\")]),_c('p',[_c('strong',[_vm._v(\"Our novel framework contextualizes model performance (e.g. F1, F2, precision, etc.) as part of a broad approach to assessing the efficacy of using ML in reducing information overload.\")]),_vm._v(\" That is, we consider other important aspects in the design of our system in order to better serve those we aim to assist, crisis managers. We investigate classification task creation, data quality & interannotator agreement analysis, issues of class imbalance, involving crisis managers in the development process, and the determination of a performance metric & corresponding performance baselines \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h4',[_vm._v(\"Summary of Main Contribution\")]),_c('p',[_vm._v(\" With this thesis, we have introduced and exemplified a framework which aims to embed all of the mentioned considerations into the process of designing, developing, and iterating on a ML methodology for enhancing crisis awareness and response. \"),_c('strong',[_vm._v(\"Most notably, this framework situates model development and evaluation, which is commonplace in prior work, as one piece of a broader, contextualized understanding of the efficacy ML methodologies can have for crisis managers in mitigating information overload from crowdsourced crisis reports.\")]),_vm._v(\" Additionally, this framework \"),_c('strong',[_vm._v(\"promotes the iterative development of AI systems\")]),_vm._v(\" which is informed \"),_c('strong',[_vm._v(\"from the insights gained from engaging with crisis managers, aiming to address this gap in prior work.\")])]),_c('p',[_vm._v(\" This framework is only the beginning for similar work in this domain, as the development of AI systems and ML methodologies for mitigating information overload of crisis managers has many complex intricacies for which we only scratch the surface in our attempt to broaden this discussion within the crisis informatics field. \"),_c('strong',[_vm._v(\"We contribute this framework and the full exhibition of the principles and analyses contained within it to work closer towards a goal worth striving for: enhanced crisis awareness and response from automated assessment of crowdsourced crisis reporting.\")])]),_c('p',[_vm._v(\" In the next slides, we further detail the specifics of the other contributions of this thesis. \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h4',[_vm._v(\"Contributions 1/3\")]),_c('h5',[_vm._v(\"Open-source Python Packages/Code\")]),_c('p',[_vm._v(\" To ensure the reuse of the analysis conducted in this work, we release two open-source Python packages: One for the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-image-module/0.27.0/\",\"target\":\"_blank\"}},[_vm._v(\"Image Analysis Module\")]),_vm._v(\" and the other for the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/url-text-module/0.6.1/\",\"target\":\"_blank\"}},[_vm._v(\"Text Analysis Module\")]),_vm._v(\". These packages include utilities for training, testing, and prediction using the models presented in this work, computing statistics for interannotator agreement, and computing metrics for model performance. For text data, there are also utilities for featurization of text, performing nested cross validation, and clustering. We note that there are utilities for plotting such as methods for producing the plots shown throughout this project. \")]),_c('p',[_vm._v(\" In addition to these packages, we release an \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\",\"target\":\"_blank\"}},[_vm._v(\"open-source repository\")]),_vm._v(\" containing Jupyter notebooks, relevant documents (e.g. the mentioned annotation guide), and other code required to reproduce the experiments and reuse the analysis conducted in this work. \")]),_c('h5',[_vm._v(\"Flood Presence Task Creation, Labeled Image Dataset, and Performance Benchmark\")]),_c('p',[_vm._v(\" Since we focused on flood crises, we defined the image prediction task of Flood Presence classification. The Flood Presence task is the binary classification task of determining whether or not there is presence of flood in an image. We construct a labeled dataset for the task consisting of approx. 23.6 images by combining various open-source datasets which were labeled with labels useful for this task, although they were originally developed for other adjacent tasks. We contribute this dataset for further research in crisis informatics. In addition, we provide Train/Dev/Test splits and an associated benchmark performance on the test split using a state-of-the-art Convolutional Neural Network (CNN) model, EfficientNet-B1. \")]),_c('h5',[_vm._v(\"Data Annotation Procedure and Analysis of Interannotator Agreement\")]),_c('p',[_vm._v(\" We developed a procedure for annotating images in order to label the unlabeled image data provided to us by our partners in Fukuchiyama, Japan. This procedure included creating an annotation guide to assist annotators in their labeling. This guide included the name of each task, the names of the mutually-exclusive classes associated with each task, and an associated description and an example image for each class. We then had annotators from the Urban Risk Lab independently annotate the images using this guide. \")]),_c('p',[_vm._v(\" After the annotation effort was completed, we were able analyze the interannotator agreement between the annotators and construct ground-truth labels for these images. We computed interannotator reliability statistics to get a sense of how reproducible the annotation procedure was for each task as well as to have transparency of the data quality prior to using it for ML purposes. Finally, we created ground-truth datasets using these labels for the Fukuchiyama images to use in evaluating the image classification models we developed. \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h4',[_vm._v(\"Contributions 2/3\")]),_c('h5',[_vm._v(\"Classification and Clustering of Crowdsourced Japanese Crisis Text\")]),_c('p',[_vm._v(\" Research in crisis informatics on crowdsourced crisis data focuses mostly on English, and thus research on crowdsourced Japanese crisis text is sparse. The Text Analysis Module developed in this work focused exclusively on Japanese text data. We explored various numerical representations of the Japanese crisis text reports provided by our partners and developed a pipeline for preprocessing the raw text and producing the numerical representations. Additionally, our partners provided labels along with the text reports, so we experimented with classifying the text. Lastly, we explored the text data using unsupervised learning, specifically, we employed clustering methods to help inform development of classification tasks in future work \")]),_c('h6',[_vm._v(\"Pipeline for Japanese Crisis Text Preprocessing, Tokenization, and Featurization\")]),_c('p',[_vm._v(\" In order to use the text reports as input to the various ML models employed in this work, we represent the raw text string of each report as a numerical vector, or featurization. Depending on the featurization we choose to use for a text report, we may first preprocess the text. This preprocessing included various steps including tokenization, stopword removal, and lemmatization, which we performed using open-source software (i.e. tokenizer and lemmatizer) and publicly available data (i.e. stopwords list) for the Japanese language. We provide a pipeline for preprocessing and performing the following featurizations on Japanese text: \"),_c('ul',[_c('li',[_vm._v(\"n-gram Bag-of-Words (BOW)\")]),_c('li',[_vm._v(\"n-gram Term Frequency-Inverse Document Frequency (TF-IDF)\")]),_c('li',[_vm._v(\"Pretrained Japanese Masked Language Modeling (MLM) BERT Model with Classification (CLS) Pooling Embedding\")])]),_vm._v(\" The resultant feature vectors representing the text enabled us to use them as input to ML models. Thus, we can then employ classification and clustering techniques on the text data. \")]),_c('h6',[_vm._v(\"Human Risk Task Creation and Performance Metric Determination\")]),_c('p',[_vm._v(\" We devised a new text classification task, Human Risk classification. The human risk text classification task determines whether or not a crisis text report indicates if there are people in need of rescue from a crisis. This includes people being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), and/or being in need of life-saving emergency medical care. This classification task was unique among the classification tasks presented in this work because it was devised using labels that came with the text reports given to us by our crisis management partners. Relatedly, we determine the metric to use in model performance evaluation using the insights we gained from our partners. \")]),_c('h6',[_vm._v(\"Exploratory Analysis of Japanese Crisis Text using Unsupervised Learning\")]),_c('p',[_vm._v(\" With the intention of finding cohesive groupings within the Japanese crisis text corpus, which can inform the development of future text classification tasks, we devise a pipeline for featurizing Japanese crisis text, reducing the high-dimensional text feature vector to 2 dimensions, and clustering the data. We evaluate this pipeline both quantitatively and qualitatively, experimenting with various text featurizations including unigram TF-IDF features and pretrained Japanese MLM BERT with CLS Pooling embeddings mentioned above, t-Distributed Stochastic Neighbor Embedding (t-SNE) and Principle Component Analysis (PCA) for dimensionality reduction, and finally K-means and K-medoids for the algorithm which clusters the data. \")]),_c('p',[_vm._v(\" After we determine the optimal combination of text embedding, dimensionality reduction technique, and clustering algorithm, we create brief summaries of each cluster using the unigrams with highest TF-IDF score for each cluster and the closest reports (by euclidean distance) within each cluster to the cluster center to help in the determination of a human-interpretable label for each cluster. Lastly, a member of the Urban Risk Lab at MIT who is fluent in Japanese used these summaries to determine an interpretable label to accompany each cluster found. We thus provide various labels which can be used for classification experiments and analysis in a future work. \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h4',[_vm._v(\"Contributions 3/3\")]),_c('h6',[_vm._v(\"Quantitative and Qualitative Evaluation in Japanese Flood Crisis Context\")]),_c('p',[_vm._v(\" Prior work has typically evaluated ML methods using quantitative measures, mainly classification performance metrics, e.g. accuracy, precision, recall, F1, and AUROC (Area Under the Receiver Operating Characteristic Curve) and their macro and micro variants. However, \"),_c('strong',[_vm._v(\"with the framework we present in this thesis, we aimed to expand the evaluation of the efficacy ML models have in reducing information overload to not only include similar quantitative measures mentioned above, but also qualitative evaluation derived from engaging with our crisis management partners.\")]),_vm._v(\" Beyond having good performance, we hoped to use the qualitative evaluation used in this study to gain a broader understanding of the efficacy a model can provide crisis managers in mitigating information overload and gaining situational awareness. \")]),_c('p',[_vm._v(\" We held image annotation workshops with various crisis managers and aimed to understand what type of information they seek to gain from a crowdsourced image during a flood crisis event. With their insights, we began to understand how our models can be refined or improved, or how new models can be created in order to better serve the information needs of crisis managers more effectively, such as by tailoring the labels and their associated meanings to the information needs of crisis managers suggested from their annotations. Additionally, we gained more insight into the appropriate metrics to use when evaluating models based on the priorities of crisis managers as it relates to the task. From these workshops, we share key lessons that can influence the design of this framework and AI-augmented crisis information systems of the future. In fact, within this work, we used the lessons learned from the image analysis workshops to assist us in determining the performance metric to use when developing models for the human risk text classification task. This exercise exhibited the principle of iterative development our framework intends to promote. \")])])])])\n},function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h4',[_vm._v(\"Adjacent Projects\")]),_c('img',{staticClass:\"rounded img-fluid\",attrs:{\"id\":\"overview-pic\",\"src\":require(\"../../../../public/assets/adjacent-projects.png\"),\"alt\":\"First slide\"}}),_c('p',[_vm._v(\" During my time on this project, I defined and mentored undergraduate research projects that were adjacent to my research. These projects included novel topics in the field of Crisis Informatics such as: \"),_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"Interpretability in Image and Text Classification Models:\")])]),_c('p',[_vm._v(\" Investigated interpretability algorithms such as GradCAM (Class Activation Mapping) on image classification models used to classify crisis report images and Local Interpretable Model-Agnostic Explaination (LIME) on text classification models used to classify report text in an effort to increase the interpretability of the models which would be employed during crisis and potentially improve model performance through relabeling & retraining. \")])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Multilabel Image Classification:\")])]),_c('p',[_vm._v(\" Investigated CNNs for image classification tasks which can perform multilabel classification (as opposed to single label classification) in the crisis informatics context as most classification tasks in the literature were single-label. \")])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Towards Establishing Interannotator Agreement Standards & Tools in the Crisis Informatics Community:\")])]),_c('p',[_vm._v(\" Investigated the establishment of standards and analysis tools for understanding interannotator agreement (and disagreement) on human-annotated datasets in the crisis informatics community for both single-label and multilabel classification tasks as such standards and tools did not exist in the crisis informatics community. \")])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Development of a Crisis Management Dashboard and Simulations using ML Models and their Predictions:\")])]),_c('p',[_vm._v(\" Investigated the development of an interactive dashboard for crisis managers to use during a crisis event that visualizes the predictions of various machine learning models on a map to provide situational summarization on individual report and aggregate report levels. A simulation was constructed using past citizen crisis reports and predictions on those reports by trained machine learning models. \")])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12 col-md-8\">  \n            <h3>Towards Automated Assessment of Crowdsourced Crisis Reporting for Enhanced Crisis Awareness and Response</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"MLForCrowdsourcedCrisisDataCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"2\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"3\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"4\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"5\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"6\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"7\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"8\"></li>\n                    <li data-bs-target=\"#MLForCrowdsourcedCrisisDataCarousel\" data-bs-slide-to=\"9\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-4\">\n                            <div class=\"col-12\">\n                                <img class=\"rounded img-fluid\" id=\"overview-pic\" src=\"../../../../public/assets/masters-thesis-overview.png\" alt=\"First slide\">\n                            </div>\n                            <div class=\"col-12 col-md-9 pb-4\">\n                                <h5>Thesis Document & Code and Related Open-source Python Packages</h5>\n                                <p>\n                                    This was a project I worked on as part of my thesis that I wrote as a research assistant at the Urban Risk Lab (URL) at MIT for my Master of Engineering in Electrical Engineering and Computer Science degree. This research investigated \n                                    the development of machine learning models for assisting crisis managers in gaining situational awareness from crowdsourced crisis data for enhanced crisis response.\n                                    This presentation briefly introduces the motivation behind this work, explains how the investigation unfolded (i.e. methodology), and discusses the main findings and implications from the research for the broader \n                                    crisis informatics community. \n                                    Added bonus at the end featuring an overview of adjacent work done conducted by undergrads that I supervised during my time on this\n                                    project.\n                                    <br>\n                                    <br>\n                                    The published thesis document can be found <a href=\"https://dspace.mit.edu/handle/1721.1/144911\" target=\"_blank\">here.</a>\n                                    <br>\n                                    <br>\n                                    The code for this thesis was written in Python as packages we developed as well as Jupyter Notebooks. <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\" target=\"_blank\">Here's the GitHub Repo.</a>\n                                    <br>\n                                    With this thesis, we produced two open-source Python packages to better enable readability, reusability, and reproducibility of the computational utilities derived \n                                    for performing various experiments and analysis on crowdsourced crisis image and text data:\n                                    <ul>\n                                        <li><a href=\"https://pypi.org/project/url-image-module/0.27.0/\" target=\"_blank\">URL Image Module</a> - Utilities for training, testing, and predicting with pretrained Convolutional Neural Networks for classifying\n                                            crowdsourced crisis image data and constructing & analyzing annotated datasets</li>\n                                        <li><a href=\"https://pypi.org/project/url-text-module/0.6.1/\" target=\"_blank\">URL Text Module</a> - Utilities for featurizing crisis text data, training and testing with a variety of classification machine learning models, \n                                            and visualizing clusters of featurized text data</li>\n                                    </ul>\n                                    <br>\n                                    This research was supported by a grant from Google.org and the Tides Foundation.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-5\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5><strong>Abstract</strong></h5>\n                                <p>\n                                    The availability of information during a climate crisis event is critical for crisis managers to assess and respond to crisis impact. \n                                    During crisis events, affected residents post real-time crisis updates on platforms such as <a href=\"https://riskmap.mit.edu/japan.html\" target=\"_blank\">RiskMap</a> and <a href=\"https://twitter.com/?lang=en\" target=\"_blank\">Twitter</a>. \n                                    These updates provide localized information, which has the potential to enhance crisis awareness and response. \n                                    However, with limited resources, crisis managers may endure information overload from the inundation of these updates. \n                                    Prior work has demonstrated the potential of machine learning (ML) methodologies to mitigate this problem. \n                                    We have identified limitations in the prior work including the lack of involvement of crisis managers in the development and evaluation of a ML methodology.\n                                </p>\n                                <p>\n                                    To address these limitations, we propose a novel framework and ML methodology which investigate the efficacy of various ML methods in enhancing crisis awareness \n                                    and response beyond model performance metrics. This framework aims to iteratively embed the information needs and priorities of crisis managers during crisis \n                                    into the design of the ML methodology. We cooperated with crisis managers in Fukuchiyama City (FC), a city in Japan which is susceptible to flood events, and \n                                    analyzed crowdsourced crisis image and text data from past FC flood events. We devised the Flood Presence image classification task, constructed Train/Dev/Test splits, \n                                    and annotated images from FC. We report a weighted F1 score of 92.1% on the test split and 82.5% on the FC images. Using the results of our image analysis \n                                    ML methodology and the insights we gained from crisis managers, we iterated on the design of our text analysis ML methodology. This led to the creation of the \n                                    Human Risk text classification task which is tailored to a subset of the identified information needs of the crisis managers. \n                                    To align with the priorities of crisis managers for this task, we determined the model evaluation metric to be the F2 score. \n                                    We report an F2 score of 92.8% on an FC crisis text test dataset, which is a significant improvement over the baseline score of 43.4%. \n                                </p>\n                                <h5><strong>Research Question</strong></h5>\n                                <strong id=\"research-question\">\n                                    In collaborating with crisis managers, how can machine learning methods be utilized and evaluated to effectively reduce the information overload of crowdsourced data on\n                                    crisis managers during flood events for enhanced crisis awareness and response?\n                                </strong>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-5\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Main Research Aims</h5>\n                                <p id=\"research-aims\">\n                                    <ol>\n                                        <li><strong>To reduce information overload during a crisis</strong> using accurate, efficient, automated image and text classification of crisis reports by machine learning (ML) models during crisis.</li>\n                                        <li><strong>To embed tacit knowledge, information needs, and decision-making priorities of crisis managers</strong> into the designed ML methodology.</li>\n                                        <li><strong>To evaluate methodology in collaboration with crisis managers</strong>, e.g. crisis managers in Fukuchiyama, Japan.</li>\n                                        <li><strong>To incorporate evaluation results and iterate on the ML methodology</strong> in order to better reach aim of using AI to assist crisis managers during crisis.</li>\n                                    </ol>\n                                    We investigate various ML techniques\n                                    that can be applied to mitigate information overload for crisis managers during crisis\n                                    events while also assessing if those techniques can satisfy the information needs and\n                                    priorities of crisis managers through qualitative and quantitative evaluation.\n                                </p>\n                                <h5>Novel Framework</h5>\n                                <p>\n                                    To achieve the aims above we develop a novel framework in the crisis informatics community consisting of the following components:\n                                    <ul>\n                                        <li><strong><u>Classification Task Creation</u></strong> - We develop new classification tasks using labels provided to us by crisis managers\n                                            and labels present in open-source datasets</li>\n                                        <li><strong><u>Data Annotation Procedure</u></strong> - We open-souce the annotation guide we develop for greater transparency into the procurement of human-annotated data\n                                            that is used to train and evaluate ML models</li>\n                                        <li><strong><u>Interannotator Agreement/Data Reliability Analysis</u></strong> - After performing an annotation effort on data provided by crisis managers, we analyze the quality\n                                            of the annotations through interannotator agreement analysis to demonstrate the importance of understanding data quality prior to using it for ML purposes</li>\n                                        <li><strong><u>Model Development and Evaluation; Per-Class Performance Analysis</u></strong> - The metrics we used to evaluate models are derived either from metrics reported in the literature or, more notably, <strong>metrics determined from insights \n                                            provided by crisis managers directly</strong>. Additionally, we consider issues of class imbalance and report per-class performance and confusion matrices to provide more granular insight\n                                            into model performance. Finally, we establish baselines to compare against the models we develop to assess the degree to which the models we develop outperform the baseline</li>\n                                        <li><strong><u>Qualitative Analysis through Workshops with Crisis Managers</u></strong> - We sought to cooperate with crisis managers with the intent of using this framework to\n                                            iteratively design ML systems, such as the specific classification tasks performed by the models and their associated performance metric(s), by iteratively incorporating feedback and insights from\n                                            crisis managers, so that the system better aligns with their information needs and decision-making priorities</li>\n                                    </ul>\n                                    This framework situates <i>Model Development and Evaluation</i>, which is commonplace in prior work, as part of a larger, contextualized analysis. On that note, we\n                                    expand on the notion of <i>Model Development and Evaluation</i> beyond what is typically seen in the prior work.\n                                </p>\n                            </div>   \n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Machine Learning Methodology</h5>\n                                    With crisis management partners in Fukuchiyama (FC), Japan, we present our framework through two ML modules:<br> Image Analysis Module & Text Analysis Module<br>\n                                <br>\n                                <p>\n                                    We note that our findings from the Image Analysis Module influenced the design of the Text Analysis Module in order to meet our aim of\n                                    developing machine learning systems for crisis management which iteratively incorporate the feedback received from crisis managers.\n                                </p>\n                                <p>\n                                    The presentations below describe details of the ML methodologies we developed and their associated results and discussions.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 col-lg-6 pt-3\" v-for=\"module in modules\" :key=\"module.link\">\n                                <ProjectCard :project=\"module\"></ProjectCard>\n                            </div>\n                            <div class=\"col-12 col-md-9 pt-4\">\n                                <p>\n                                    In the next slides, we discuss the important implications of this study overall and summarize our contributions to the field of Crisis Informatics.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h4>Discussion & Implications of Study</h4>\n                                <h5>Involvement of Crisis Managers in the development & iteration of ML methodology</h5>\n                                <p>\n                                    While crisis classification tasks have been published in the literature, to the best of our knowledge, <strong>this work is the first of its kind \n                                    to both involve & incorporate the insights of crisis managers into an ML methodology</strong> to reduce information overload during crisis. \n                                    This is best seen from the image annotation workshops we held with EOC and the incorporation of those results into our text analysis ML methodology.\n                                </p>\n                                <h5>Investigation of Non-English Crisis Text </h5>\n                                <p>\n                                    While we have developed the infrastructure to develop machine learning text models for any language, \n                                    <strong><u>we underscore our investigation of Japanese crisis text</u> as prior work has focused on applying NLP techniques to <u>predominantly English crisis text.</u></strong>\n                                </p>\n                                <h5>Contextualized Framework towards better human-centered design of an AI system</h5>\n                                <p>\n                                    <strong>Our novel framework contextualizes model performance (e.g. F1, F2, precision, etc.) as part of a broad approach to assessing the efficacy of using ML in \n                                    reducing information overload.</strong> That is, we consider other important aspects in the design of our system in order to better serve those \n                                    we aim to assist, crisis managers. We investigate classification task creation, data quality & interannotator agreement analysis,\n                                    issues of class imbalance, involving crisis managers in the development process, and the determination of a performance metric \n                                    & corresponding performance baselines\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h4>Summary of Main Contribution</h4>\n                                <p>\n                                    With this thesis, we have introduced and exemplified a framework which aims to\n                                    embed all of the mentioned considerations into the process of designing, developing,\n                                    and iterating on a ML methodology for enhancing crisis awareness and\n                                    response. <strong>Most notably, this framework situates model development and evaluation,\n                                    which is commonplace in prior work, as one piece of a broader, contextualized understanding\n                                    of the efficacy ML methodologies can have for crisis managers in mitigating\n                                    information overload from crowdsourced crisis reports.</strong> Additionally, this framework\n                                    <strong>promotes the iterative development of AI systems</strong> which is\n                                    informed <strong>from the insights gained from engaging with crisis managers, aiming to\n                                    address this gap in prior work.</strong>\n                                </p>\n                                <p>\n                                    This framework is only the beginning for similar work in this domain, as the development\n                                    of AI systems and ML methodologies for mitigating information overload\n                                    of crisis managers has many complex intricacies for which we only scratch the surface\n                                    in our attempt to broaden this discussion within the crisis informatics field. <strong>We contribute this\n                                    framework and the full exhibition of the principles and analyses contained within it to\n                                    work closer towards a goal worth striving for: enhanced crisis awareness and response\n                                    from automated assessment of crowdsourced crisis reporting.</strong>\n                                </p>\n                                <p>\n                                    In the next slides, we further detail the specifics of the other contributions of this thesis. \n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h4>Contributions 1/3</h4>\n                                <h5>Open-source Python Packages/Code</h5>\n                                <p>\n                                    To ensure the reuse of the analysis conducted in this work, we release two open-source\n                                    Python packages: One for the <a href=\"https://pypi.org/project/url-image-module/0.27.0/\" target=\"_blank\">Image Analysis Module</a> and the other for the <a href=\"https://pypi.org/project/url-text-module/0.6.1/\" target=\"_blank\">Text\n                                    Analysis Module</a>. These packages include utilities for training, testing, and prediction using the models presented in this work, computing statistics for interannotator\n                                    agreement, and computing metrics for model performance. For text data, there are also utilities for featurization of text, performing nested cross validation, and clustering.\n                                    We note that there are utilities for plotting such as methods for producing the plots shown throughout this project.\n                                </p>\n                                <p>\n                                    In addition to these packages, we release an <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting\" target=\"_blank\">open-source repository</a> containing\n                                    Jupyter notebooks, relevant documents (e.g. the mentioned annotation guide), and other code required to reproduce the experiments and reuse the analysis\n                                    conducted in this work.\n                                </p>\n                                <h5>Flood Presence Task Creation, Labeled Image Dataset, and Performance Benchmark</h5>\n                                <p>\n                                    Since we focused on flood crises, we defined the image prediction task of Flood Presence classification. \n                                    The Flood Presence task is the binary classification task of determining whether or not there is presence of flood in an image. We construct a\n                                    labeled dataset for the task consisting of approx. 23.6 images by combining various open-source datasets which were labeled with labels useful for this task, although they\n                                    were originally developed for other adjacent tasks. We contribute this dataset for\n                                    further research in crisis informatics. In addition, we provide Train/Dev/Test splits\n                                    and an associated benchmark performance on the test split using a state-of-the-art\n                                    Convolutional Neural Network (CNN) model, EfficientNet-B1.\n                                </p>\n                                <h5>Data Annotation Procedure and Analysis of Interannotator Agreement</h5>\n                                <p>\n                                    We developed a procedure for annotating images in order to label the unlabeled image\n                                    data provided to us by our partners in Fukuchiyama, Japan. This procedure included creating\n                                    an annotation guide to assist annotators in their labeling. This guide included the\n                                    name of each task, the names of the mutually-exclusive classes associated with each\n                                    task, and an associated description and an example image for each class. We then had\n                                    annotators from the Urban Risk Lab independently annotate the images using this\n                                    guide.\n                                </p>\n                                <p>\n                                    After the annotation effort was completed, we were able analyze the interannotator\n                                    agreement between the annotators and construct ground-truth labels for these images.\n                                    We computed interannotator reliability statistics to get a sense of how reproducible\n                                    the annotation procedure was for each task as well as to have transparency of the\n                                    data quality prior to using it for ML purposes. Finally, we created ground-truth\n                                    datasets using these labels for the Fukuchiyama images to use in evaluating the image\n                                    classification models we developed.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h4>Contributions 2/3</h4>\n                                <h5>Classification and Clustering of Crowdsourced Japanese Crisis Text</h5>\n                                <p>\n                                    Research in crisis informatics on crowdsourced crisis data focuses mostly on English, \n                                    and thus research on crowdsourced Japanese crisis text is sparse. The Text Analysis \n                                    Module developed in this work focused exclusively on Japanese text data. We explored \n                                    various numerical representations of the Japanese crisis text reports provided by our \n                                    partners and developed a pipeline for preprocessing the raw text and producing the numerical representations. \n                                    Additionally, our partners provided labels along with the text reports, so we experimented with classifying the\n                                    text. Lastly, we explored the text data using unsupervised learning, specifically, we\n                                    employed clustering methods to help inform development of classification tasks in\n                                    future work\n                                </p>\n                                <h6>Pipeline for Japanese Crisis Text Preprocessing, Tokenization, and Featurization</h6>\n                                <p>\n                                    In order to use the text reports as input to the various ML models employed in\n                                    this work, we represent the raw text string of each report as a numerical vector, or featurization. \n                                    Depending on the featurization we choose to use for a text\n                                    report, we may first preprocess the text. This preprocessing included various steps\n                                    including tokenization, stopword removal, and lemmatization, which we performed\n                                    using open-source software (i.e. tokenizer and lemmatizer) and publicly available\n                                    data (i.e. stopwords list) for the Japanese language.\n                                    We provide a pipeline for preprocessing and performing the following featurizations\n                                    on Japanese text:\n                                    <ul>\n                                        <li>n-gram Bag-of-Words (BOW)</li>\n                                        <li>n-gram Term Frequency-Inverse Document Frequency (TF-IDF)</li>\n                                        <li>Pretrained Japanese Masked Language Modeling (MLM) BERT Model with Classification (CLS) Pooling Embedding</li>\n                                    </ul>\n                                    The resultant feature vectors representing the text enabled us to use them as input\n                                    to ML models. Thus, we can then employ classification and clustering techniques on\n                                    the text data.\n                                </p>\n                                <h6>Human Risk Task Creation and Performance Metric Determination</h6>\n                                <p>\n                                    We devised a new text classification task, Human Risk classification. The human\n                                    risk text classification task determines whether or not a crisis text report indicates if\n                                    there are people in need of rescue from a crisis. This includes people being unable\n                                    to evacuate due to physical disability (such as unable to use stairs), surrounding\n                                    conditions (such as being trapped in a submerged car), and/or being in need of\n                                    life-saving emergency medical care. This classification task was unique among the\n                                    classification tasks presented in this work because it was devised using labels that\n                                    came with the text reports given to us by our crisis management partners. Relatedly,\n                                    we determine the metric to use in model performance evaluation using the insights\n                                    we gained from our partners.\n                                </p>\n                                <h6>Exploratory Analysis of Japanese Crisis Text using Unsupervised Learning</h6>\n                                <p>\n                                    With the intention of finding cohesive groupings within the Japanese crisis text corpus, which can inform the development of future text classification tasks, we devise\n                                    a pipeline for featurizing Japanese crisis text, reducing the high-dimensional text feature vector to 2 dimensions, and clustering the data. We evaluate this pipeline\n                                    both quantitatively and qualitatively, experimenting with various text featurizations\n                                    including unigram TF-IDF features and pretrained Japanese MLM BERT with CLS\n                                    Pooling embeddings mentioned above, t-Distributed Stochastic Neighbor Embedding\n                                    (t-SNE) and Principle Component Analysis (PCA) for dimensionality reduction, and finally K-means and K-medoids for the algorithm which clusters\n                                    the data.\n                                </p>\n                                <p>\n                                    After we determine the optimal combination of text embedding, dimensionality\n                                    reduction technique, and clustering algorithm, we create brief summaries of each\n                                    cluster using the unigrams with highest TF-IDF score for each cluster and the closest\n                                    reports (by euclidean distance) within each cluster to the cluster center to help in the\n                                    determination of a human-interpretable label for each cluster. Lastly, a member of the Urban Risk Lab at\n                                    MIT who is fluent in Japanese used these summaries to determine an interpretable\n                                    label to accompany each cluster found. We thus provide various labels which can be\n                                    used for classification experiments and analysis in a future work.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h4>Contributions 3/3</h4>\n                                <h6>Quantitative and Qualitative Evaluation in Japanese Flood Crisis Context</h6>\n                                <p>\n                                    Prior work has typically evaluated ML methods using quantitative measures, mainly\n                                    classification performance metrics, e.g. accuracy, precision, recall, F1, and AUROC\n                                    (Area Under the Receiver Operating Characteristic Curve) and their macro and micro\n                                    variants. However, <strong>with the framework we present in this thesis, we aimed to expand\n                                    the evaluation of the efficacy ML models have in reducing information overload to\n                                    not only include similar quantitative measures mentioned above, but also qualitative\n                                    evaluation derived from engaging with our crisis management partners.</strong> Beyond having good performance, we hoped to use the qualitative evaluation used in this study\n                                    to gain a broader understanding of the efficacy a model can provide crisis managers\n                                    in mitigating information overload and gaining situational awareness.\n                                </p>\n                                <p>\n                                    We held image annotation workshops with various crisis managers and aimed to\n                                    understand what type of information they seek to gain from a crowdsourced image\n                                    during a flood crisis event. With their insights, we began to understand how our\n                                    models can be refined or improved, or how new models can be created in order to\n                                    better serve the information needs of crisis managers more effectively, such as by\n                                    tailoring the labels and their associated meanings to the information needs of crisis\n                                    managers suggested from their annotations. Additionally, we gained more insight\n                                    into the appropriate metrics to use when evaluating models based on the priorities of\n                                    crisis managers as it relates to the task. From these workshops, we share key lessons\n                                    that can influence the design of this framework and AI-augmented crisis information\n                                    systems of the future. In fact, within this work, we used the lessons learned from\n                                    the image analysis workshops to assist us in determining the performance metric to\n                                    use when developing models for the human risk text classification task. This exercise\n                                    exhibited the principle of iterative development our framework intends to promote.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h4>Adjacent Projects</h4>\n                                <img class=\"rounded img-fluid\" id=\"overview-pic\" src=\"../../../../public/assets/adjacent-projects.png\" alt=\"First slide\">\n                                <p>\n                                    During my time on this project, I defined and mentored undergraduate research projects that were adjacent to my research.\n\n                                    These projects included novel topics in the field of Crisis Informatics such as: \n                                    <ul>\n                                        <li>\n                                            <strong><u>Interpretability in Image and Text Classification Models:</u></strong>\n                                            <p>\n                                                Investigated interpretability algorithms such as GradCAM (Class Activation Mapping) on image classification models used to classify crisis report images and Local Interpretable Model-Agnostic Explaination\n                                                (LIME) on text classification models used to classify report text in an effort to increase the interpretability of the models which would be employed during crisis and potentially \n                                                improve model performance through relabeling & retraining.\n                                            </p>\n                                        </li>\n                                        <li>\n                                            <strong><u>Multilabel Image Classification:</u></strong>\n                                            <p>\n                                                Investigated CNNs for image classification tasks which can perform multilabel classification (as opposed to single label classification) in the crisis informatics context as\n                                                most classification tasks in the literature were single-label.\n                                            </p>\n                                        </li>\n                                        <li>\n                                            <strong><u>Towards Establishing Interannotator Agreement Standards & Tools in the Crisis Informatics Community:</u></strong>\n                                            <p>\n                                                Investigated the establishment of standards and analysis tools for understanding interannotator agreement (and disagreement) on human-annotated datasets in the crisis informatics community for\n                                                both single-label and multilabel classification tasks as such standards and tools did not exist in the crisis informatics community.\n                                            </p>\n                                        </li>\n                                        <li>\n                                            <strong><u>Development of a Crisis Management Dashboard and Simulations using ML Models and their Predictions:</u></strong>\n                                            <p>\n                                                Investigated the development of an interactive dashboard for crisis managers to use during a crisis event that visualizes the predictions of\n                                                various machine learning models on a map to provide situational summarization on individual report and aggregate report levels. \n                                                A simulation was constructed using past citizen crisis reports and predictions on those reports by trained machine learning models.\n                                            </p>\n                                        </li>\n                                    </ul>\n                                </p>\n                            </div>\n                        </div>\n                    </div>        \n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport ProjectCard from '../../ProjectCard.vue';\nimport { ML_MODULES, scrollUpFunc, enableSwipeOnCarousel } from '../../../constants.js';\n\nexport default {\n  name: 'MLForCrowdsourcedCrisisData',\n  components: {\n    ProjectCard\n  },\n  data() {\n    return {\n        modules: ML_MODULES\n    }\n  },\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('MLForCrowdsourcedCrisisDataCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h4, h5 {\n    color: white;\n}\n\n#overview-pic {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 90vh;\n    height: 40vh;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n@media (max-width: 500px) {\n\n    #overview-pic {\n        max-height: 50vw;\n    }\n\n    h3 {\n        font-size: 4.5vw;\n    }\n\n}\n\n</style>","import mod from \"-!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./MLForCrowdsourcedCrisisData.vue?vue&type=template&id=76888782&scoped=true&\"\nimport script from \"./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"\nexport * from \"./MLForCrowdsourcedCrisisData.vue?vue&type=script&lang=js&\"\nimport style0 from \"./MLForCrowdsourcedCrisisData.vue?vue&type=style&index=0&id=76888782&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"76888782\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-8\"},[_c('h3',[_vm._v(\"Image Analysis Module\")])]),_c('div',{staticClass:\"col-12 col-md-10\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"ImageAnalysisCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"5\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"6\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"7\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"8\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"9\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"10\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"11\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"12\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"13\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"14\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"15\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"16\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"17\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"18\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ImageAnalysisCarousel\",\"data-bs-slide-to\":\"19\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item cc-carousel-item active\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-5\"},[_c('div',{staticClass:\"col-12 col-md-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"image-analysis-module\",\"src\":require(\"../../../../public/assets/image-analysis-module.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3\"},[_c('h5',[_vm._v(\"Image Analysis Module Overview\")]),_c('h6',[_vm._v(\"Aim\")]),_c('p',[_vm._v(\" The aim of the Image Analysis Module is to utilize pretrained Convolutional Neural Network (CNN) models to yield efficient and accurate predictions from image data in crowdsourced crisis reports such as those on RiskMap & Twitter. The classification tasks of \"),_c('strong',[_vm._v(\"Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN)\")]),_vm._v(\", and \"),_c('strong',[_vm._v(\"Flood Presence (FP)\")]),_vm._v(\" form a diverse suite of labels for the automatic categorization of crisis data, which can \"),_c('strong',[_vm._v(\"reduce the time necessary for manual assessment of reports; in a fraction of a second, the model predictions made for these tasks provide a series of categorizations for an individual report.\")]),_vm._v(\" We leverage state-of-the-art CNNs, which strike a necessary balance between model complexity, memory and storage constraints, and model performance to provide these predictions. \")]),_c('h6',[_vm._v(\"Methodology\")]),_c('p',[_vm._v(\" To achieve this aim, we use large, labeled, open-source datasets for training and evaluating the models. We formulate a new crisis image classification task for detecting flood presence in an image and construct a new dataset altogether using flood images from various open-source datasets, which contained flood-adjacent labels. \"),_c('strong',[_vm._v(\"We were given unlabeled flood crisis images from crisis managers in Fukuchiyama (FC), Japan.\")]),_vm._v(\" We devise an annotation procedure to label this data and analyze the results of human-annotations on the images. \"),_c('strong',[_vm._v(\"Since this data was not used to train or develop the models, we aimed to see if our trained models could accurately classify unseen images from the FC context\")]),_vm._v(\". \"),_c('strong',[_vm._v(\"We held various image annotation workshops with crisis managers\")]),_vm._v(\" to identify the \"),_c('strong',[_vm._v(\"limitations in our approach\")]),_vm._v(\" and understand \"),_c('strong',[_vm._v(\"how we could iterate on the design of our ML methodology.\")]),_vm._v(\" This procedure enabled us to evaluate the use of image classification models in assisting crisis managers using \"),_c('strong',[_vm._v(\"quantitative metrics\")]),_vm._v(\" as well as \"),_c('strong',[_vm._v(\"qualitatively through the feedback\")]),_vm._v(\" we received through image annotation workshops. This evaluation directly influenced our approach for devising the Text Analysis Module. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-5\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Summary of Results\")]),_c('h5',[_vm._v(\"Development of Flood Presence Task, Open-source Dataset Creation, and Performance Benchmark\")]),_c('p',[_vm._v(\" In our research focus of flood-crisis events, we created a new crisis image classification task for predicting the presence of flood in an image. We form a large, labeled dataset of approx. 23k images from smaller open-source Twitter crisis image datasets with flood-adjacent labels. We created randomized, non-overlapping train/dev/test splits, developed an EfficientNet-B1 CNN model to perform the task, and \"),_c('strong',[_vm._v(\"report a benchmark performance of 92.1% weighted F1 score\")]),_vm._v(\". We open-source the dataset, the data splits, and the trained model weights. \")]),_c('h5',[_vm._v(\"Annotation, Ground-truthing of Fukuchiyama Crisis Images & Interannotator Agreement Analysis\")]),_c('p',[_c('strong',[_vm._v(\"In an effort to involve crisis managers in the development of the ML methodology\")]),_vm._v(\" for mitigating information overload during crisis events, we were given a dataset of \"),_c('strong',[_vm._v(\"658 on-the-ground, unlabeled images from crisis managers in Fukuchiyama, Japan, a context which is suspectible to flooding, in order to evaluate the CNN models we trained on an unseen context.\")])]),_c('p',[_vm._v(\" We developed an annotation guide for labeling these images for the image classification tasks of Damage Severity, Humanitarian Categories, Informativeness, and Flood Presence. Members of the Urban Risk Lab labeled these images for each of these tasks. \")]),_c('p',[_vm._v(\" To understand how reproducible the labeling was for each of the tasks as well as to have a \"),_c('strong',[_vm._v(\"measure of the data quality\")]),_vm._v(\", we computed the Fleiss' Kappa score for each task. \"),_c('strong',[_vm._v(\"We report Fleiss' Kappa scores of 0.413, 0.304, 0.313, and 0.829 for the Damage Severity, Humanitarian Categories, Informativeness, and Flood Presence tasks, respectively. \")]),_vm._v(\"This suggests that further investigation should be conducted in refining the label definitions and the annotation guide for clarity for the Damage Severity, Humanitarian Categories, and Informativeness tasks. \")]),_c('p',[_vm._v(\" We use the most-frequent label provided for an image for a specific task as the ground-truth label. We thus create a ground-truth dataset for each task to evaluate the trained CNN models on. We call these ground-truth datasets the Fukuchiyama Crisis Images. \")]),_c('h5',[_vm._v(\"Development of CNN Models for various Crisis Image Classification Tasks & Evaluation on Fukuchiyama Crisis Images\")]),_c('p',[_vm._v(\" Using large, open-source, \"),_c('strong',[_vm._v(\"labeled Twitter crisis image datasets, covering a variety of crisis and geographical contexts, we trained EfficientNet-B1 CNN models\")]),_vm._v(\" to perform the tasks of Damage Severity, Humanitarian Categories, Informativeness, and Flood Presence. In order to understand how well the trained models would perform on the unseen Fukuchiyama context, we tested them on the Fukuchiyama Crisis Images previously mentioned. We investigated aggregate metrics such as those reported in the crisis informatics literature (e.g. weighted F1) and per-class performance metrics (precision, recall, and F1). \")]),_c('p',[_vm._v(\" Of all the models we developed, we found that the \"),_c('strong',[_vm._v(\"Flood Presence model performed relatively well on the Fukuchiyama data\")]),_vm._v(\", achieving a \"),_c('strong',[_vm._v(\"weighted F1 score of 82.5%\")]),_vm._v(\", having scores \"),_c('strong',[_vm._v(\"at or above 0.793 on both the \\\"Flood\\\" & \\\"Not Flood\\\" classes by precision, recall, and F1\")]),_vm._v(\". Additionally, we note that the \"),_c('strong',[_vm._v(\"Flood Presence task\")]),_vm._v(\" had the highest Fleiss' Kappa score among the tasks, and with all of these measures considered, \"),_c('strong',[_vm._v(\"sets precedent for the development of crisis image classification tasks & corresponding models in future work.\")])]),_c('h5',[_vm._v(\"Image Annotation Workshops for Insights from Crisis Managers\")]),_c('p',[_vm._v(\" From the image annotation workshops we held with crisis managers in various contexts, we began to understand limitations in the tasks we investigated in the image analysis module such as the \"),_c('strong',[_vm._v(\"Damage Severity, Humanitarian Categories, and Informativeness having class labels which were too subjective to be useful during crisis\")]),_vm._v(\". The Humanitarian Categories task has the \\\"Rescue, Volunteering, and Donation Effort\\\" class which corresponds to the recovery phase of a crisis event and not the emergency phase which was more imperative to the crisis managers. Finally, \"),_c('strong',[_vm._v(\"the Flood Presence task has classes which are considered too simplistic to be useful during a crisis event\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" As we held workshops with crisis managers from both the Fukuchiyama context and the US, we came to understand \"),_c('strong',[_vm._v(\"cross-contextual and contextual insights\")]),_vm._v(\". \")]),_c('p',[_c('strong',[_vm._v(\"Cross-contextually, assessing the potential of human casualty in crisis imagery is considered a top priority.\")]),_vm._v(\" Furthermore, \"),_c('strong',[_vm._v(\"the cost of not investigating the potential for human casualty when there is actually human casualty (false negative) is considered more costly than investgating when there is NOT human casualty (false positive).\")]),_vm._v(\" The presence of people in crisis images should be assessed with high-priority and the insights derived from those images should be \"),_c('strong',[_vm._v(\"specific\")]),_vm._v(\" as specific insights can help personnel assess crisis severity. Lastly, physical markers in images can often indicate to crisis managers broader crisis impact to an area, e.g. muddy water suggests potential nearby landslide. \")]),_c('p',[_vm._v(\" The \"),_c('strong',[_vm._v(\"crisis managers in Fukuchiyama used contextual insights\")]),_vm._v(\", namely they assessed flood impact severity using \"),_c('strong',[_vm._v(\"National Standards in Japan\")]),_vm._v(\". Additionally, when assessing an image, the Fukuchyama crisis experts used both the image and their knowledge about the area where an image was taken, e.g. if an image shows flooding but the area in which that image was taken doesn't typically flood, this causes them more concern for that specific area. \")]),_c('p',[_vm._v(\" We determined that depending on the task, \"),_c('strong',[_vm._v(\"rather than using common metrics reported in the literature, we can integrate the priorities of crisis managers during crisis into our selection of an appropriate performance metric when developing an ML model\")]),_vm._v(\", e.g. capturing a priority in the performance metric that for a specific task, a false negative is more costly than a false positive. We investigate this further in the Text Analysis Module. \")]),_c('p')])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Datasets & Train/Dev/Test Splits for Model Development & Evaluation\")]),_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-6\"},[_c('p',[_vm._v(\" Since we used CNN models, specifically the \"),_c('strong',[_vm._v(\"EfficientNet-B1 architecture pretrained on ImageNet\")]),_vm._v(\", we wanted to make use of \"),_c('strong',[_vm._v(\"large open-source Twitter crisis image datasets\")]),_vm._v(\" for \"),_c('strong',[_vm._v(\"finetuning\")]),_vm._v(\" the models to perform the classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP). \")]),_c('h6',[_vm._v(\"Open-source Consolidated Crisis Image Datasets\")]),_c('p',[_vm._v(\" For the DS, HC, and IN tasks, we made use of the open-source train/dev/test splits made available at \"),_c('a',{attrs:{\"href\":\"https://crisisnlp.qcri.org/crisis-image-datasets-asonam20\",\"target\":\"_blank\"}},[_vm._v(\"CrisisNLP\")]),_vm._v(\". We train the models for these tasks using the train and dev splits. For evaluation, we make use of the respective test splits for each task. \")]),_c('h6',[_vm._v(\"Flood Presence Task Creation and Dataset Formation\")]),_c('p',[_c('strong',[_vm._v(\"In this work we focus on flood-crisis events\")]),_vm._v(\", thus we used various open-source image datasets which have flood-adjacent labels and map them to the \"),_c('strong',[_vm._v(\"binary labels of \\\"Flood\\\"/\\\"Not Flood\\\".\")]),_vm._v(\" Using the resulting dataset, \"),_c('strong',[_vm._v(\"we create randomized, non-overlapping Train/Dev/Test splits.\")]),_vm._v(\" Similar to the above, we use the train & dev splits to develop the FP model, and the test split to evaluate the model. \")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-6 pb-4\"},[_c('h5',[_vm._v(\"Flood Presence Dataset Composition\")]),_c('h6',[_vm._v(\"Composition of the Flood Presence (FP) dataset from the original data sources and the number of images for each label\")]),_c('table',{attrs:{\"id\":\"fp-table\"}},[_c('tr',[_c('th',[_c('strong',[_vm._v(\"Dataset\")])]),_c('th',[_c('strong',[_vm._v(\"Flood\")])]),_c('th',[_c('strong',[_vm._v(\"Not Flood\")])]),_c('th',[_c('strong',[_vm._v(\"Total\")])])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Consolidated Disaster Types\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.08916\",\"target\":\"_blank\"}},[_vm._v(\"Alam et al. 2020\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"3201\")]),_c('td',[_vm._v(\"14310\")]),_c('td',[_vm._v(\"17511\")])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Central European Floods 2013\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/1908.03361\",\"target\":\"_blank\"}},[_vm._v(\"Barz et al. 2018\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"3151\")]),_c('td',[_vm._v(\"559\")]),_c('td',[_vm._v(\"3710\")])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Harz Region Floods 2017\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.05756\",\"target\":\"_blank\"}},[_vm._v(\"Barz et al. 2020\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"264\")]),_c('td',[_vm._v(\"405\")]),_c('td',[_vm._v(\"669\")])]),_c('tr',[_c('td',[_c('i',[_c('strong',[_vm._v(\"Rhine River Floods 2018\")]),_c('br'),_vm._v(\" (\"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.05756\",\"target\":\"_blank\"}},[_vm._v(\"Barz et al. 2020\")]),_vm._v(\") \")])]),_c('td',[_vm._v(\"730\")]),_c('td',[_vm._v(\"1007\")]),_c('td',[_vm._v(\"1737\")])]),_c('tr',[_c('td',[_vm._v(\"Total\")]),_c('td',[_vm._v(\"7346\")]),_c('td',[_vm._v(\"16281\")]),_c('td',[_vm._v(\"23627\")])])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_vm._v(\"Image Training Sets\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/ds-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/hc-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/in-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/fp-train.png\")}})]),_c('div',{staticClass:\"col-12 col-md-8\"},[_c('br'),_c('p',[_vm._v(\" As part of the framework we developed, we investigated the class imbalance for each of the image classification training sets. We observed \"),_c('strong',[_vm._v(\"significant imbalance in the label distributions for the Damage Severity\")]),_vm._v(\" and the \"),_c('strong',[_vm._v(\"Humanitarian Categories tasks.\")]),_vm._v(\" We note that this imbalance \"),_c('strong',[_vm._v(\"could be problematic for the performance of the models on the minority classes\")]),_vm._v(\" for those tasks, e.g. the \"),_c('strong',[_vm._v(\"\\\"Mild\\\", \\\"Rescue, Volunteering, or Donation Effort\\\"\")]),_vm._v(\", and \"),_c('strong',[_vm._v(\"\\\"Affected, Injured, or Dead People\\\"\")]),_vm._v(\" classes. \")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-3\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Model Evaluation on Test Splits & Flood Presence Benchmark Performance\")])]),_c('div',{staticClass:\"col-12 col-md-6\"},[_c('h6',{staticClass:\"pb-2\"},[_vm._v(\" Performance of image classification models on their respective test splits. [1] as referred to in the table can be found in the footnote below.\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn1\",\"id\":\"ref1\"}},[_vm._v(\"1\")])])]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"test-set-eval\",\"src\":require(\"../../../../public/assets/test-set-eval.png\")}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('br'),_c('p',[_vm._v(\" Similar to the authors in [1]\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn1\",\"id\":\"ref1\"}},[_vm._v(\"1\")])]),_vm._v(\", we report overall model performance as \"),_c('strong',[_vm._v(\"weighted metrics in order to take into account the class imbalance\")]),_vm._v(\" which is present in the test splits. From the table above, we observe that the models we finetuned achieve a weighted F1 score within a 1% difference compared to the model performances reported in [1]. \"),_c('strong',[_vm._v(\"We report a benchmark performance of 92.1% weighted F1 for the Flood Presence (FP) task.\")]),_vm._v(\" We postulate the comparatively higher performance of for the FP task to be due to the task being binary, having relatively low class imbalance in the training and test sets, as well as being the most clear and objective task, thus being a comparatively simple task for a model to learn. We explore this more through interannotator agreement analysis discussed in the next few slides. \")]),_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fn1\"}},[_vm._v(\"1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.08916\",\"target\":\"_blank\"}},[_vm._v(\"Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response\")]),_vm._v(\", In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.\"),_c('a',{attrs:{\"href\":\"#ref1\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pb-3\"},[_c('div',{staticClass:\"col-12 col-md-6 pb-3\"},[_c('h5',[_vm._v(\"Annotating Fukuchiyama Crisis Images\")]),_c('p',[_vm._v(\" To evaluate our developed models and framework in a context which is susceptible to flood events, we cooperated with \"),_c('strong',[_vm._v(\"crisis managers in Fukuchiyama (FC), Japan to attain 658 images\")]),_vm._v(\" from \"),_c('strong',[_vm._v(\"previous FC flood events\")]),_vm._v(\" as well as \"),_c('strong',[_vm._v(\"non-crisis normal days in FC\")]),_vm._v(\", which were collected on the ground, similar to RiskMap & Twitter crisis images. This evaluation enabled us to understand \"),_c('strong',[_vm._v(\"how well training the models on the large, consolidated crisis datasets\")]),_vm._v(\", which cover a diverse set geographical locations and a multitude of crisis events, would \"),_c('strong',[_vm._v(\"perform on the unseen data from flood events in FC\")]),_vm._v(\". However, in order to form evaluation/test sets from this data, we \"),_c('strong',[_vm._v(\"needed to label the images\")]),_vm._v(\" for each of the four image classification tasks we've discussed. \")]),_c('p',[_c('ul',[_c('li',[_vm._v(\"The images were labeled independently by Urban Risk Lab researchers for each task. The researchers used \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/blob/main/Image%20Analysis%20Module/Annotation/Image%20Labeling%20Guide.docx\",\"target\":\"_blank\"}},[_vm._v(\"this guide\")]),_vm._v(\" to inform their decisions.\")]),_c('li',[_vm._v(\"Each image was \"),_c('strong',[_vm._v(\"independently\")]),_vm._v(\" provided a \"),_c('strong',[_vm._v(\"single label for each task\")]),_vm._v(\" by \"),_c('strong',[_vm._v(\"3 annotators\")]),_vm._v(\".\")]),_c('li',[_c('strong',[_vm._v(\"We enforced independent labeling\")]),_vm._v(\" by hiding the labels given by the other labelers during someone's labeling.\")])])]),_c('p',[_vm._v(\" Since each image was given three labels for a task, we use the plurality, or \"),_c('strong',[_vm._v(\"most frequent label\")]),_vm._v(\", given to the image as the \"),_c('strong',[_vm._v(\"ground-truth label\")]),_vm._v(\" for that image. We chose this method of ground-truthing \"),_c('strong',[_vm._v(\"to minimize any specific person's contributed bias\")]),_vm._v(\" towards the ground-truth label. If there was not a plurality label, we do not label the image and thus do not put that image in the test set for that task, but \"),_c('strong',[_vm._v(\"made note of the disagreement for later analysis\")]),_vm._v(\". \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images\")]),_c('div',{staticClass:\"row align-items-center justify-content-around pt-3\"},[_c('div',{staticClass:\"col-12 col-md-6\"},[_c('h2',[_vm._v(\"... a computer cannot generally agree with annotators at a rate that is higher than the rate at which the annotators agree with each other.\\\"\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn2\",\"id\":\"ref2\"}},[_vm._v(\"1\")])])])]),_c('div',{staticClass:\"col-12 col-md-6\"},[_c('p',[_vm._v(\" Since the data we have is \"),_c('strong',[_vm._v(\"human-annotated\")]),_vm._v(\" and thus permits \"),_c('strong',[_vm._v(\"subjectivity\")]),_vm._v(\", we aimed to assess and make transparent \"),_c('strong',[_vm._v(\"the quality of the annotated datasets\")]),_vm._v(\", thus we computed measures of \"),_c('strong',[_vm._v(\"inter-annotator agreement (IAA)\")]),_vm._v(\". \"),_c('br'),_c('br'),_vm._v(\" This IAA analysis enabled us to determine, for each task: \"),_c('ul',[_c('li',[_vm._v(\"How reproducible labeling for the task is\")]),_c('li',[_vm._v(\"If our annotation procedure can be improved, such as by:\")]),_c('ul',[_c('li',[_vm._v(\"Refinement of task label definitions\")]),_c('li',[_vm._v(\"Clarifying data points of disagreement between annotators\")]),_c('li',[_vm._v(\"Adding more illustrative examples for each class\")])])]),_vm._v(\" This analysis has the advantage of happening \"),_c('strong',[_vm._v(\"before any model development\")]),_vm._v(\", focusing on \"),_c('strong',[_vm._v(\"improvement of classification task formulation itself rather than building a model which will ultimately perform poorly on an ill-formed task.\")])])]),_c('div',{staticClass:\"col-12 col-md-9 pb-4\"},[_c('br'),_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fn2\"}},[_vm._v(\"1. D. T. Nguyen, K. Al-Mannai, S. R. Joty, H. Sajjad, M. Imran, and P. Mitra, \"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/1608.03902\",\"target\":\"_blank\"}},[_vm._v(\"Rapid classification of crisis-related data on social networks using convolutional neural networks,\")]),_vm._v(\" CoRR, vol. abs/1608.03902, 2016.\"),_c('a',{attrs:{\"href\":\"#ref2\",\"title\":\"Jump back to footnote 2 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images\")])]),_c('div',{staticClass:\"col-12 col-md-4\"},[_c('h6',[_vm._v(\"Agreement Measures by Task for Labeled Fukuchiyama Images\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/iaa.png\")}})]),_c('div',{attrs:{\"col\":\"col-12\"}}),_c('div',{staticClass:\"col-12 col-md-6\"},[_c('br'),_c('p',[_c('strong',[_vm._v(\"Fleiss' Kappa\")]),_vm._v(\" [-1, +1] score incorporates the level of agreement attained if the annotators did not look at the data when labeling, i.e. \"),_c('strong',[_vm._v(\"random chance agreement\")]),_vm._v(\", which is an advantage over the complete agreement percentage (\\\"Unanimous Agreement Percentage\\\" pictured above), that is, the percentage of data points in which all annotators agree on the same label. \"),_c('ul',[_c('li',[_vm._v(\"By Fleiss' Kappa, we observe smaller improvements over random chance agreement for Damage Severity (0.413), Humanitarian Categories (0.304), and Informativeness (0.313) as compared to Flood Presence (0.829)\")]),_c('ul',[_c('li',[_vm._v(\"This \"),_c('strong',[_vm._v(\"suggests that further investigation should be conducted in refining the label definitions and the annotation guide for clarity\")]),_vm._v(\" by understanding potential causes for disagreement on the tasks with lower Fleiss' Kappa scores, in order \"),_c('strong',[_vm._v(\"to improve agreement and thus the quality of the dataset\")]),_vm._v(\".\")])]),_c('li',[_vm._v(\"We use the \"),_c('strong',[_vm._v(\"plurality labels found for each task to form the ground-truth FC datasets\")]),_vm._v(\" for each of the tasks which we evaluate the previously mentioned trained CNN models on.\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Annotated Fukuchiyama Image Test Sets\")]),_c('div',{staticClass:\"row align-items-center justify-content-between\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/ds-fc.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/hc-fc.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/in-fc.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-3 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/fp-fc.png\")}})])])]),_c('div',{staticClass:\"row align-items-center justify-content-center pb-5\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('br'),_c('p',[_vm._v(\" The ground-truth datasets are formed from the plurality labels found from the annotations given to the FC images. We again \"),_c('strong',[_vm._v(\"observe imbalance in the resulting datasets\")]),_vm._v(\", albeit to varying degrees. Therefore, we again make \"),_c('strong',[_vm._v(\"use of weighted aggregate metrics for model evaluation\")]),_vm._v(\", however, for a more granular insight into model performance, we \"),_c('strong',[_vm._v(\"also investigate the per-class performance of each model by precision, recall, and F1\")]),_vm._v(\" score for each class and visualize the \"),_c('strong',[_vm._v(\"confusion matrix\")]),_vm._v(\". Lastly, we \"),_c('strong',[_vm._v(\"establish a comparison to a baseline classifier\")]),_vm._v(\" using the \"),_c('strong',[_vm._v(\"Cohen's Kappa score\")]),_vm._v(\". \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_c('strong',[_vm._v(\"Damage Severity\")])]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/damage_severity_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/damage_severity_per_class_metric.png\")}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('br'),_c('p',[_vm._v(\" For the damage severity model, we notice that when the model mispredicts the \\\"Little or None\\\" class (i.e. looking at the True Label row for \\\"Little or None\\\"), it predicts \\\"Mild\\\" far more than \\\"Severe\\\". Relatedly, when the damage severity model mispredicts the \\\"Mild\\\" class, it far more often predicts \\\"Little or None\\\" than \\\"Severe\\\". Finally, when the model mispredicts the \\\"Severe\\\" class, it predicts \\\"Mild\\\" more than either \\\"Little or None\\\" or \\\"Severe\\\". \")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_c('strong',[_vm._v(\"Humanitarian Categories\")])]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/humanitarian_categories_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/humanitarian_categories_per_class_metric.png\")}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('br'),_c('p',[_vm._v(\" We observe from the per-class performance metrics that the \"),_c('strong',[_vm._v(\"humanitarian categories model performance varies greatly between the classes for the task\")]),_vm._v(\". Namely, we see that the \"),_c('strong',[_vm._v(\"\\\"Affect, Injured, or Dead People\\\" (AIDP) class has scores of 0 across all metrics\")]),_vm._v(\". From the training set distributions discussed earlier, we observe that the \"),_c('strong',[_vm._v(\"AIDP class is only 6.12% of the entire training set for the humanitarian categories task\")]),_vm._v(\". This is the smallest training set class proportion for any of the image classification tasks examined in this work, with the next lowest training set class proportions being those for the RVDE class in the humanitarian categories task and the \\\"Mild\\\" class of damage severity at 14.0% and 14.4%, respectively. This \"),_c('strong',[_vm._v(\"suggests that the imbalance of the humanitarian categories training set impacts the performance of the humanitarian categories severely on the minority classes\")]),_vm._v(\", especially the AIDP class, the class with the lowest proportion. \")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('div',{staticClass:\"carousel-text\"},[_c('h5',[_c('strong',[_vm._v(\"Informativeness\")])]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/informativeness_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/informativeness_per_class_metric.png\")}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('br'),_c('p',[_vm._v(\" By nature of the labeled FC crisis image data being almost exclusively related to crisis events or \\\"normal day\\\" photos, the \\\"Not Informative\\\" class is only 69 images as opposed to the 589 \\\"Informative\\\" photos. We note that \"),_c('strong',[_vm._v(\"in the original conception of the informativeness task in [1]\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn3\",\"id\":\"ref3\"}},[_vm._v(\"1\")])]),_vm._v(\", the informativeness classifier is intended to be used for filtering noisy tweets which are completely unrelated to crisis events from relevant tweets\")]),_vm._v(\", however as we report, it is \"),_c('strong',[_vm._v(\"not an adequate classifier for filtering images indicative of crisis impact and those of \\\"normal-day\\\" scenes, because, as we have learned, that is a different task altogether\")]),_vm._v(\". We observe that the model correctly classifies most of the images labeled \\\"Informative\\\" with a recall score of 0.781. When classifying the images labeled as \\\"Not Informative\\\", the classifier classifies 53.6% of these images incorrectly as \\\"Informative\\\" and 46.4% of these images correctly as \\\"Not Informative\\\". \"),_c('strong',[_vm._v(\"Across all per-class metrics, the model performs reasonably well on the \\\"Informative\\\" class, but significantly worse on the \\\"Not Informative\\\".\")])]),_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fn3\"}},[_vm._v(\"1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \"),_c('a',{attrs:{\"href\":\"https://arxiv.org/abs/2011.08916\",\"target\":\"_blank\"}},[_vm._v(\"Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response\")]),_vm._v(\", In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.\"),_c('a',{attrs:{\"href\":\"#ref3\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Per-Class Metrics\")]),_c('h5',[_c('strong',[_vm._v(\"Flood Presence\")])]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-4 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/flood_confusion_matrix.png\")}})]),_c('div',{staticClass:\"col-12 col-md-6 col-lg-5 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/flood_presence_per_class_metric.png\")}})]),_c('div',{staticClass:\"col-12 col-md-7 pb-4\"},[_c('br'),_c('p',[_vm._v(\" Unlike the models for damage severity, humanitarian categories, and informativeness, the flood presence model \"),_c('strong',[_vm._v(\"performs consistently well (less variation and higher values) by all metrics across all classes in the task\")]),_vm._v(\", attaining metric scores at or above 0.793 across all metrics for both classes. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-5\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Aggregate Metrics\")])]),_c('div',{staticClass:\"col-12 col-md-5\"},[_c('h6',[_vm._v(\"Performance of Image Classification models on task-respective Fukuchiyama Data\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"agg-metrics\",\"src\":require(\"../../../../public/assets/agg-metrics-fc.png\")}})]),_c('div',{staticClass:\"col-12\"}),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('br'),_c('p',[_vm._v(\" By Cohen's Kappa score, we see that the \"),_c('strong',[_vm._v(\"damage severity and informativeness\")]),_vm._v(\" tasks provide a relatively \"),_c('strong',[_vm._v(\"small improvement over the random classifier\")]),_vm._v(\" for their corresponding datasets as compared to the humanitarian categories model and far more so for the \"),_c('strong',[_vm._v(\"flood presence model, which provides the most improvement over the random classifier\")]),_vm._v(\" for its dataset. \")]),_c('p',[_vm._v(\" We observe across all of the weighted metrics for all tasks, the performances of the models on the Fukuchiyama data are lower than the performances achieved on the consolidated crisis image test splits and flood presence test set reported earlier. This is most apparent for the damage severity model, which achieves a weighted F1 score of 43.2% on the Fukuchiyama data. Similar to the performance of the flood presence model on flood presence test split, the \"),_c('strong',[_vm._v(\"flood presence model performs relatively well on the Fukuchiyama flood presence task images achieving a weighted F1 of 82.5%\")]),_vm._v(\". \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Model Evaluation on Fukuchiyama Data - Discussion\")]),_c('div',{staticClass:\"row align-items-center justify-content-center pb-5\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h6',[_vm._v(\"Performance on Fukuchiyama Data\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Investigating the \"),_c('strong',[_vm._v(\"per-class performance\")]),_vm._v(\" allowed us to see which classes may be suffering from \"),_c('strong',[_vm._v(\"class imbalance issues\")]),_vm._v(\" (e.g. \\\"Affected, Injured, or Dead People\\\") as well as the common mistakes a model makes when predicting, such as when the damage severity model commonly mispredicts \\\"mild\\\" for actual \\\"little-or-none\\\" images. \")]),_c('li',[_vm._v(\"There are likely multiple reasons why the model performance is comparatively lower for the Fukuchiyama data as compared to the test splits for the consolidated crisis image datasets and the flood presence dataset: \"),_c('ul',[_c('li',[_vm._v(\"May be in part due to \"),_c('strong',[_vm._v(\"concept drift\")]),_vm._v(\" between the data the models were \"),_c('strong',[_vm._v(\"trained on\")]),_vm._v(\" and the Fukuchiyama data which the models were \"),_c('strong',[_vm._v(\"evaluated on\")])]),_c('li',[_vm._v(\"Labeled Fukuchiyama data may have been of \"),_c('strong',[_vm._v(\"poorer data quality\")]),_vm._v(\" as suggested from the relatively \"),_c('strong',[_vm._v(\"low Fleiss Kappa coefficients\")]),_vm._v(\" for the damage severity, humanitarian categories, and informativeness tasks\")])])]),_c('li',[_vm._v(\" The \"),_c('strong',[_vm._v(\"low Fleiss' Kappa scores\")]),_vm._v(\" for the damage severity, humanitarian categories, and informativeness tasks can \"),_c('strong',[_vm._v(\"potentially be improved\")]),_vm._v(\" by converting the abstract \"),_c('strong',[_vm._v(\"definitions\")]),_vm._v(\" of different classes \"),_c('strong',[_vm._v(\"into checklists\")]),_vm._v(\", understanding common \"),_c('strong',[_vm._v(\"annotator disagreements\")]),_vm._v(\", and by adding \"),_c('strong',[_vm._v(\"more clarification/examples\")]),_vm._v(\" where necessary in the annotation guide. \")]),_c('li',[_vm._v(\" We \"),_c('strong',[_vm._v(\"consider the performance of the Flood Presence model across the datasets to be robust\")]),_vm._v(\", which we theorize is due to the task is binary (as opposed to multiclass) and has classes which yield higher agreement between annotators. \")])])]),_c('h6',[_vm._v(\"Importance of Selecting a Metric based on Crisis Management Priorities and the Nature of the Data\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\"While the Flood Presence model may be performant, we aimed to understand if this task as well as the other tasks have \"),_c('strong',[_vm._v(\"informative utility to crisis managers during flood crisis\")]),_vm._v(\".\")]),_c('li',[_vm._v(\"Although \"),_c('strong',[_vm._v(\"weighted F1\")]),_vm._v(\" is a popular metric reported in the literature, it is \"),_c('strong',[_vm._v(\"biased towards the model's performance on the majority classes.\")])]),_c('li',[_vm._v(\"Cohen's Kappa provides the level of accuracy achieved that is above the \"),_c('strong',[_vm._v(\"random classifier baseline.\")])]),_c('li',[_vm._v(\"Cohens Kappa can be a more appropriate metric to use in the case of multiclass classification tasks when the dataset is imbalanced, but \"),_c('strong',[_vm._v(\"there is likely an even more appropriate metric depending on the task and the priorities of crisis managers\")]),_vm._v(\".\")]),_c('li',[_vm._v(\"We have determined that selecting a model performance metric should be a process which both considers the \"),_c('strong',[_vm._v(\"nature of the data\")]),_vm._v(\" (i.e. class imbalance) & uses \"),_c('strong',[_vm._v(\"insights from crisis managers\")]),_vm._v(\".\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center pt-3 pb-4\"},[_c('div',{staticClass:\"col-12 col-md-6\"},[_c('h5',[_vm._v(\"Understanding the Informative Utility of the Image Analysis Module\")]),_c('p',[_vm._v(\" To broaden the discussion of the efficacy the image analysis module has in mitigating information overload and enhancing situational awareness, we also \"),_c('strong',[_vm._v(\"qualitatively examine the informative utility the image models have in assisting crisis managers during a flood crisis event\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" We held \"),_c('strong',[_vm._v(\"image annotation workshops\")]),_vm._v(\" with various crisis managers and aimed to understand \"),_c('strong',[_vm._v(\"what type of information they seek to gain from a crowdsourced image\")]),_vm._v(\" during a flood crisis event as well as \"),_c('strong',[_vm._v(\"what their priorities are\")]),_vm._v(\". We iterate on our methodology by using insights we gained from the crisis managers in the development of the Text Analysis Module, exhibiting the \"),_c('strong',[_vm._v(\"principle of iterative development our framework intends to promote\")]),_vm._v(\". \")])]),_c('h5',[_vm._v(\" Image Annotation Workshops with EOC & Methodology Iteration \")]),_c('div',{staticClass:\"col-12 col-md-6\"},[_c('p',[_vm._v(\" The Urban Risk Lab\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn4\",\"id\":\"ref4\"}},[_vm._v(\"1\")])]),_vm._v(\" held image annotation workshops in December 2021 with the following crisis experts in Fukuchiyama: \"),_c('ul',[_c('li',[_c('strong',[_vm._v(\"Director of the Regional Disaster Management Research Center\")]),_vm._v(\", Fukuchiyama Public University and Former Crisis Management Supervisor of Fukuchiyama City\")]),_c('li',[_c('strong',[_vm._v(\"3 Crisis Managers\")]),_vm._v(\" at an EOC in Fukuchiyama\")]),_c('li',[_c('strong',[_vm._v(\"5 Associates\")]),_vm._v(\" (including Fire Department Director & 1 Firefighter) \"),_c('strong',[_vm._v(\"of the Fire Department\")]),_vm._v(\" in Fukuchiyama\")])]),_vm._v(\" In January 2022, the same workshop was held with a \"),_c('strong',[_vm._v(\"former Deputy Administrator\")]),_vm._v(\" of the Federal Emergency Management Agency \"),_c('strong',[_vm._v(\"(FEMA)\")]),_vm._v(\" in the US. \")]),_c('p',[_vm._v(\" Crisis experts were \"),_c('strong',[_vm._v(\"presented 25 images\")]),_vm._v(\" from past FC flood crises. The images represented various types of crisis impact.\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fn5\",\"id\":\"ref5\"}},[_vm._v(\"2\")])]),_vm._v(\" A subset of the images were selected because they had \"),_c('strong',[_vm._v(\"disagreement between annotators\")]),_vm._v(\" & were given \"),_c('strong',[_vm._v(\"a wrong prediction by the CNN model\")]),_vm._v(\" developed for a task. \")]),_c('p',[_vm._v(\" The \"),_c('strong',[_vm._v(\"crisis experts were tasked with labeling images with a variety of labels\")]),_vm._v(\" & \"),_c('strong',[_vm._v(\"identifying insights\")]),_vm._v(\" from an image that are \"),_c('strong',[_vm._v(\"useful for decision making and response\")]),_vm._v(\" during crisis events. \")]),_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fn4\"}},[_vm._v(\" 1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who conducted the qualitative focus-group research in the form of interviews, workshops, and general interfacing with our partners in Fukuchiyama and in the US, provided the translations of the results from Japanese to English to enable the analysis of those results as it relates to the work presented in this thesis, and assisted in the synthesis of the main findings from the observations and discourse that occurred during the image annotation workshops. We note that Saeko Baird has formal training in Human-Computer Interaction (HCI) research.\"),_c('a',{attrs:{\"href\":\"#ref4\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])]),_c('br'),_c('sup',{attrs:{\"id\":\"fn5\"}},[_vm._v(\" 2. \"),_c('strong',[_vm._v(\"Crisis Impact Types:\")]),_vm._v(\" river flood, rock-fall, landslide, residential housing damage, blocked roads, agricultural land damage, submerged residential areas, and damaged infrastructure.\"),_c('a',{attrs:{\"href\":\"#ref5\",\"title\":\"Jump back to footnote 2 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Image Annotation Workshop & Methodology Iteration - Aims\")]),_c('p',[_vm._v(\" With these workshops, we aimed to: \"),_c('ul',[_c('li',[_vm._v(\"Understand \"),_c('strong',[_vm._v(\"cross-contextual insights\")]),_vm._v(\", i.e. information needs and decision-making priorities common to both Fukuchiyama & US crisis management\")]),_c('li',[_c('strong',[_vm._v(\"Compare our devised image analysis ML methodology\")]),_vm._v(\" for automatic insights to the insights gained from \"),_c('strong',[_vm._v(\"manual assessment\")]),_vm._v(\" of crisis images \"),_c('strong',[_vm._v(\"by crisis experts.\")])]),_c('li',[_vm._v(\"Use results to \"),_c('strong',[_vm._v(\"iterate on design of ML methodology\")]),_vm._v(\" to better embed information needs and priorities of crisis managers.\")])])]),_c('p',[_vm._v(\" The following questions were posed for each of the images to actively engage crisis managers in the annotation process. We note, however, that the conversations often expanded beyond these questions, getting to the core of what their main insights would be through manual assessment of the image data and what their main decision priorities would be during a flood crisis event as it relates to analyzing the image data: \")])]),_c('div',{staticClass:\"col-12 col-md-8 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"src\":require(\"../../../../public/assets/workshop-preface-questions.png\")}})])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Image Annotation Workshops with Crisis Experts - Results\")]),_c('p',[_vm._v(\" We report qualitative summaries describing the insights derived from manual assessment by crisis experts of the crisis report images and their expressed information needs. We used these summaries to compare how the insights the Image Analysis Module aims to automatically provide to crisis managers compares to the insights derived from manual assessment by crisis experts and their expressed information needs. This comparison allowed us to qualitatively assess the utility of the Image Analysis Module in attaining situational awareness as it was devised in this work, and helped us in determining ways to iterate on the module in the future to better accommodate the information needs of the crisis managers during a crisis event through new prediction tasks which align with their information needs. \")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h6',[_vm._v(\"Cross-contextual Insights\")]),_c('p',[_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"Potential of Human Casualties in Crisis Imagery\")])]),_c('ul',[_c('li',[_vm._v(\"Possibility of Human Casualty is \"),_c('strong',[_vm._v(\"Top Priority\")])]),_c('li',[_c('strong',[_vm._v(\"Identified physical markers\")]),_vm._v(\" suggesting \"),_c('strong',[_vm._v(\"potential for human casualty:\")]),_c('ul',[_c('li',[_vm._v(\"Submerged Vehicles\")]),_c('li',[_vm._v(\"Collapsed Buildings\")]),_c('li',[_vm._v(\"Housing in Close Proximity to Rockfall or Landslide\")])])]),_c('li',[_vm._v(\" Not investigating when there is actually human casualty (False Negative) \"),_c('strong',[_vm._v(\"is more costly\")]),_vm._v(\" than investigating when there is not actually human casualty (False Positive) \")])])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Presence of People in Crisis Imagery\")])]),_c('ul',[_c('li',[_c('strong',[_vm._v(\"People in crisis imagery\")]),_vm._v(\" is important and should be \"),_c('strong',[_vm._v(\"assessed with high priority\")])]),_c('li',[_c('strong',[_vm._v(\"Insights should be specific,\")]),_vm._v(\" e.g. people laying down vs. people standing casually/walking about - can indicate crisis impact severity to personnel \")])])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Insights of Broader Impact derived from Physical Markers in Images:\")])]),_c('ul',[_c('li',[_vm._v(\" Experts \"),_c('strong',[_vm._v(\"identified physical markers\")]),_vm._v(\" which suggest \"),_c('strong',[_vm._v(\"potential broader impact to area\")]),_vm._v(\", including: \"),_c('ul',[_c('li',[_vm._v(\"Muddy Water  potential nearby landslide\")]),_c('li',[_vm._v(\"Fallen Power Pole  potential power outage\")]),_c('li',[_vm._v(\"Road Passability  possibility of emergency vehicle use & isolated residential areas\")])])])])])])])]),_c('div',{staticClass:\"col-12 col-md-7 pb-4\"},[_c('h6',[_vm._v(\"Contextual Insights\")]),_c('p',[_c('ul',[_c('li',[_c('strong',[_c('u',[_vm._v(\"National Standards in Japan for Assessing Impact Severity\")])]),_c('ul',[_c('li',[_vm._v(\" Standards for Flood Impact Severity on Housing used in Fukuchiyama: \"),_c('ul',[_c('li',[_vm._v(\"Water reaches up to the first-floor ceiling  Severe Flooding/Destruction\")]),_c('li',[_vm._v(\"Water reaches 1m above first-floor level  Partial Flooding/Destruction\")]),_c('li',[_vm._v(\"Water reaches below floor level  Minor Flooding/Destruction\")])])])])]),_c('li',[_c('strong',[_c('u',[_vm._v(\"Insights Derived from both the Image and Contextual-Knowledge:\")])]),_c('ul',[_c('li',[_c('strong',[_vm._v(\"FC crisis experts used contextual knowledge\")]),_vm._v(\" of the area where the image was taken in gaining insights \"),_c('ul',[_c('li',[_vm._v(\"E.g. Image showing flooding in an area that doesnt typically flood causes more concern for that area\")])])])])])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Image Annotation Workshop & Methodology Iteration - Discussion\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" From crisis expert insights and feedback, we have determined that the tasks as presented in this work have classes with interpretations that are either \"),_c('strong',[_vm._v(\"too vague and subjective (damage severity, humanitarian categories, and informativeness)\")]),_vm._v(\" or \"),_c('strong',[_vm._v(\"too simplistic (flood presence)\")]),_vm._v(\" to be useful for them in gaining situational awareness about an unfolding crisis event. \")]),_c('li',[_vm._v(\" The humanitarian categories task has the \\\"Rescue, Volunteering, or Donation Effort\\\" class, which has insights for the \"),_c('strong',[_vm._v(\"recovery phase of a crisis event rather than the emergency phase. Since our ML methodology aims to assist crisis managers during the emergency phase of a crisis event, such classes should be revised or replaced with classes which have insights directly for the emergency phase.\")])]),_c('li',[_vm._v(\" Although the \"),_c('strong',[_vm._v(\"flood presence task\")]),_vm._v(\" has classes with interpretations which are too simple for attaining situational awareness, we note that the \"),_c('strong',[_vm._v(\"relatively high performance, high consistency between independent annotators, and clarity\")]),_vm._v(\" in the interpretation of the classes associated with the flood presence task \"),_c('strong',[_vm._v(\"sets precedent for task creation and model performance for the future tasks\")]),_vm._v(\" developed from the insights and feedback received from the workshops discussed in this work and future workshops. \")])])]),_c('p',[_vm._v(\" The insights and feedback provided by crisis experts enabled us to determine how the Image Analysis Module we have developed in this work is limited in helping to gain insights about the unfolding crisis event. \"),_c('strong',[_vm._v(\"Where our ML methodology falls short in meeting their information needs, their feedback will assist in developing new classification tasks which would be informative enough to assist them during a crisis event and clear enough to yield more consistent labels between annotators, ensuring better quality data to train and evaluate models.\")]),_vm._v(\" The development of new image prediction tasks and associated models will be conducted in a future work. However, we were able to apply some of these insights to inform the ML methodology of the \"),_c('a',{attrs:{\"href\":\"#/projects/ml-for-crowdsourced-crisis-data/text-analysis-module\"}},[_vm._v(\"Text Analysis Module\")]),_vm._v(\". \")])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-8\">  \n            <h3>Image Analysis Module</h3>\n        </div>\n        <div class=\"col-12 col-md-10\">\n            <div id=\"ImageAnalysisCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"2\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"3\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"4\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"5\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"6\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"7\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"8\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"9\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"10\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"11\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"12\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"13\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"14\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"15\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"16\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"17\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"18\"></li>\n                    <li data-bs-target=\"#ImageAnalysisCarousel\" data-bs-slide-to=\"19\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item cc-carousel-item active\">\n                        <div class=\"row align-items-center justify-content-center pb-5\">\n                            <div class=\"col-12 col-md-4 pt-3\">\n                                <img id=\"image-analysis-module\" class=\"img-fluid\" src=\"../../../../public/assets/image-analysis-module.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3\">\n                                <h5>Image Analysis Module Overview</h5>\n                                <h6>Aim</h6>\n                                <p> \n                                    The aim of the Image Analysis Module is to utilize pretrained Convolutional Neural Network (CNN) models to yield\n                                    efficient and accurate predictions from image data in crowdsourced crisis reports such as those on RiskMap & Twitter. The\n                                    classification tasks of <strong>Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN)</strong>, and\n                                    <strong>Flood Presence (FP)</strong> form a diverse suite of labels for the automatic categorization of crisis data, which can <strong>reduce the time necessary for manual assessment of reports; in a fraction of a second, the model\n                                    predictions made for these tasks provide a series of categorizations for an individual\n                                    report.</strong> We leverage state-of-the-art CNNs, which strike a necessary balance between\n                                    model complexity, memory and storage constraints, and model performance to provide\n                                    these predictions.\n                                </p>\n                                <h6>Methodology</h6>\n                                <p>\n                                    To achieve this aim, we use large, labeled, open-source datasets for training and\n                                    evaluating the models. We formulate a new crisis image classification task for detecting flood presence in an image and construct\n                                    a new dataset altogether using flood images from various open-source datasets, which contained\n                                    flood-adjacent labels. <strong>We were given unlabeled flood crisis images from crisis managers in Fukuchiyama (FC), Japan.</strong> We devise an annotation procedure to label this data and analyze the results of human-annotations on the images. \n                                    <strong>Since this data was not used to train or develop the models, we aimed to see if our trained models could accurately classify unseen images from the FC context</strong>.\n                                    <strong>We held various image annotation workshops with crisis managers</strong> to identify the <strong>limitations in our\n                                    approach</strong> and understand <strong>how we could iterate on the design of our ML methodology.</strong> This procedure enabled us to evaluate the use of image classification models in assisting crisis managers\n                                    using <strong>quantitative metrics</strong> as well as <strong>qualitatively through the feedback</strong> we received through image annotation workshops. \n                                    This evaluation directly influenced our approach for devising the Text Analysis Module.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-5\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Summary of Results</h5>\n                                <h5>Development of Flood Presence Task, Open-source Dataset Creation, and Performance Benchmark</h5>\n                                <p>\n                                   In our research focus of flood-crisis events, we created a new crisis image classification task for predicting the presence of \n                                   flood in an image. We form a large, labeled dataset of approx. 23k images from smaller open-source Twitter crisis image datasets with flood-adjacent labels.\n                                   We created randomized, non-overlapping train/dev/test splits, developed an EfficientNet-B1 CNN model to perform the task, and <strong>report a benchmark performance of \n                                   92.1% weighted F1 score</strong>. We open-source the dataset, the data splits, and the trained model weights.\n                                </p>\n                                <h5>Annotation, Ground-truthing of Fukuchiyama Crisis Images & Interannotator Agreement Analysis</h5>\n                                <p>\n                                    <strong>In an effort to involve crisis managers in the development of the ML methodology</strong> for mitigating information overload during crisis events, we were given\n                                    a dataset of <strong>658 on-the-ground, unlabeled images from crisis managers in Fukuchiyama, Japan, a context which is suspectible to flooding, in order to evaluate the CNN models we trained on an unseen context.</strong>\n                                </p>\n                                <p>\n                                    We developed an annotation guide for labeling these images for the image classification tasks of Damage Severity, Humanitarian Categories, Informativeness, and Flood Presence. Members of the \n                                    Urban Risk Lab labeled these images for each of these tasks.\n                                </p>\n                                <p>\n                                    To understand how reproducible the labeling was for each of the tasks as well as to have a <strong>measure of the data quality</strong>, we computed the Fleiss' Kappa score\n                                    for each task. <strong>We report Fleiss' Kappa scores of 0.413, 0.304, 0.313, and 0.829 for the Damage Severity, Humanitarian Categories, Informativeness, and Flood Presence tasks, respectively. </strong>This suggests that further investigation \n                                    should be conducted in refining the label definitions and the annotation guide for clarity for the Damage Severity, Humanitarian Categories, and Informativeness tasks.\n                                <p>\n                                    We use the most-frequent label provided for an image for a specific task as the ground-truth label. We thus create a ground-truth dataset for each task to evaluate the trained \n                                    CNN models on. We call these ground-truth datasets the Fukuchiyama Crisis Images.\n                                </p>\n                                <h5>Development of CNN Models for various Crisis Image Classification Tasks & Evaluation on Fukuchiyama Crisis Images</h5>\n                                <p>\n                                    Using large, open-source, <strong>labeled Twitter crisis image datasets, covering a variety of crisis and geographical contexts, we trained EfficientNet-B1 CNN models</strong> to perform the tasks of Damage Severity, \n                                    Humanitarian Categories, Informativeness, and Flood Presence. In order to understand how well the trained models would perform on the unseen Fukuchiyama context, we\n                                    tested them on the Fukuchiyama Crisis Images previously mentioned. We investigated aggregate metrics such as those reported in the crisis informatics literature (e.g. weighted F1) and \n                                    per-class performance metrics (precision, recall, and F1). \n                                </p>\n                                <p>\n                                    Of all the models we \n                                    developed, we found that the <strong>Flood Presence model performed relatively well on the Fukuchiyama data</strong>, achieving a <strong>weighted F1 score of 82.5%</strong>, having scores <strong>at or above 0.793 on both the \"Flood\" & \"Not Flood\" classes by \n                                    precision, recall, and F1</strong>. Additionally, we note that the <strong>Flood Presence task</strong> had the highest Fleiss' Kappa score among the tasks, and with all of these measures considered, <strong>sets precedent for the development of crisis image classification\n                                    tasks & corresponding models in future work.</strong>\n                                </p>\n                                <h5>Image Annotation Workshops for Insights from Crisis Managers</h5>\n                                <p>\n                                    From the image annotation workshops we held with crisis managers in various contexts, we began to understand limitations in the tasks we investigated in\n                                    the image analysis module such as the <strong>Damage Severity, Humanitarian Categories, and Informativeness having class labels which were too subjective to be useful during crisis</strong>. The Humanitarian Categories task has the\n                                    \"Rescue, Volunteering, and Donation Effort\" class which corresponds to the recovery phase of a crisis event and not the emergency phase which was more imperative to the crisis managers.\n                                    Finally, <strong>the Flood Presence task has classes which are considered too simplistic to be useful during a crisis event</strong>.\n                                </p>\n                                <p>\n                                    As we held workshops with crisis managers from both the Fukuchiyama context and the US, we came to understand <strong>cross-contextual and contextual insights</strong>.\n                                </p>\n                                <p>\n                                    <strong>Cross-contextually, assessing the potential of human casualty in crisis imagery is considered a top priority.</strong> Furthermore, <strong>the cost of not investigating the potential for human casualty when there is actually\n                                    human casualty (false negative) is considered more costly than investgating when there is NOT human casualty (false positive).</strong> The presence of people in crisis images should be assessed with high-priority and the insights derived from those images \n                                    should be <strong>specific</strong> as specific insights can help personnel assess crisis severity. Lastly, physical markers in images can often indicate to crisis managers broader crisis impact to an area, e.g. muddy water suggests potential \n                                    nearby landslide.\n                                </p>\n                                <p>\n                                    The <strong>crisis managers in Fukuchiyama used contextual insights</strong>, namely they assessed flood impact severity using <strong>National Standards in Japan</strong>. Additionally, when\n                                    assessing an image, the Fukuchyama crisis experts used both the image and their knowledge about the area where an image was taken, e.g. if an image shows flooding but the area\n                                    in which that image was taken doesn't typically flood, this causes them more concern for that specific area.\n                                </p>\n                                <p>\n                                    We determined that depending on the task, <strong>rather than using common metrics reported in the literature, we can integrate the priorities of crisis managers during crisis into our selection of\n                                    an appropriate performance metric when developing an ML model</strong>, e.g. capturing a priority in the performance metric that for a specific task, a false negative is more costly than a false positive. We investigate this further in the Text Analysis Module.\n                                </p>\n                                <p>\n\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Datasets & Train/Dev/Test Splits for Model Development & Evaluation</h5>\n                            <div class=\"row justify-content-around\">\n                                <div class=\"col-12 col-md-6\">\n                                    <p>\n                                        Since we used CNN models, specifically the <strong>EfficientNet-B1 architecture pretrained on ImageNet</strong>, we wanted to make use of <strong>large open-source Twitter crisis image datasets</strong>\n                                        for <strong>finetuning</strong> the models to perform the classification tasks of Damage Severity (DS), Humanitarian Categories (HC), Informativeness (IN), and Flood Presence (FP).\n                                    </p>\n                                    <h6>Open-source Consolidated Crisis Image Datasets</h6>\n                                    <p>\n                                        For the DS, HC, and IN tasks, we made use of the open-source train/dev/test splits made available at \n                                        <a href=\"https://crisisnlp.qcri.org/crisis-image-datasets-asonam20\" target=\"_blank\">CrisisNLP</a>. We train the models for these tasks using\n                                        the train and dev splits. For evaluation, we make use of the respective test splits for each task.\n                                    </p>\n                                    <h6>Flood Presence Task Creation and Dataset Formation</h6>\n                                    <p>\n                                        <strong>In this work we focus on flood-crisis events</strong>, thus we used various open-source image datasets which have flood-adjacent labels\n                                        and map them to the <strong>binary labels of \"Flood\"/\"Not Flood\".</strong> Using the resulting dataset, <strong>we create randomized, non-overlapping Train/Dev/Test splits.</strong>\n                                        Similar to the above, we use the train & dev splits to develop the FP model, and the test split to evaluate the model.\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <div class=\"row align-items-center justify-content-around\">\n                                <div class=\"col-12 col-md-6 pb-4\">\n                                    <h5>Flood Presence Dataset Composition</h5>\n                                    <h6>Composition of the Flood Presence (FP) dataset from the original data sources and the number of images for each label</h6>\n                                    <table id=\"fp-table\">\n                                        <tr>\n                                            <th><strong>Dataset</strong></th>\n                                            <th><strong>Flood</strong></th>\n                                            <th><strong>Not Flood</strong></th>\n                                            <th><strong>Total</strong></th>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Consolidated Disaster Types</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/2011.08916\" target=\"_blank\">Alam et al. 2020</a>)\n                                                </i>\n                                            </td>\n                                            <td>3201</td>\n                                            <td>14310</td>\n                                            <td>17511</td>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Central European Floods 2013</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/1908.03361\" target=\"_blank\">Barz et al. 2018</a>)\n                                                </i>\n                                            </td>\n                                            <td>3151</td>\n                                            <td>559</td>\n                                            <td>3710</td>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Harz Region Floods 2017</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/2011.05756\" target=\"_blank\">Barz et al. 2020</a>)\n                                                </i>\n                                            </td>\n                                            <td>264</td>\n                                            <td>405</td>\n                                            <td>669</td>\n                                        </tr>\n                                        <tr>\n                                            <td>\n                                                <i>\n                                                    <strong>Rhine River Floods 2018</strong>\n                                                    <br>\n                                                    (<a href=\"https://arxiv.org/abs/2011.05756\" target=\"_blank\">Barz et al. 2020</a>)\n                                                </i>\n                                            </td>\n                                            <td>730</td>\n                                            <td>1007</td>\n                                            <td>1737</td>\n                                        </tr>\n                                        <tr>\n                                            <td>Total</td>\n                                            <td>7346</td>\n                                            <td>16281</td>\n                                            <td>23627</td>\n                                        </tr>\n                                    </table>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"carousel-text\">\n                            <h5>Image Training Sets</h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/ds-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/hc-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/in-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/fp-train.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-8\">\n                                    <br>\n                                    <p>\n                                        As part of the framework we developed, we investigated the class imbalance for each of the\n                                        image classification training sets. We observed <strong>significant imbalance in the label distributions for the Damage Severity</strong> \n                                        and the <strong>Humanitarian Categories tasks.</strong> We note that this imbalance <strong>could be problematic for the performance of the models on\n                                        the minority classes</strong> for those tasks, e.g. the <strong>\"Mild\", \"Rescue, Volunteering, or Donation Effort\"</strong>, and <strong>\"Affected, Injured, or Dead People\"</strong> classes.\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-3\">\n                            <div class=\"col-12\">\n                                <h5>Model Evaluation on Test Splits & Flood Presence Benchmark Performance</h5>\n                            </div>\n                            <div class=\"col-12 col-md-6\">\n                                <h6 class=\"pb-2\">\n                                    Performance of image classification models on their respective test splits. [1] as referred to in the table can be found in the footnote below.<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>\n                                </h6>\n                                <img id=\"test-set-eval\" src=\"../../../../public/assets/test-set-eval.png\" class=\"img-fluid\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <br>\n                                <p>\n                                        Similar to the authors in [1]<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup>, we report overall model performance as\n                                        <strong>weighted metrics in order to take into account the class imbalance</strong> which is present in the test splits.\n                                        From the table above, we observe that the models we finetuned achieve a weighted F1 score within a 1% difference\n                                        compared to the model performances reported in [1]. <strong>We report a benchmark performance of 92.1% weighted F1 for the Flood Presence (FP) task.</strong>\n                                        We postulate the comparatively higher performance of for the FP task to be due to the task being binary, having relatively low class imbalance in the training and test sets, as well as being the most clear and objective task, thus being a comparatively\n                                        simple task for a model to learn. We explore this more through interannotator agreement analysis discussed in the next few slides.\n                                </p>\n                                <hr>\n                                <p>\n                                    <sup id=\"fn1\">1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \n                                        <a href=\"https://arxiv.org/abs/2011.08916\" target=\"_blank\">Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response</a>, \n                                        In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.<a href=\"#ref1\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                    </sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pb-3\">\n                            <div class=\"col-12 col-md-6 pb-3\">\n                            <h5>Annotating Fukuchiyama Crisis Images</h5>\n                            <p>\n                                To evaluate our developed models and framework in a context which is susceptible to flood events, we cooperated with <strong>crisis managers in \n                                Fukuchiyama (FC), Japan to attain 658 images</strong> from <strong>previous FC flood events</strong> as well as <strong>non-crisis normal days in FC</strong>, which were collected on the ground,\n                                similar to RiskMap & Twitter crisis images. This evaluation enabled us to understand <strong>how well training the models on the large, consolidated crisis datasets</strong>, which cover a diverse set\n                                geographical locations and a multitude of crisis events, would <strong>perform on the unseen\n                                data from flood events in FC</strong>. However, in order to form evaluation/test sets from this data, we <strong>needed to label the images</strong> for each of the four image classification tasks we've discussed.\n                            </p>\n                            <p>\n                                <ul>\n                                    <li>The images were labeled independently by Urban Risk Lab researchers for each task. The researchers used <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/blob/main/Image%20Analysis%20Module/Annotation/Image%20Labeling%20Guide.docx\" target=\"_blank\">this guide</a> to inform their decisions.</li>\n                                    <li>Each image was <strong>independently</strong> provided a <strong>single label for each task</strong> by <strong>3 annotators</strong>.</li>\n                                    <li><strong>We enforced independent labeling</strong> by hiding the labels given by the other labelers during someone's labeling.</li>\n                                </ul>\n                            </p>\n                            <p>\n                                Since each image was given three labels for a task, we use the plurality, or <strong>most frequent label</strong>, given to the image as the <strong>ground-truth label</strong> for that image. We chose this method of ground-truthing <strong>to minimize any specific person's contributed bias</strong> towards the ground-truth label.\n                                If there was not a plurality label, we do not label the image and thus do not put that image in the test set for that task, but <strong>made note of the disagreement for later analysis</strong>.\n                            </p>\n                        </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images</h5>\n                        <div class=\"row align-items-center justify-content-around pt-3\">\n                            <div class=\"col-12 col-md-6\">\n                                <h2>... a computer cannot generally agree with annotators at a rate that is higher than the rate at which the annotators agree with each other.\"<sup><a href=\"#fn2\" id=\"ref2\">1</a></sup></h2>\n                            </div>\n                            <div class=\"col-12 col-md-6\">\n                                <p> \n                                    Since the data we have is <strong>human-annotated</strong> and thus permits <strong>subjectivity</strong>, we aimed to assess and make transparent <strong>the quality of the annotated datasets</strong>, thus we computed measures of <strong>inter-annotator agreement (IAA)</strong>.\n                                    <br>\n                                    <br>\n                                    This IAA analysis enabled us to determine, for each task:\n                                    <ul>\n                                        <li>How reproducible labeling for the task is</li>\n                                        <li>If our annotation procedure can be improved, such as by:</li>\n                                            <ul>\n                                                <li>Refinement of task label definitions</li>\n                                                <li>Clarifying data points of disagreement between annotators</li>\n                                                <li>Adding more illustrative examples for each class</li>\n                                            </ul>\n                                    </ul>\n                                    This analysis has the advantage of happening <strong>before any model development</strong>, focusing on <strong>improvement of classification task formulation itself rather than building a model which will ultimately perform poorly on an ill-formed task.</strong>\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-9 pb-4\">\n                                <br>\n                                <hr>\n                                <p>\n                                    <sup id=\"fn2\">1. D. T. Nguyen, K. Al-Mannai, S. R. Joty, H. Sajjad, M. Imran, and P. Mitra, <a href=\"https://arxiv.org/abs/1608.03902\" target=\"_blank\">Rapid classification of crisis-related data on social networks using convolutional neural networks,</a> CoRR, vol. abs/1608.03902, 2016.<a href=\"#ref2\" title=\"Jump back to footnote 2 in the text.\"></a></sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-4\">\n                            <div class=\"col-12\">\n                                <h5>Inter-Annotator Agreement Analysis on Annotated Fukuchiyama Images</h5>\n                            </div>\n                            <div class=\"col-12 col-md-4\">\n                                <h6>Agreement Measures by Task for Labeled Fukuchiyama Images</h6>\n                                <img src=\"../../../../public/assets/iaa.png\" class=\"img-fluid\">\n                            </div>\n                            <div col=\"col-12\"></div>\n                            <div class=\"col-12 col-md-6\">\n                                <br>\n                                <p>\n                                    <strong>Fleiss' Kappa</strong> [-1, +1] score incorporates the level of agreement attained if the annotators did not look at the data when labeling, i.e. \n                                    <strong>random chance agreement</strong>, which is an advantage over the complete agreement percentage (\"Unanimous Agreement Percentage\" pictured above), that is, the percentage of data points in which all annotators\n                                    agree on the same label.\n                                    <ul>\n                                        <li>By Fleiss' Kappa, we observe smaller improvements over random chance agreement for Damage Severity (0.413), Humanitarian Categories (0.304), and Informativeness (0.313) as compared to Flood Presence (0.829)</li>\n                                            <ul>\n                                                <li>This <strong>suggests that further investigation should be conducted in refining the label definitions and the annotation guide for clarity</strong> by understanding potential causes for disagreement on the tasks with lower Fleiss' Kappa scores,\n                                                    in order <strong>to improve agreement and thus the quality of the dataset</strong>.</li>\n                                            </ul>\n                                        <li>We use the <strong>plurality labels found for each task to form the ground-truth FC datasets</strong> for each of the tasks which we evaluate the previously mentioned trained CNN models on.</li>\n                                    </ul>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"col-12\">\n                            <h5>Annotated Fukuchiyama Image Test Sets</h5>\n                            <div class=\"row align-items-center justify-content-between\">\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/ds-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/hc-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/in-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-3 pt-3\">\n                                    <img src=\"../../../../public/assets/fp-fc.png\" class=\"img-fluid\"/>\n                                </div>\n                            </div>\n                        </div>\n                        <div class=\"row align-items-center justify-content-center pb-5\">\n                            <div class=\"col-12 col-md-7\">\n                                <br>\n                                <p>\n                                    The ground-truth datasets are formed from the plurality labels found from the annotations given to the FC images.\n\n                                    We again <strong>observe imbalance in the resulting datasets</strong>, albeit to varying degrees. Therefore, we again make <strong>use of weighted aggregate metrics for model evaluation</strong>,\n                                    however, for a more granular insight into model performance, we <strong>also investigate the per-class performance of each model by precision, recall, \n                                    and F1</strong> score for each class and visualize the <strong>confusion matrix</strong>. Lastly, we <strong>establish a comparison to a baseline classifier</strong> using the <strong>Cohen's Kappa score</strong>.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5><strong>Damage Severity</strong></h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                    <img src=\"../../../../public/assets/damage_severity_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                    <img src=\"../../../../public/assets/damage_severity_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-7\">\n                                    <br>\n                                    <p>\n                                        For the damage severity model, we notice that when the model\n                                        mispredicts the \"Little or None\" class (i.e. looking at the True Label row for \"Little or None\"), it predicts \"Mild\" far more than \"Severe\".\n                                        Relatedly, when the damage severity model mispredicts the \"Mild\" class, it far more\n                                        often predicts \"Little or None\" than \"Severe\". Finally, when the model mispredicts\n                                        the \"Severe\" class, it predicts \"Mild\" more than either \"Little or None\" or \"Severe\".\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5><strong>Humanitarian Categories</strong></h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                    <img src=\"../../../../public/assets/humanitarian_categories_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                    <img src=\"../../../../public/assets/humanitarian_categories_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-7\">\n                                    <br>\n                                    <p> \n                                        We observe from the per-class performance metrics that the <strong>humanitarian categories model performance varies greatly between the classes for the task</strong>. \n                                        Namely, we see that the <strong>\"Affect, Injured, or Dead People\" (AIDP) class has scores of 0 across all metrics</strong>. From the training set distributions discussed earlier, \n                                        we observe that the <strong>AIDP class is only 6.12% of the entire training set for the humanitarian categories task</strong>. \n                                        This is the smallest training set class proportion for any of the image classification tasks examined in this work, \n                                        with the next lowest training set class proportions being those for the RVDE class in the humanitarian categories task \n                                        and the \"Mild\" class of damage severity at 14.0% and 14.4%, respectively. \n                                        This <strong>suggests that the imbalance of the humanitarian categories training set impacts the performance of the humanitarian \n                                        categories severely on the minority classes</strong>, especially the AIDP class, the class with the lowest proportion.\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>                 \n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <div class=\"carousel-text\">\n                            <h5><strong>Informativeness</strong></h5>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-4 pt-3\">\n                                    <img src=\"../../../../public/assets/informativeness_confusion_matrix.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-4 pt-3\">\n                                    <img src=\"../../../../public/assets/informativeness_per_class_metric.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12 col-md-7\">\n                                    <br>\n                                    <p>\n                                        By nature of the labeled FC crisis image data being almost\n                                        exclusively related to crisis events or \"normal day\" photos, the \"Not Informative\"\n                                        class is only 69 images as opposed to the 589 \"Informative\" photos. We note that\n                                        <strong>in the original conception of the informativeness task in [1]<sup><a href=\"#fn3\" id=\"ref3\">1</a></sup>, the informativeness classifier is intended\n                                        to be used for filtering noisy tweets which are completely unrelated to crisis events\n                                        from relevant tweets</strong>, however as we report, it is <strong>not an adequate classifier for filtering\n                                        images indicative of crisis impact and those of \"normal-day\" scenes, because, as we\n                                        have learned, that is a different task altogether</strong>. We observe that the model correctly\n                                        classifies most of the images labeled \"Informative\" with a recall score of 0.781. When\n                                        classifying the images labeled as \"Not Informative\", the classifier classifies 53.6% of\n                                        these images incorrectly as \"Informative\" and 46.4% of these images correctly as\n                                        \"Not Informative\". <strong>Across all per-class metrics, the model performs reasonably well on the\n                                        \"Informative\" class, but significantly worse on the \"Not Informative\".</strong>\n                                    </p>\n                                    <hr>\n                                    <p>\n                                        <sup id=\"fn3\">1. Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, \n                                            <a href=\"https://arxiv.org/abs/2011.08916\" target=\"_blank\">Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response</a>, \n                                            In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.<a href=\"#ref3\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                        </sup>\n                                    </p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Per-Class Metrics</h5>\n                        <h5><strong>Flood Presence</strong></h5>\n                        <div class=\"row align-items-center justify-content-center\">\n                            <div class=\"col-12 col-md-6 col-lg-4 pt-3\">\n                                <img src=\"../../../../public/assets/flood_confusion_matrix.png\" class=\"img-fluid\">\n                            </div>\n                            <div class=\"col-12 col-md-6 col-lg-5 pt-3\">\n                                <img src=\"../../../../public/assets/flood_presence_per_class_metric.png\" class=\"img-fluid\">\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-4\">\n                                <br>\n                                <p>\n                                    Unlike the models for damage severity, humanitarian categories, and\n                                    informativeness, the flood presence model <strong>performs consistently well (less variation\n                                    and higher values) by all metrics across all classes in the task</strong>, attaining metric scores at or\n                                    above 0.793 across all metrics for both classes.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                            <div class=\"row align-items-center justify-content-around pb-5\">\n                                <div class=\"col-12\">\n                                    <h5>Model Evaluation on Fukuchiyama Data - Aggregate Metrics</h5>\n                                </div>\n                                <div class=\"col-12 col-md-5\">\n                                    <h6>Performance of Image Classification models on task-respective Fukuchiyama Data</h6>\n                                    <img id=\"agg-metrics\" src=\"../../../../public/assets/agg-metrics-fc.png\" class=\"img-fluid\">\n                                </div>\n                                <div class=\"col-12\"></div>\n                                <div class=\"col-12 col-md-7\">\n                                    <br>\n                                    <p> \n                                        By Cohen's Kappa score, we see that the <strong>damage severity and informativeness</strong> tasks provide \n                                        a relatively <strong>small improvement over the random classifier</strong> for their corresponding datasets \n                                        as compared to the humanitarian categories model and far more so for the <strong>flood presence model, \n                                        which provides the most improvement over the random classifier</strong> for its dataset.\n                                    </p>\n                                    <p>\n                                        We observe across all of the weighted metrics for all tasks, the performances of the models \n                                        on the Fukuchiyama data are lower than the performances achieved on the consolidated crisis image \n                                        test splits and flood presence test set reported earlier. This is most apparent for the damage severity model, \n                                        which achieves a weighted F1 score of 43.2% on the Fukuchiyama data. Similar to the\n                                        performance of the flood presence model on flood presence test split, the <strong>flood presence model performs \n                                        relatively well on the Fukuchiyama flood presence task images achieving a weighted F1 of 82.5%</strong>.\n                                    </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Model Evaluation on Fukuchiyama Data - Discussion</h5>\n                        <div class=\"row align-items-center justify-content-center pb-5\">\n                            <div class=\"col-12 col-md-7\">\n                                <h6>Performance on Fukuchiyama Data</h6>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            Investigating the <strong>per-class performance</strong> allowed us to see which classes may be suffering from <strong>class imbalance issues</strong> \n                                            (e.g. \"Affected, Injured, or Dead People\") as well as the common mistakes a model makes when predicting, such as when the damage severity model \n                                            commonly mispredicts \"mild\" for actual \"little-or-none\" images.\n                                        </li>\n                                        <li>There are likely multiple reasons why the model performance is comparatively lower\n                                            for the Fukuchiyama data as compared to the test splits for the consolidated crisis image datasets and the flood presence dataset:\n                                            <ul>\n                                                <li>May be in part due to <strong>concept drift</strong> between the data the models were <strong>trained on</strong> and the Fukuchiyama data which the models were <strong>evaluated on</strong></li>\n                                                <li>Labeled Fukuchiyama data may have been of <strong>poorer data quality</strong> as suggested from the relatively <strong>low Fleiss Kappa coefficients</strong> for the damage\n                                            severity, humanitarian categories, and informativeness tasks</li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            The <strong>low Fleiss' Kappa scores</strong> for the damage severity, humanitarian categories, \n                                            and informativeness tasks can <strong>potentially be improved</strong> by converting the abstract <strong>definitions</strong> \n                                            of different classes <strong>into checklists</strong>, understanding common <strong>annotator disagreements</strong>, \n                                            and by adding <strong>more clarification/examples</strong> where necessary in the annotation guide.\n                                        </li>\n                                        <li>\n                                            We <strong>consider the performance of the Flood Presence model across the datasets to be robust</strong>, \n                                            which we theorize is due to the task is binary (as opposed to multiclass) and has classes which yield higher agreement between annotators.\n                                        </li>\n                                    </ul>\n                                </p>\n                                <h6>Importance of Selecting a Metric based on Crisis Management Priorities and the Nature of the Data</h6>\n                                <p>\n                                    <ul>\n                                        \n                                        <li>While the Flood Presence model may be performant, we aimed to understand if this task as well as the other tasks have <strong>informative utility to crisis managers during flood crisis</strong>.</li>\n                                        <li>Although <strong>weighted F1</strong> is a popular metric reported in the literature, it is <strong>biased towards the model's performance on the majority classes.</strong></li>\n                                        <li>Cohen's Kappa provides the level of accuracy achieved that is above the <strong>random classifier baseline.</strong></li>\n                                        <li>Cohens Kappa can be a more appropriate metric to use in the case of multiclass classification tasks when the dataset is imbalanced, \n                                        but <strong>there is likely an even more appropriate metric depending on the task and the priorities of crisis managers</strong>.</li>\n                                        <li>We have determined that selecting a model performance metric should be a process which both considers the <strong>nature of the data</strong> (i.e. class imbalance) & uses <strong>insights from crisis managers</strong>.</li>\n                                    </ul>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center pt-3 pb-4\">\n                            <div class=\"col-12 col-md-6\">\n                                <h5>Understanding the Informative Utility of the Image Analysis Module</h5>\n                                <p>\n                                    To broaden the discussion of the efficacy the image analysis module has in mitigating information overload and enhancing situational awareness, we also <strong>qualitatively\n                                    examine the informative utility the image models have in assisting crisis managers during a flood crisis event</strong>. \n                                </p>\n                                <p>\n                                    We held <strong>image annotation workshops</strong> with various crisis managers and aimed to\n                                    understand <strong>what type of information they seek to gain from a crowdsourced image</strong> during a flood crisis event as well as <strong>what their priorities are</strong>. We iterate on our methodology\n                                    by using insights we gained from the crisis managers in the development of the Text Analysis Module, exhibiting the <strong>principle of iterative development our framework intends to promote</strong>.\n                                </p>\n                            </div>\n                            <h5> Image Annotation Workshops with EOC & Methodology Iteration </h5>\n                            <div class=\"col-12 col-md-6\">\n                                <p>\n                                    The Urban Risk Lab<sup><a href=\"#fn4\" id=\"ref4\">1</a></sup> held image annotation workshops in December 2021 with the following crisis experts in Fukuchiyama:\n                                    <ul>\n                                        <li><strong>Director of the Regional Disaster Management Research Center</strong>, Fukuchiyama Public University  and Former Crisis Management Supervisor of Fukuchiyama City</li>\n                                        <li><strong>3 Crisis Managers</strong> at an EOC in Fukuchiyama</li>\n                                        <li><strong>5 Associates</strong> (including Fire Department Director & 1 Firefighter) <strong>of the Fire Department</strong> in Fukuchiyama</li>\n                                    </ul>\n                                    In January 2022, the same workshop was held with a <strong>former Deputy Administrator</strong> of the Federal Emergency Management Agency <strong>(FEMA)</strong> in the US.\n                                </p>\n                                <p>\n                                    Crisis experts were <strong>presented 25 images</strong> from past FC flood crises. The images represented various types of crisis impact.<sup><a href=\"#fn5\" id=\"ref5\">2</a></sup> A subset of the images were selected because they had <strong>disagreement between annotators</strong> & \n                                    were given <strong>a wrong prediction by the CNN model</strong> developed for a task.\n                                </p>\n                                <p>\n                                    The <strong>crisis experts were tasked with labeling images with a variety of labels</strong> & <strong>identifying insights</strong> from an image that are <strong>useful for decision making and response</strong> during crisis events.\n                                </p>\n                                <hr>\n                                <p>\n                                    <sup id=\"fn4\">\n                                            1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who conducted the qualitative focus-group research in the form of interviews, workshops, and general interfacing with our partners\n                                            in Fukuchiyama and in the US, provided the translations of the results from Japanese to\n                                            English to enable the analysis of those results as it relates to the work presented in this thesis, and assisted in the synthesis of the main\n                                            findings from the observations and discourse that occurred during the image annotation workshops. We\n                                            note that Saeko Baird has formal training in Human-Computer Interaction (HCI) research.<a href=\"#ref4\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                    </sup>\n                                    <br>\n                                    <sup id=\"fn5\">\n                                            2. <strong>Crisis Impact Types:</strong> river flood, rock-fall, landslide, residential housing damage, blocked roads, agricultural land damage, submerged residential areas, and damaged infrastructure.<a href=\"#ref5\" title=\"Jump back to footnote 2 in the text.\"></a>\n                                    </sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-center\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Image Annotation Workshop & Methodology Iteration - Aims</h5>\n                                <p>\n                                    With these workshops, we aimed to:\n                                    <ul>\n                                        <li>Understand <strong>cross-contextual insights</strong>, i.e. information needs and decision-making priorities common to both Fukuchiyama & US crisis management</li>\n                                        <li><strong>Compare our devised image analysis ML methodology</strong> for automatic insights to the insights gained from <strong>manual assessment</strong> of crisis images <strong>by crisis experts.</strong></li>\n                                        <li>Use results to <strong>iterate on design of ML methodology</strong> to better embed information needs and priorities of crisis managers.</li>\n                                    </ul>\n                                </p>\n                                <p>\n                                    The following questions were posed for each of the images to actively engage crisis managers in the annotation process. We note, however, that the conversations\n                                    often expanded beyond these questions, getting to the core of what their main insights would be through manual assessment of the image data and what their main decision priorities would be during a flood crisis event as it relates to analyzing the image data:\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-8 pb-5\">\n                                    <img src=\"../../../../public/assets/workshop-preface-questions.png\" class=\"img-fluid\">\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Image Annotation Workshops with Crisis Experts - Results</h5>\n                                <p>\n                                    We report qualitative summaries describing the insights derived\n                                    from manual assessment by crisis experts of the crisis report images and their\n                                    expressed information needs. We used these summaries to compare how the\n                                    insights the Image Analysis Module aims to automatically provide to crisis managers\n                                    compares to the insights derived from manual assessment by crisis experts and their\n                                    expressed information needs. This comparison allowed us to qualitatively assess the utility of the Image Analysis Module in attaining situational awareness as it was\n                                    devised in this work, and helped us in determining ways to iterate on the module in the future to better accommodate the information needs of the crisis managers during\n                                    a crisis event through new prediction tasks which align with their information needs.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <h6>Cross-contextual Insights</h6>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            <strong><u>Potential of Human Casualties in Crisis Imagery</u></strong>\n                                            <ul>\n                                                <li>Possibility of Human Casualty is <strong>Top Priority</strong></li>\n                                                <li>\n                                                    <strong>Identified physical markers</strong> suggesting <strong>potential for human casualty:</strong>\n                                                    <ul>\n                                                        <li>Submerged Vehicles</li>\n                                                        <li>Collapsed Buildings</li>\n                                                        <li>Housing in Close Proximity to Rockfall or Landslide</li>\n                                                    </ul>\n                                                </li>\n                                                <li>\n                                                    Not investigating when there is actually human casualty (False Negative) <strong>is more costly</strong> \n                                                    than investigating when there is not actually human casualty (False Positive)\n                                                </li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            <strong><u>Presence of People in Crisis Imagery</u></strong>\n                                            <ul>\n                                                <li><strong>People in crisis imagery</strong> is important and should be <strong>assessed with high priority</strong></li>\n                                                <li><strong>Insights should be specific,</strong> e.g. people laying down vs. people standing casually/walking about - can indicate crisis impact\n                                                    severity to personnel\n                                                </li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            <strong><u>Insights of Broader Impact derived from Physical Markers in Images:</u></strong>\n                                            <ul>\n                                                <li>\n                                                    Experts <strong>identified physical markers</strong> which suggest <strong>potential broader impact to area</strong>, including:\n                                                    <ul>\n                                                        <li>Muddy Water  potential nearby landslide</li>\n                                                        <li>Fallen Power Pole  potential power outage</li>\n                                                        <li>Road Passability  possibility of emergency vehicle use & isolated residential areas</li>\n                                                    </ul>\n                                                </li>\n                                                \n                                            </ul>\n                                        </li>\n                                    </ul>\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-4\">\n                                <h6>Contextual Insights</h6>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            <strong><u>National Standards in Japan for Assessing Impact Severity</u></strong>\n                                            <ul>\n                                                <li>\n                                                    Standards for Flood Impact Severity on Housing used in Fukuchiyama:\n                                                    <ul>\n                                                        <li>Water reaches up to the first-floor ceiling  Severe Flooding/Destruction</li>\n                                                        <li>Water reaches 1m above first-floor level   Partial Flooding/Destruction</li>\n                                                        <li>Water reaches below floor level  Minor Flooding/Destruction</li>\n                                                    </ul>\n                                                </li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            <strong><u>Insights Derived from both the Image and Contextual-Knowledge:</u></strong>\n                                            <ul>\n                                                <li>\n                                                    <strong>FC crisis experts used contextual knowledge</strong> of the area where the image was taken in gaining insights\n                                                    <ul>\n                                                        <li>E.g. Image showing flooding in an area that doesnt typically flood causes more concern for that area</li>\n                                                    </ul>\n                                                </li>\n                                            </ul>\n                                        </li>\n                                    </ul>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Image Annotation Workshop & Methodology Iteration - Discussion</h5>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            From crisis expert insights and feedback, we have determined that the tasks as\n                                            presented in this work have classes with interpretations that are either <strong>too\n                                            vague and subjective (damage severity, humanitarian categories, and informativeness)</strong>\n                                            or <strong>too simplistic (flood presence)</strong> to be useful for them in gaining situational awareness\n                                            about an unfolding crisis event.\n                                        </li>\n                                        <li>\n                                            The humanitarian categories task has the \"Rescue, Volunteering,\n                                            or Donation Effort\" class, which has insights for the <strong>recovery phase of a crisis\n                                            event rather than the emergency phase. Since our ML methodology aims to assist\n                                            crisis managers during the emergency phase of a crisis event, such classes should be\n                                            revised or replaced with classes which have insights directly for the emergency phase.</strong>\n                                        </li>\n                                        <li>\n                                            Although the <strong>flood presence task</strong> has classes with interpretations which are too simple for attaining situational awareness, we note that the <strong>relatively high performance,\n                                            high consistency between independent annotators, and clarity</strong> in the interpretation\n                                            of the classes associated with the flood presence task <strong>sets precedent for task creation and model performance for the future tasks</strong> developed from the insights and\n                                            feedback received from the workshops discussed in this work and future workshops.\n                                        </li>\n                                    </ul>\n                                </p>\n                                <p> \n                                    The insights and feedback provided by crisis experts enabled us to determine\n                                    how the Image Analysis Module we have developed in this work is limited in helping\n                                    to gain insights about the unfolding crisis event. <strong>Where our ML methodology falls\n                                    short in meeting their information needs, their feedback will assist in developing new\n                                    classification tasks which would be informative enough to assist them during a crisis\n                                    event and clear enough to yield more consistent labels between annotators, ensuring\n                                    better quality data to train and evaluate models.</strong> The development of new image\n                                    prediction tasks and associated models will be conducted in a future work. However,\n                                    we were able to apply some of these insights to inform the ML methodology of the\n                                    <a href=\"#/projects/ml-for-crowdsourced-crisis-data/text-analysis-module\">Text Analysis Module</a>.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../../constants';\n\nexport default {\n  name: 'ImageAnalysisCarousel',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('ImageAnalysisCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h5 {\n    color: white;\n}\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n#overview-pic {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 90vh;\n    height: 40vh;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n#pika-gif {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 60vh;\n    height: 40vh;\n}\n\n@media (min-width: 501px) and (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n@media (max-width: 500px) {\n\n    #test-set-eval {\n        height: 42vw;\n    }\n\n    #agg-metrics {\n        height: 42vw;\n    }\n}\n\n#fp-table {\n  font-family: Arial, Helvetica, sans-serif;\n  border-collapse: collapse;\n  color: black;\n  width: 100%;\n}\n\n#fp-table td, #fp-table th {\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n\n#fp-table tr:nth-child(even){background-color: #f2f2f2;}\n#fp-table tr:hover{background-color: #ddd;}\n#fp-table tr:nth-child(odd) {background-color: #ddd;}\n\n#fp-table th {\n  padding-top: 12px;\n  padding-bottom: 12px;\n  text-align: center;\n  background-color: darkturquoise;\n  color: white\n}\n\n</style>","import mod from \"-!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./ImageAnalysisCarousel.vue?vue&type=template&id=5b5db31d&scoped=true&\"\nimport script from \"./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"\nexport * from \"./ImageAnalysisCarousel.vue?vue&type=script&lang=js&\"\nimport style0 from \"./ImageAnalysisCarousel.vue?vue&type=style&index=0&id=5b5db31d&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"5b5db31d\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h3',[_vm._v(\"Text Analysis Module\")])]),_c('div',{staticClass:\"col-12 col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"TextAnalysisCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"5\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"6\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"7\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"8\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"9\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"10\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"11\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"12\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"13\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"14\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"15\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"16\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"17\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TextAnalysisCarousel\",\"data-bs-slide-to\":\"18\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item cc-carousel-item active\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"text-analysis-module\",\"src\":require(\"../../../../public/assets/text-analysis-module.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Text Analysis Methodology Overview\")]),_c('p',[_vm._v(\" The Text Analysis Module aims to provide accurate and efficient classifications of crisis reports using the text modality that is often present in crowdsourced reports. Another aim was to incorporate the insights we gained from the results of our qualitative analysis of the Image Analysis Module that were transferable between the data modalities, i.e. the importance of identifying potential for human casualty or risk to humans. We did this to exemplify our frameworks intention of producing iteratively developed ML methodologies and AI systems to enhance crisis awareness and response using insights gained from crisis managers. \")]),_c('h6',[_vm._v(\"Incorporating Crisis Expert Insights into Model Development & Performance Metric Selection\")]),_c('p',[_vm._v(\" To incorporate the insights of the crisis managers into the design and development of a new text classification model, we first created a classification task and associated \"),_c('strong',[_vm._v(\"classes that align with the expressed information needs of crisis managers during a crisis event\")]),_vm._v(\". Then, we \"),_c('strong',[_vm._v(\"selected a performance evaluation metric that aligns with the priorities of the crisis managers\")]),_vm._v(\" for that task, finally developing a model that is evaluated using the selected performance metric. \")]),_c('h6',[_vm._v(\"Human Risk Text Classification Experiments\")]),_c('p',[_vm._v(\" In the process of conducting this exercise, we performed various classification experiments, experimenting with various text featurizations and classical machine learning algorithms. \")]),_c('h6',[_vm._v(\"Preprocessing & Featurization of Japanese Text\")]),_c('p',[_vm._v(\" We note that since this study focused exclusively on \"),_c('strong',[_vm._v(\"Japanese crisis text\")]),_vm._v(\", we constructed a preprocessing pipeline that uses open-source Japanese tokenizers, stop-words, and a lemmatizer to preprocess the Japanese text. Additionally, we investigated the use of text embeddings of the Japanese crisis text that are created by applying CLS pooling, a process which creates a contextualized numerical embedding of inputted text, using a pretrained Japanese Masked Language Modeling (MLM) BERT model in both our supervised and unsupervised learning experiments. \")]),_c('h6',[_vm._v(\"Clustering Data to Uncover Semantically-similar Groupings\")]),_c('p',[_vm._v(\" Finally, we conclude the development of this ML module on an exploratory note, devising a pipeline that evaluates a combination of text featurizations, dimensionality reduction techniques, and clustering algorithms to provide intuitive groupings of text to help inform the development of text classification tasks in future work. \")]),_c('h6',[_vm._v(\"Evaluation\")]),_c('p',[_vm._v(\" In our evaluation of our text classification experiments, we perform quantitative evaluation, assessing the performance of the model for the task based on the determined performance evaluation metric mentioned above in addition to other metrics, e.g. per-class performance metrics. For our unsupervised experiments, we include both quantitative and qualitative evaluation. Using the Within-Cluster Sum of Squares (WCSS) metric, we determine a set of optimal clustering pipeline configurations and their corresponding optimal number of clusters to use for further investigation. We assess qualitatively by investigating the resulting clusters and determining for each cluster, whether or not the representative documents within that cluster have a cohesive, interpretable label, and if they do, what that label is. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-5\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Summary of Results\")]),_c('h5',[_vm._v(\"Iterating on ML Methodology based on Crisis Managers Insights\")]),_c('p',[_vm._v(\" Our framework was developed to both highlight the importance of involving crisis managers in the process of developing a ML methodology and contextualize model performance among other measures of efficacy for the ML methodology. Since our framework seeks to be used in the development and iteration of an ML methodology based on the insights gained from crisis managers, we iterated on the Text Analysis Module using insights we had gained from our results on the Image Analysis Module. \")]),_c('h5',[_vm._v(\"Developing the Human Risk Task\")]),_c('p',[_vm._v(\" Using labels provided directly to us by crisis managers, we created a new text classification task in an effort to better fulfill their information needs during a crisis event. \"),_c('strong',[_vm._v(\"Using insights gained from the results of image annotation workshops of the Image Analysis Module, we determined F2 score to be an appropriate performance metric for model performance evaluation as false negatives are considered more costly than false positives for assessing human risk from text reports\")]),_vm._v(\". \")]),_c('p',[_c('strong',[_vm._v(\"To the best of our knowledge, the exercises of creating a classification task from labels provided directly by crisis managers and formulating an appropriate model performance metric informed from crisis expert insights are novel contributions of this work.\")]),_vm._v(\" These exercises follow directly inline with our framework, \"),_c('strong',[_vm._v(\"using the results from the Image Analysis Module to iteratively design and develop ML models for the Text Analysis Module.\")])]),_c('h5',[_vm._v(\"Human Risk Task Model Evaluation\")]),_c('p',[_vm._v(\" Using F2 as the metric to optimize for during 5 x 5 Nested Cross Validation (Nested CV), we were able to identify the Support Vector Machine (SVM) algorithm and its corresponding hyperparameter grid as achieving a relatively high mean F2 performance with low variance. To assess the models ability to perform the human risk classification task, we found the tuned \"),_c('strong',[_vm._v(\"SVM model to achieve an F2 score of 92.8%, which is a substantial improvement over the baseline models F2 score of 43.4%\")]),_vm._v(\". Having a baseline is an important aspect of our framework as it enables us the ability to determine if a developed model is performing the task well, i.e. if it does not perform the task better than the baseline, it is not a useful model for the task. This \"),_c('strong',[_vm._v(\"suggests the tuned SVM model is a useful classifier for the task and performs the task reasonably well\")]),_vm._v(\". This is further evidenced from the Precision-Recall curve, where the tuned SVM model achieved an Area Under the Precision-Recall Curve (AUCPR) score of 0.919, which is a significant improvement over the typical baseline classifier used for that metric which achieves an AUCPR of 0.133. We note that recall for the \\\"Human Risk\\\" class is higher than precision likely being a result of using F2 as the performance metric to optimize in the classification experiments. Lastly, when looking at the per-class performance metrics for each class, we see that the model performs reasonably well on both classes achieving scores at or above 0.857 for the \\\"Human Risk\\\" class and at or above 0.976 for the \\\"Not Human Risk\\\" class. \")]),_c('h5',[_vm._v(\"Clustering of Firefighter Crisis Text Reports\")]),_c('p',[_vm._v(\" From our preliminary clustering assessments, we observed that clustering using \"),_c('strong',[_vm._v(\"K-medoids rather than K-means with all else equal (i.e. text featurization and dimensionality reduction technique), typically yielded lower WCSS scores across all  values between 2-20\")]),_vm._v(\". This is likely due to the K-medoids algorithms robustness to outliers and noise, suggesting that there may exist some reports in the corpus which are quite different from the rest. \")]),_c('p',[_vm._v(\" We note that since the corpus we studied was specific to flood and typhoon crisis events, it is no surprise that \"),_c('strong',[_vm._v(\"many of the identified cluster labels are geared towards flood-related information such as \\\"Areas with Flood Risk\\\", \\\"River Water Level and Corresponding Warning for Emergency Operation Center (EOC)/Fire Department (FD)\\\", \\\"Residential Areas/Buildings in Flood (Risk)\\\", and \\\"Landslide/Fallen Tree\\\"\")]),_vm._v(\". Although some of the categories are \"),_c('strong',[_vm._v(\"quite general such as \\\"Rescue (Activities/Requests)\\\", \\\"Closed Roads by the City\\\", and \\\"Impassable Roads (due to Flood/Obstacles/Damage)\\\"\")]),_vm._v(\", we also see that some of the cluster labels are \"),_c('strong',[_vm._v(\"specific to the fire department\")]),_vm._v(\" such as \"),_c('strong',[_vm._v(\"\\\"Areas where FD is active\\\" and \\\"FD Activities/Weather Warning/Flood Control Alert\\\"\")]),_vm._v(\". \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Fukuchiyama Flood Text Reports Data Collection\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-4 pt-1 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"text-data-collection\",\"src\":require(\"../../../../public/assets/txt-data-collection.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 pt-2 pb-4 col-md-6\"},[_c('p',[_vm._v(\" Our crisis management partners in Fukuchiyama City (FC) compiled \"),_c('strong',[_vm._v(\"716 Japanese (JA) text transcripts\")]),_vm._v(\" of radio communications from \"),_c('strong',[_vm._v(\"on-the-ground firefighters\")]),_vm._v(\" which occurred during the following past FC flood events: \")]),_c('div',{staticClass:\"centered-list-parent\"},[_c('ul',{staticClass:\"centered-list\"},[_c('li',[_vm._v(\"Typhoon Manyi in 2013\")]),_c('li',[_vm._v(\"Heavy Rain Event in August 2014\")]),_c('li',[_vm._v(\"Typhoon Lan in 2017\")]),_c('li',[_vm._v(\"Heavy Rain Event in July 2018\")])])]),_c('p',[_vm._v(\" The data collection process which took place during these events is depicted in the adjacent figure. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Fukuchiyama Flood Text Reports Dataset Characteristics\")]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-7 pt-3 pb-3\"},[_c('table',{attrs:{\"id\":\"text-characteristics-table\"}},[_c('tr',[_c('th',[_c('strong',[_vm._v(\"Total Number of Reports\")])]),_c('th',[_c('strong',[_vm._v(\"Reports Labeled for Human Risk\")])]),_c('th',[_c('strong',[_vm._v(\"Reports Labeled for Emergency Operation Center (EOC) Humanitarian Categories\")])]),_c('th',[_c('strong',[_vm._v(\"Unique EOC Humanitarian Categories\")])])]),_c('tr',[_c('td',[_vm._v(\"716\")]),_c('td',[_vm._v(\"715\")]),_c('td',[_vm._v(\"584\")]),_c('td',[_vm._v(\"108\")])])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" We use all 716 reports in our clustering experiments. Since 715 out of the total 716 reports are labeled for Human Risk, we use those labeled reports in our classification experiments. \"),_c('br'),_c('br'),_vm._v(\" To understand how these firefighter crisis text reports compare to other Japanese crisis reports, we compare the FC firefighter reports character length distribution against another Japanese crisis text report corpus, the text of Tokyo crisis reports received by RiskMap (RM) during Typhoon Hagibis in 2019. We note that there are 68 reports in total for the Typhoon Hagibis RM reports dataset. \")])]),_c('div',{staticClass:\"col-10\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"dataset-comparison\",\"src\":require(\"../../../../public/assets/character_box_and_whisk_fc_rm.png\"),\"alt\":\"First slide\"}})]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-6 col-lg-4 pt-3\"},[_c('h6',[_vm._v(\"FC Firefighter Reports Characteristics\")]),_c('div',{staticClass:\"centered-list-parent\"},[_c('ul',{staticClass:\"centered-list\"},[_c('li',[_vm._v(\"N = 716 Reports\")]),_c('li',[_vm._v(\"Median = 22 Characters\")])])])]),_c('div',{staticClass:\"col-6 col-lg-4 pt-3\"},[_c('h6',[_vm._v(\"Typhoon Hagibis RM Reports Characteristics\")]),_c('div',{staticClass:\"centered-list-parent\"},[_c('ul',{staticClass:\"centered-list\",attrs:{\"id\":\"hagibis-details\"}},[_c('li',[_vm._v(\"N = 68 Reports\")]),_c('li',[_vm._v(\"Median = 14 Characters\")])])])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" Most Japanese (JA) crisis text reports are between only a \"),_c('strong',[_vm._v(\"few characters\")]),_vm._v(\" to about \"),_c('strong',[_vm._v(\"50 characters\")]),_vm._v(\". We also observe that each distribution is right-skewed. This is further seen by Twitter research, which finds that \"),_c('strong',[_vm._v(\"JA tweets\")]),_vm._v(\" have a mode of \"),_c('strong',[_vm._v(\"15 characters\")]),_vm._v(\", with a character distribution exhibiting right-skew. It is noted that \"),_c('strong',[_vm._v(\"English (EN) tweets\")]),_vm._v(\" have a mode of \"),_c('strong',[_vm._v(\"34 characters\")]),_vm._v(\", which as the authors state, \")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h6',[_vm._v(\" This is because in languages like Japanese, Korean, and Chinese you can convey about double the amount of information in one character as you can in many other languages, like English, Spanish, Portuguese, or French\\\"\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnA\",\"id\":\"refA\"}},[_vm._v(\"1\")])])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_c('strong',[_vm._v(\"Comparing these various dataset distributions suggests that the FC firefighter text reports are of similar character length to RM JA reports and JA tweets.\")])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnA\"}},[_vm._v(\"1. A. Rosen and I. Ihara, Giving you more characters to express yourself. \"),_c('a',{attrs:{\"href\":\"https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself\",\"target\":\"_blank\"}},[_vm._v(\"https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself\")]),_vm._v(\", Sept. 2017.\"),_c('a',{attrs:{\"href\":\"#refA\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Text Preprocessing & Featurization Pipeline\")]),_c('div',{staticClass:\"row justify-content-center align-items-center pb-3\"},[_c('div',{staticClass:\"col-12 col-md-9 pb-4\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"text-pipeline\",\"src\":require(\"../../../../public/assets/text-preprocessing.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-9 pb-4\"},[_c('p',[_vm._v(\" In order to use the FC firefighter report text data in our classification and clustering experiments, we needed to featurize, or construct numerical representations (feature vectors) of the text to use as input to ML models. In the pipeline we developed for the featurization of Japanese text, \"),_c('strong',[_vm._v(\"we investigate 4 different featurizations of the text\")]),_vm._v(\", namely, \"),_c('strong',[_vm._v(\"Bag-of-Words (BOW)\")]),_vm._v(\" based on unigram & bigram representations, \"),_c('strong',[_vm._v(\"Term-Frequency Inverse-Document-Frequency (TF-IDF)\")]),_vm._v(\" based on unigram, and finally \"),_c('strong',[_vm._v(\"pretrained BERT Masked Language Model (MLM) embeddings using CLS Pooling\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Depending on the featurization, we integrate various preprocessing steps in order to perform commonplace Natural Language Processing (NLP) preprocessing steps. Due to our limited knowledge of the Japanese language, we make use of popular tokenizers & a lemmatizer pretrained on Japanese text as well as an open-source Japanese stopwords list for preprocessing the JA text. \")]),_c('p',[_vm._v(\" In the following slide we describe the preprocessing and featurization steps of our pipeline as applied to the input data in order to yield the featurizations we have mentioned. Additionally, we note the associated pros and cons of the featurizations. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Text Preprocessing & Featurization Pipeline\")]),_c('h5',[_c('u',[_vm._v(\"BOW & TF-IDF Preprocessing & Featurization\")])]),_c('div',{staticClass:\"row align-items-center justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7 pb-2 pt-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"n-gram-preprocessing\",\"src\":require(\"../../../../public/assets/n-gram-preprocessing.png\"),\"alt\":\"N-gram Preprocessing\"}})]),_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\"Preprocessing\")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" For the BOW based on unigram, BOW based on bigram, and TF-IDF (based on unigram) featurizations, we leverage a popular, open-source JA tokenizer, stopwords list, and lemmatizer to \"),_c('strong',[_vm._v(\"preprocess the raw input text\")]),_c('sup',[_c('a',{attrs:{\"href\":\"#fnB\",\"id\":\"refB\"}},[_vm._v(\"1\")])]),_vm._v(\", these steps can be seen in the figure above: \"),_c('ol',[_c('li',[_vm._v(\" Break up raw report text into \"),_c('strong',[_vm._v(\"word tokens\")]),_vm._v(\", i.e. tokenize: \"),_c('ul',[_c('li',[_vm._v(\"E.g. the road is submerged.  [the, road, is, submerged, .]\")])])]),_c('li',[_vm._v(\" We remove stopwords (e.g. the, as, it, is, .)  otherwise could add noise to the input: \"),_c('ul',[_c('li',[_vm._v(\"E.g. [\"),_c('strong',[_vm._v(\"the\")]),_vm._v(\", road, \"),_c('strong',[_vm._v(\"is\")]),_vm._v(\", submerged, \"),_c('strong',[_vm._v(\".\")]),_vm._v(\"]  [road, submerged]\")])])]),_c('li',[_vm._v(\" We \"),_c('strong',[_vm._v(\"lemmatize\")]),_vm._v(\" word tokens, i.e. convert word to its lemma, or dictionary form: \"),_c('ul',[_c('li',[_vm._v(\"E.g. [road, \"),_c('strong',[_vm._v(\"submerged\")]),_vm._v(\"]  [road, \"),_c('strong',[_vm._v(\"submerge\")]),_vm._v(\"] \")])])])])])]),_c('h6',[_vm._v(\"Featurization\")]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" After preprocessing the raw text, we then use the preprocessed input to form an n-gram representation, which in our work was limited to \"),_c('strong',[_vm._v(\"unigram\")]),_vm._v(\" and \"),_c('strong',[_vm._v(\"bigram\")]),_vm._v(\" representations, but we note our pipeline generalizes to produce n-gram representations. For example, the unigram representation of the processed example used above [\\\"road\\\", \\\"submerge\\\"] would be as [\\\"road\\\", \\\"submerge\\\"] and the bigram representation would be [\\\"road submerge\\\"]. Once the n-gram representation is computed from the preprocessed input, we convert the preprocessed word tokens into the BOW or TF-IDF feature vector representations, or featurizations, which can be used as inputs to ML models. The values of the \"),_c('strong',[_vm._v(\"BOW n-gram features\")]),_vm._v(\" are simply their associated \"),_c('strong',[_vm._v(\"frequency\")]),_vm._v(\" in a text report. We show the resulting feature vector for BOW based on unigram for the [\\\"road\\\", \\\"submerge\\\"] example below: \")])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"bow-unigram-ex\",\"src\":require(\"../../../../public/assets/bag-of-word-features.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" For the TF-IDF featurizations based on unigrams, the feature values are computed by considering both the \"),_c('strong',[_vm._v(\"frequency\")]),_vm._v(\" of the unigrams in the report as well as the \"),_c('strong',[_vm._v(\"occurence of the unigram across all reports\")]),_vm._v(\". This value gives a relative importance to a unigram that considers the unigram in a specific report and across all reports. We show the resulting feature vector for TF-IDF based on unigram for the [\\\"road\\\", \\\"submerge\\\"] example below as if it were part of a collection of reports, or a text corpus: \")])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"tfidf-unigram-ex\",\"src\":require(\"../../../../public/assets/tfidf-features.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-3 pt-3\"},[_c('h6',[_c('u',[_vm._v(\"Pros:\")])]),_c('div',{staticClass:\"centered-list-parent\"},[_c('ul',{staticClass:\"centered-list\"},[_c('li',[_vm._v(\"Interpretable\")]),_c('li',[_c('strong',[_vm._v(\"Language-agnostic\")])])])])]),_c('div',{staticClass:\"col-12 col-lg-4 pt-3\"},[_c('h6',[_c('u',[_vm._v(\"Cons:\")])]),_c('div',{staticClass:\"centered-list-parent\"},[_c('ul',{staticClass:\"centered-list\"},[_c('li',[_vm._v(\"Sparse (many 0's) & High-Dimensional\")]),_c('li',[_vm._v(\"Doesn't do well for \"),_c('strong',[_vm._v(\"Out-of-Vocab (OOV)\")]),_vm._v(\" word tokens\")]),_c('li',[_c('strong',[_vm._v(\"Language-agnostic\")]),_vm._v(\" (i.e. inability to capture specificity to a particular language)\")]),_c('li',[_c('strong',[_vm._v(\"Severely limited ability\")]),_vm._v(\" to capture \"),_c('strong',[_vm._v(\"token similarity, long-range dependencies, and understanding of a language\")])])])])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" Although these n-gram-based featurizations have the benefit of being language-agnostic, we note that they have the limitations of being high-dimensional and sparse in which most entries of the feature vector are zero, an inability to model long-range dependencies between tokens in the context of a document, and a severely limited ability to capture token similarity and understanding of a language. Therefore, we investigate a featurization strategy that yields dense, contextualized document representations specific to Japanese text documents. This strategy uses a pretrained Japanese Masked Language Modeling (MLM) Bidirectional Encoder Representations from Transformers (BERT) model and the CLS pooling technique. \")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnB\"}},[_vm._v(\"1. We note that in this work we make use of the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/fugashi/1.1.2/\",\"target\":\"_blank\"}},[_vm._v(\"fugashi\")]),_vm._v(\" (version: 1.1.2) open-source morphological tool for \"),_c('strong',[_vm._v(\"tokenizing and lemmatizing\")]),_vm._v(\" Japanese text. Since fugashi requires a dictionary to operate, we use the \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/unidic-lite/1.0.8/\",\"target\":\"_blank\"}},[_vm._v(\"Unidic Lite\")]),_vm._v(\" dictionary (version: 1.0.8). Finally, for the stopwords list, we use a versioned, open-source list available \"),_c('a',{attrs:{\"href\":\"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/5a000f6a62f9e3a12f436f36d168e2fcd2fb1878/stopwords-ja.json\",\"target\":\"_blank\"}},[_vm._v(\"here\")]),_vm._v(\".\"),_c('a',{attrs:{\"href\":\"#refB\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Text Preprocessing & Featurization Pipeline\")]),_c('h5',[_c('u',[_vm._v(\"Pretrained Japanese MLM BERT Model Embeddings Preprocessing & Featurization\")])]),_c('div',{staticClass:\"col-12 pb-2 pt-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"bert-features-preprocessing\",\"src\":require(\"../../../../public/assets/bert-preprocessing.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\"Preprocessing\")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" The BERT MLM Deep learning (NN) model is optimized to \"),_c('strong',[_vm._v(\"predict\")]),_vm._v(\" a randomly \"),_c('strong',[_vm._v(\"masked words\")]),_vm._v(\" by using the \"),_c('strong',[_vm._v(\"context\")]),_vm._v(\", or the words that surround them. \")]),_c('p',[_vm._v(\" Tohoku University Researchers \"),_c('strong',[_vm._v(\"pretrained a MLM BERT\")]),_vm._v(\" model using a dataset of approx. \"),_c('strong',[_vm._v(\"30M sentences\")]),_vm._v(\" of the \"),_c('strong',[_vm._v(\"Japanese version of Wikipedia\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Raw text data is word tokenized using Fugashi & Unidic Lite dictionary. Word tokens are further split into subwords using the \"),_c('strong',[_vm._v(\"WordPiece algorithm\")]),_vm._v(\", yielding a token vocabulary of \"),_c('strong',[_vm._v(\"32768 unique tokens.\")]),_c('sup',[_c('a',{attrs:{\"href\":\"#fnC\",\"id\":\"refC\"}},[_vm._v(\"1\")])])])]),_c('h6',[_vm._v(\"Featurization\")]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" The text input tokens resulting from the preprocessing steps are prepended with a \"),_c('strong',[_vm._v(\"[CLS]\")]),_vm._v(\" token, the Classification token. Deep in the BERT model, we extract a \"),_c('strong',[_vm._v(\"contextualized numerical representation\")]),_vm._v(\" of the report for classification tasks by grabbing the \"),_c('strong',[_vm._v(\"final hidden state\")]),_vm._v(\" corresponding to this token. This is \"),_c('strong',[_c('i',[_vm._v(\"CLS Pooling\")])]),_vm._v(\", yielding a dense, contextualized feature vector of 768 dimensions. For brevity, we refer to these feature vectors as \"),_c('strong',[_vm._v(\"BERT embeddings\")]),_vm._v(\". We show an example of a featurized text report below: \")])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"bert-features-ex\",\"src\":require(\"../../../../public/assets/bert-features.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"row justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-5 pt-3\"},[_c('h6',[_c('u',[_vm._v(\"Pros:\")])]),_c('div',[_c('p',[_c('ul',[_c('li',[_c('strong',[_vm._v(\"Optimized for the Japanese language\")])]),_c('li',[_c('strong',[_vm._v(\"Dense features \")]),_vm._v(\" (i.e. lower dimensions than previous n-gram based features & not sparse)\")]),_c('li',[_c('strong',[_vm._v(\"Contextualized representations\")]),_vm._v(\", which can better capture long-range dependencies between words in a report & State-of-the-art \"),_c('strong',[_vm._v(\"language understanding\")])]),_c('li',[_vm._v(\" Better-equipped to handle \"),_c('strong',[_vm._v(\"OOV tokens\")]),_vm._v(\" due to the use of the WordPiece algorithm \")])])])])]),_c('div',{staticClass:\"col-12 col-lg-4 pt-3\"},[_c('h6',[_c('u',[_vm._v(\"Cons:\")])]),_c('p',[_c('ul',[_c('li',[_vm._v(\"Features are \"),_c('strong',[_vm._v(\"NOT interpretable\")])]),_c('li',[_c('strong',[_vm._v(\"Specific to JA\")])]),_c('li',[_c('strong',[_vm._v(\"Not finetuned to crisis text corpus\")]),_vm._v(\", although we note this could be done in a future work\")])])])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" Having preprocessed the text and produced various featurizations with their own pros and cons, we utilized all of the featurizations in our classification experiments discussed in the next slides and TF-IDF based on unigram features as well as BERT embeddings in our clustering experiments. \")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnC\"}},[_vm._v(\"1. \"),_c('a',{attrs:{\"href\":\"https://huggingface.co/cl-tohoku/bert-base-japanese-v2\",\"target\":\"_blank\"}},[_vm._v(\"Link to Pretrained Japanese BERT MLM Model\")]),_c('a',{attrs:{\"href\":\"#refC\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])]),_c('br')])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('h5',[_vm._v(\"Human Risk Text Classification\")]),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" Since 715 out of the 716 firefighter text reports were labeled for the binary \"),_c('strong',[_vm._v(\"Human Risk/No Human Risk classes\")]),_vm._v(\", we chose to focus our text classification experiments on this classification task. Additionally, with the text analysis module, we aimed to develop a task that \"),_c('strong',[_vm._v(\"better met the information needs\")]),_vm._v(\" of crisis managers during a crisis event. We did this by using the labels our crisis management partners in Fukuchiyama provided to us directly. \")])]),_c('h6',[_vm._v(\"Task Description\")]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_c('i',[_vm._v(\" The Human Risk text classification task \"),_c('strong',[_vm._v(\"determines whether or not a crisis text report indicates if there are people in need of rescue from a crisis.\")]),_vm._v(\" This includes people being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), and/or being in need of life-saving emergency medical care. \")])]),_c('p',[_vm._v(\" In an effort to transition from abstract class descriptions to something more specific, e.g. a checklist, we choose to both provide a definition of the task and further detail the Human Risk classes as bulleted lists containing the specific traits/descriptors contained in a text report which are characteristic of each class with the aim of enhancing the clarity of each class and the task overall: \")])]),_c('div',{staticClass:\"col-12 col-md-7 pt-3 pl-md-5\"},[_c('h6',[_c('strong',[_vm._v(\"Human Risk\")]),_vm._v(\" Class Descriptors\")]),_c('div',[_c('p',[_c('ul',[_c('li',[_vm._v(\"Rescue being requested (to the Fire Department (FD))\")]),_c('li',[_vm._v(\"Evacuation support being requested (to the FD)\")]),_c('li',[_vm._v(\"Human missed the chance to evacuate from their own house, at work, shopping center, etc.\")]),_c('li',[_vm._v(\"Vulnerable population (elderly, disabled, small children) being left in the house in the flooding area\")]),_c('li',[_vm._v(\"Water rising inside the house above the floor (human inside)\")]),_c('li',[_vm._v(\"Water current is fast inside the house and hard to move upstairs (human inside) \")]),_c('li',[_vm._v(\"Sediment flowing into the house (human inside)\")]),_c('li',[_vm._v(\"Human being trapped in elevator, submerged car, or a car which is not submerged yet\")]),_c('li',[_vm._v(\"Human being washed away in a river\")]),_c('li',[_vm._v(\"Rescue team dispatched\")]),_c('li',[_vm._v(\"Rescue team in activity (such as helping evacuation, rescuing, etc.)\")]),_c('li',[_vm._v(\"Rescue activity completed\")]),_c('li',[_vm._v(\"Landslide occurrence on the highway - possible vehicle being involved\")])])])])]),_c('div',{staticClass:\"col-12 col-md-4 pt-3\"},[_c('h6',[_c('strong',[_vm._v(\"No Human Risk\")]),_vm._v(\" Class Descriptors\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\"Dam Discharge\")]),_c('li',[_vm._v(\"Meteorological Information\")]),_c('li',[_vm._v(\"River Water Level Information\")]),_c('li',[_vm._v(\"Weather Alert\")]),_c('li',[_vm._v(\"Road Closure\")]),_c('li',[_vm._v(\"Road Flood Risk (not flooded yet)\")]),_c('li',[_vm._v(\"Area Flood Risk (not flooded yet)\")])])])]),_c('div',{staticClass:\"col-12 pb-2\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-risk-classification\",\"src\":require(\"../../../../public/assets/human-risk-diagram.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" In addition to using labels/classes which better meet the information needs of crisis managers during crisis, we aimed to develop the classification model for this task \"),_c('strong',[_vm._v(\"using a performance metric which better aligns with the priorities of the crisis managers\")]),_vm._v(\" as it pertains to this task and also \"),_c('strong',[_vm._v(\"considers the nature of the data (i.e. class imbalance)\")]),_vm._v(\". We discuss these considerations and how we \"),_c('strong',[_vm._v(\"determined the performance metric\")]),_vm._v(\" for this task in the next slides. We note that \"),_c('strong',[_vm._v(\"this process\")]),_vm._v(\" of constructing a task from \"),_c('strong',[_vm._v(\"crisis manager's labels\")]),_vm._v(\" to determining the appropriate performance metric to utilize for developing a model which considers both the \"),_c('strong',[_vm._v(\"properties of the data\")]),_vm._v(\" and the \"),_c('strong',[_vm._v(\"insights we gained from crisis managers\")]),_vm._v(\" is a \"),_c('strong',[_vm._v(\"novel contribution of this research.\")])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnD\"}},[_vm._v(\"1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who determined these class definitions from examining the original Japanese reports.\"),_c('a',{attrs:{\"href\":\"#refD\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('h5',[_vm._v(\"Human Risk Classification - Determination of the Performance Evaluation Metric\")]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" In our determination of the performance metric to use for assessing the performance of the Human Risk classifier we consider both the class imbalance of the task and insights we gained from the workshops conducted in the image analysis module. \")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-riks-label-distribution\",\"src\":require(\"../../../../public/assets/human-risk-label-distribution.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 pt-3 col-md-6\"},[_c('h5',[_vm._v(\"Substantial Class Imbalance\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Across the 715 reports labeled for Human Risk, we observe that there are disporportionately more \\\"No Human Risk\\\" data points than \\\"Human Risk\\\" data points, thus the \"),_c('strong',[_vm._v(\"\\\"Human Risk\\\" class is the minority class\")]),_vm._v(\" for this task. \")]),_c('li',[_c('strong',[_vm._v(\"Accuracy\")]),_vm._v(\" of the classifier which always predicts \\\"No Human Risk\\\": \"),_c('strong',[_vm._v(\"86.7%\")]),_vm._v(\", i.e. the percentage of \\\"No Human Risk\\\" labels in the dataset \")]),_c('li',[_vm._v(\" Need to account for this imbalance in the metric, otherwise conclusions about the model's ability to perform the task can be misleading, as seen with using accuracy as the performance metric \")])])])]),_c('div',{staticClass:\"col-12 col-md-6 pt-md-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"insights-from-workshops\",\"src\":require(\"../../../../public/assets/workshop-insights.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"carousel-text col-12 col-md-6\"},[_c('h5',[_vm._v(\"Insights from Image Annotation Workshops\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" From the workshops, we understand that in assessing the potential for human casualities, the cost associated with \"),_c('strong',[_vm._v(\"NOT investigating the potential for human casualities when there ARE human casualities (False Negative (FN))\")]),_vm._v(\" is considered \"),_c('strong',[_vm._v(\"higher\")]),_vm._v(\" than the cost of \"),_c('strong',[_vm._v(\"investigating potential human casualities when there ARE NONE (False Positive (FP))\")]),_vm._v(\"  \"),_c('strong',[_vm._v(\"Performance on \\\"Human Risk\\\" class is paramount\")]),_vm._v(\". \")]),_c('li',[_c('strong',[_vm._v(\"Accuracy\")]),_vm._v(\" does NOT tell us how well the model performs on the \"),_c('strong',[_vm._v(\"\\\"Human Risk\\\"\")]),_vm._v(\" class \"),_c('strong',[_vm._v(\" Although Recall (Minimizing FNs)\")]),_vm._v(\" & \"),_c('strong',[_vm._v(\"Precision (Minimizing FPs)\")]),_vm._v(\" have different priorities, both focus on the performance of the \\\"Human Risk\\\" class \")]),_c('li',[_vm._v(\" Ideally, we'd like to minimize both FN & FP, which is captured in the \"),_c('strong',[_vm._v(\"F1 score\")]),_vm._v(\", however F1 score treats recall as equally as important as precision \")]),_c('li',[_c('strong',[_vm._v(\"F2 score\")]),_vm._v(\" treats recall as 2x as important as precision, i.e. \"),_c('strong',[_vm._v(\"the relative cost of FN is twice as much as the cost of FP\")])])])])]),_c('div',{staticClass:\"col-12 col-md-7 pb-5\"},[_c('p',[_vm._v(\"  We thus used the \"),_c('strong',[_vm._v(\"properties of the data\")]),_vm._v(\" (i.e. class imbalance) & \"),_c('strong',[_vm._v(\"the insights we gained from crisis experts\")]),_vm._v(\" to determine the performance metric to evaluate the model we develop for the Human Risk task  the \"),_c('strong',[_c('u',[_vm._v(\"F2 score\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('h5',[_vm._v(\"Human Risk Classification - Data Splits & Algorithm Selection\")]),_c('div',{staticClass:\"row align-items-center justify-content-center pb-3\"},[_c('div',{staticClass:\"col-12 col-md-6 pt-3\"},[_c('h5',[_vm._v(\"Train/Test Splits\")]),_c('p',[_vm._v(\" We split the full dataset of 715 reports into non-overlapping train and test splits in percentages of 80%/20%, respectively. Additionally, we preserve the class imbalance using stratified splitting. \")])]),_c('div',{staticClass:\"col-12 col-md-4 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"data-splitting\",\"src\":require(\"../../../../public/assets/human-risk-data-splits.png\"),\"alt\":\"Second slide\"}})])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Nested Cross Validation for Algorithm Selection\")]),_c('p',[_vm._v(\" We were interested in investigating multiple ML algorithms for the Human Risk classification task, each with their own set of tunable hyperparameters. We aimed to determine which algorithm paired with a corresponding hyperparameter grid search procedure (e.g. Grid Search, i.e. fitting a model to each unique hyperparameter combination in the grid), had the best estimated generalization performance and would use that algorithm for the final model evaluation. We note that since we had insufficient data to use train/dev/test splits, we used a variation of K-fold Cross Validation (CV). \")]),_c('p',[_vm._v(\" Since using the same K-fold CV procedure for both performing hyperparameter tuning and estimating generalization performance can yield an estimated generalization performance that is biased and overly-optimistic, we elected to use \"),_c('strong',[_vm._v(\"Nested CV\")]),_vm._v(\". Nested CV is typically inpractical in large data settings as it is substantially more computationally expensive to perform as compared to K-fold CV; for our low-data setting it was feasible to use. Nested CV is a useful variation of CV as it mitigates the bias in the generalization performance estimate of the algorithm and its corresponding search procedure by nesting the hyperparameter optimization within the generalization performance estimation procedure. \")]),_c('p',[_vm._v(\" For the algorithm selection procedure, we investigated the following classification algorithms: \")])]),_c('div',{staticClass:\"col-12\"},[_c('div',{staticClass:\"centered-list-parent\"},[_c('ul',{staticClass:\"centered-list\"},[_c('li',[_vm._v(\"Logistic Regression\")]),_c('li',[_vm._v(\"Decision Tree\")]),_c('li',[_vm._v(\"Random Forest\")]),_c('li',[_vm._v(\"Support Vector Machine\")]),_c('li',[_vm._v(\"Multinomial Naive Bayes\")]),_c('li',[_vm._v(\"K-Nearest Neighbors\")])])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" We note that we perform 5 x 5 Nested CV on the train split data & treat the various text featurizations offered by our featurization pipeline as hyperparameters in the hyperparameter grid for each algorithm. \")]),_c('p',[_vm._v(\" For the model evaluation on the test set, we select the algorithm (and corresponding search procedure) which had the highest relative mean F2 score with the lowest variance across folds (low standard deviation) from the nested CV procedure. \")])]),_c('div',{staticClass:\"col-12 col-md-8 pt-3 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"nested-cv\",\"src\":require(\"../../../../public/assets/nested-cv.png\"),\"alt\":\"Second slide\"}})])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Human Risk Classification\")]),_c('h5',[_vm._v(\"Algorithm Selection Results\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-md-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"algo-selection-table\",\"src\":require(\"../../../../public/assets/nested-cv-table.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-md-5 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"algo-selection-graph\",\"src\":require(\"../../../../public/assets/nested-cv-graph.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" The results presented above are determined from the generalization performance estimation found from the performance (by F2 score) on the outer loop 5-fold CV in Nested CV. We make available the intermediate and final results of Nested CV for each algorithm and corresponding hyperparameter grid.\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnD\",\"id\":\"refD\"}},[_vm._v(\"1\")])])]),_c('p',[_vm._v(\" From the results, we determined that the performance of the \"),_c('strong',[_vm._v(\"Support Vector Machine (SVM)\")]),_vm._v(\" algorithm with its corresponding hyperparameter search procedure yielded the highest mean F2 score, 82.0%, and the lowest standard deviation, 4.22%. \"),_c('strong',[_vm._v(\"We therefore select the SVM algorithm and its corresponding hyperparameter grid for the final human risk model evaluation on the test set.\")])])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnD\"}},[_vm._v(\"1. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module/Classification/Nested%20CV\",\"target\":\"_blank\"}},[_vm._v(\"Link to Results & Hyperparameters of Nested CV\")]),_c('a',{attrs:{\"href\":\"#refD\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around pb-5\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Human Risk Classification\")]),_c('h5',[_vm._v(\"Final Model Evaluation\")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Tuning the SVM Model\")]),_c('p',[_vm._v(\" Prior to performing the final evaluation of the SVM on the test split data, we performed 5-fold CV on the full train split data, applying grid search with the hyperparameter grid associated with the SVM algorithm to find optimal hyperparameter values. The optimal hyperparameter values found for the SVM are shown in the table below: \")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"svm-hyperparameters\",\"src\":require(\"../../../../public/assets/svm-hyperparameters.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" We report the estimated generalization performance of the tuned SVM found from the 5-fold CV mentioned above, noting that this is \"),_c('strong',[_vm._v(\"likely a biased estimate of generalization performance\")]),_vm._v(\" as the 5-fold CV procedure was also used to tune the model. The \"),_c('strong',[_vm._v(\"tuned SVM model\")]),_vm._v(\" achieves a \"),_c('strong',[_vm._v(\"mean F2 score of 85.0%\")]),_vm._v(\" and has a \"),_c('strong',[_vm._v(\"standard deviation of 7.40%\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" After tuning the SVM model to find the optimal hyperparameters above, we fit the SVM algorithm using those optimal hyperparameters on the \"),_c('strong',[_vm._v(\"entire train split data\")]),_vm._v(\". We then use this \"),_c('strong',[_vm._v(\"fitted SVM model to predict on the unseen test split data\")]),_vm._v(\". The model's predictions on the test split yield the final evaluation of the model's generalization performance. As part of our framework, we report aggregate metrics including the F2 and Area Under the Precision-Recall Curve (AUCPR) score and compare our trained model to baseline scores. Lastly, we report per-class performance metrics and the confusion matrix of the model's predictions. \")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"model-evalution-diagram\",\"src\":require(\"../../../../public/assets/human-risk-model-evaluation.png\"),\"alt\":\"Second slide\"}})])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Human Risk Classification\")]),_c('h5',[_vm._v(\"Final Model Evaluation - Results\")])]),_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Aggregate Metrics & Comparison to Baseline Scores\")])]),_c('div',{staticClass:\"col-12 col-md-8\"},[_c('p',[_vm._v(\" We report an \"),_c('strong',[_vm._v(\"F2 score of 92.8% on the test split data.\")]),_vm._v(\" \"),_c('strong',[_vm._v(\"The baseline classifier which always predicts \\\"Human Risk\\\", has an F2 score of 43.4%\")]),_vm._v(\", so the \"),_c('strong',[_vm._v(\"trained classifier is a significant improvement over the baseline classifier\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" In addition to F2, we plot the Precision-Recall curve, visualizing the tradeoff between precision and recall for different classification thresholds used by the classifier when classifying the data. It is advised to use the PR curve over the Receiver Operating Characteristic (ROC) curve in the case of imbalanced data, as ROC can give an optimistic estimate of the classifiers output quality by considering true negatives in the computation, which in high quantity can dramatically lessen the effect of the false positives, false negatives, and true positives in the performance estimate, giving a misleadingly high estimate of performance. \")])]),_c('div',{staticClass:\"col-12\"}),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-4 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"aucpr-curve\",\"src\":require(\"../../../../public/assets/human-risk-aucpr.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-5\"},[_c('p',[_vm._v(\" The better the classifier (higher recall and higher precision), the closer the AUCPR score is to 1. For AUCPR, the performance of a \"),_c('strong',[_vm._v(\"baseline model has a score which is given by the proportion of positive samples to the total number of samples\")]),_vm._v(\" in the test dataset, which in this case is \"),_c('strong',[_vm._v(\"0.133\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" We report an \"),_c('strong',[_vm._v(\"AUCPR of 0.919 for the SVM model\")]),_vm._v(\" on the test split data, a significant improvement over the baseline score. \")])])]),_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Confusion Matrix & Per-Class Metrics\")])]),_c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-4 pt-3 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-risk-confusion-matrix\",\"src\":require(\"../../../../public/assets/human-risk-cm-svm.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-4 pt-3 pb-5\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"human-risk-per-class\",\"src\":require(\"../../../../public/assets/human-risk-per-class-metric.png\"),\"alt\":\"Second slide\"}})])]),_c('div',{staticClass:\"col-12 col-md-8 pb-5\"},[_c('p',[_vm._v(\" From the confusion matrix, we see that the model made very few misclassifications on the test split data. Specifically, the model only misclassified one data point which was labeled as \\\"Human Risk\\\" as \\\"No Human Risk\\\" out of all 19 data points labeled as \\\"Human Risk\\\", thus the model had \"),_c('strong',[_vm._v(\"low false negatives\")]),_vm._v(\". The model misclassified 3 \\\"No Human Risk\\\" data points as \\\"Human Risk\\\" out of a total of 124 \\\"No Human Risk\\\" data points, thus the model predicted \"),_c('strong',[_vm._v(\"3 false positives\")]),_vm._v(\". We note that the model had more false positives than false negatives, but \"),_c('strong',[_vm._v(\"few of each\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" The model performs well by all per-class metrics on the \\\"No Human Risk\\\" class achieving scores at and above 0.976. Comparatively lower performance is observed across all metrics for the \\\"Human Risk\\\" class with precision, recall, and F1 scores of 0.857, 0.947, and 0.9, respectively. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Human Risk Text Classification - Discussion\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" When determing the performance metric for the human risk task, we asked the following technical questions: \"),_c('ol',[_c('li',[_c('strong',[_vm._v(\"Does the metric account for imbalance present in the data distribution?\")])]),_c('li',[_c('strong',[_vm._v(\"Once the metric is determined, what is the performance of the baseline model for the task?\")])])])])])]),_c('p',[_vm._v(\" While these technical questions are no doubt important for assessing model efficacy for the task, we underscore that there were other questions we asked which \"),_c('strong',[_c('u',[_vm._v(\"could only be answered by our engagement with crisis managers.\")])])]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Before we began developing the human risk model, we asked a question about the task itself: \"),_c('strong',[_vm._v(\"Do the classes for the task sufficiently capture the expressed information needs of crisis managers during a crisis?\")]),_c('ul',[_c('li',[_vm._v(\" Since these \"),_c('strong',[_vm._v(\"labels were provided directly by crisis managers\")]),_vm._v(\" and given the \"),_c('strong',[_vm._v(\"crisis manager's insight into the importance of assessing the potential of human casualty\")]),_vm._v(\", we determined that these classes sufficiently capture an important information need of crisis managers during crisis. \")])])]),_c('li',[_vm._v(\" For the determining the performance metric for the task, questions which required crisis manager engagement included: \"),_c('ul',[_c('li',[_c('strong',[_vm._v(\" Does the metric incorporate the priorities of the crisis managers as it relates to the task, e.g. the cost of a false negative is significantly higher than the cost of a false positive for assessing human risk? \")])]),_c('li',[_c('strong',[_vm._v(\" Are there multiple metrics that should considered in assessing model efficacy in performing the classification task, e.g. precision and recall, or F2? \")])])])])])]),_c('p',[_vm._v(\" Asking these questions allowed us to both consider the technical intricacies for the task (i.e. data imbalance and baseline performance) and directly embed the information needs and priorities of crisis managers into our text ML methodology and evaluation, which are important aims of our framework. \")]),_c('p',[_vm._v(\" The \"),_c('strong',[_vm._v(\"significant improvement over the baseline classifier by F2\")]),_vm._v(\" suggests the tuned \"),_c('strong',[_vm._v(\"SVM model is a useful classifier for the human risk task and performs the task reasonably well\")]),_vm._v(\". This is further evidenced from the Precision-Recall curve, where the tuned SVM model achieved an AUCPR score of 0.919, a significant improvement over the typical baseline classifier used for that metric which achieves an AUCPR of 0.133 \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering of Crowdsourced Japanese Crisis Text Data\")]),_c('h5',[_vm._v(\"Overview\")])]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('p',[_vm._v(\" Beyond investigating the human risk classification task, we aimed to explore the Fukuchiyama firefighter flood crisis report corpus to see if we could \"),_c('strong',[_vm._v(\"uncover other coherent categories which may exist in the data\")]),_vm._v(\". These \"),_c('strong',[_vm._v(\"uncovered categories could inform the development of classification tasks in future work\")]),_vm._v(\", in addition to any of the humanitarian categories provided by crisis managers. \")]),_c('p',[_vm._v(\" This exploration is powered by a series of unsupervised learning techniques including dimensionality reduction and clustering. We developed a pipeline that utilizes these unsupervised techniques to perform the clustering experiments we conducted to uncover coherent categories in the data. \")]),_c('p',[_vm._v(\" For our clustering experiments, we note that we use \"),_c('strong',[_vm._v(\"all 716 FC firefighter crisis reports\")]),_vm._v(\". Additionally, we focus on using the \"),_c('strong',[_vm._v(\"TF-IDF based on unigrams embeddings\")]),_vm._v(\" and \"),_c('strong',[_vm._v(\"Pretrained Japanese BERT embeddings\")]),_vm._v(\", which we refer to as BERT embeddings for brevity. Since these featurizations are 1489 and 768 dimensions respectively, this motivated our incorporation of dimensionality reduction techniques into our clustering pipeline. \")]),_c('div',{staticClass:\"col-12 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"clustering-features\",\"src\":require(\"../../../../public/assets/text-features-for-clustering.png\"),\"alt\":\"Second slide\"}})])]),_c('div',{staticClass:\"col-12\"},[_c('h6',[_vm._v(\"Evaluation Overview\")])]),_c('div',{staticClass:\"col-12 col-md-7 pb-5\"},[_c('p',[_vm._v(\" The evaluation of our clustering experiments consisted of multiple stages. \"),_c('strong',[_vm._v(\"First, we perform quantitative analysis\")]),_vm._v(\", producing \"),_c('strong',[_vm._v(\"Within-Cluster Sum of Squares (WCSS)\")]),_vm._v(\", or \\\"Elbow\\\" plots for each combination of featurization type, dimensionality reduction technique, and clustering algorithm (12 combos in total). We refer to these as configuration combinations. Using these plots, we identify \"),_c('strong',[_vm._v(\"a query subset\")]),_vm._v(\" of the combinations to further investigate for our \"),_c('strong',[_vm._v(\"qualitative evaluation\")]),_vm._v(\". In the first stage of our qualitative evaluation we used the \"),_c('strong',[_vm._v(\"english translations of the closest documents to each cluster center\")]),_vm._v(\" to select a configuration combination from the query subset for the \"),_c('strong',[_vm._v(\"final stage of qualitative assessment\")]),_vm._v(\". In the final stage, a \"),_c('strong',[_vm._v(\"fluent Japanese speaker\")]),_vm._v(\" investigated the raw Japanese reports for the clusters found by the selected configuration combination and \"),_c('strong',[_vm._v(\"determined a human-interpretable label to describe the cluster overall\")]),_vm._v(\" for each cluster. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row align-items-center justify-content-around\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering Pipeline & Experiments\")])]),_c('div',{staticClass:\"col-12 col-md-5 pt-3 pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"clustering-pipeline\",\"src\":require(\"../../../../public/assets/clustering-pipeline.png\"),\"alt\":\"Pipeline for Clustering\"}})]),_c('div',{staticClass:\"col-12 col-md-7 pb-5\"},[_c('h6',[_vm._v(\"Featurize, Reduce Dimensions, and Cluster\")]),_c('p',[_vm._v(\" Using the devised clustering pipeline, we can sequentially reduce the dimensions of the input features to \"),_c('strong',[_vm._v(\"2-dimensions\")]),_vm._v(\", using \"),_c('strong',[_vm._v(\"Principle Component Analysis (PCA)\")]),_vm._v(\", \"),_c('strong',[_vm._v(\"t-distributed Stochastic Neighbor Embedding (t-SNE)\")]),_vm._v(\", or we \"),_c('strong',[_vm._v(\"do not apply dimensionality reduction\")]),_vm._v(\" at all. We finally cluster the reduced data using either \"),_c('strong',[_vm._v(\"K-means\")]),_vm._v(\" or \"),_c('strong',[_vm._v(\"K-medoids\")]),_vm._v(\", which is more robust to outliers present in the data. These hyperparameters to the clustering pipeline are summarized in the adjacent figure. \")]),_c('div',{staticClass:\"row justify-content-center pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"clustering-hyperparameters\",\"src\":require(\"../../../../public/assets/clustering-hyperparameters.png\"),\"alt\":\"Hyperparameters for Clustering\"}})]),_c('h6',[_vm._v(\"Outputs of Clustering Pipeline\")]),_c('p',[_c('strong',[_vm._v(\"Having selected a hyperparameter configuration and a K-value to use\")]),_vm._v(\", our clustering pipeline produces the clustered data points, the text associated with the \"),_c('strong',[_vm._v(\"20 closest documents to the cluster center (in raw JA text & EN translations)\")]),_vm._v(\" for each cluster, and the top 20 unigrams (in JA) by TF-IDF score for the document formed from concatenating the documents in a cluster, for each cluster, forming a cluster-level document corpus. We note that our pipeline can take any positive values x & y for displaying the top x unigrams or top y documents in a cluster. \")])]),_c('div',{staticClass:\"col-12 col-md-7 pb-5\"},[_c('h5',[_vm._v(\"Clustering Experiments - Identifying the Query Subset\")]),_c('p',[_c('ol',[_c('li',[_c('p',[_vm._v(\" We investigated all 12 featurization, dimensionality reduction, and clustering algorithm combinations by investigating the corresponding \"),_c('strong',[_vm._v(\"WCSS or Elbow plot\")]),_vm._v(\" for \"),_c('strong',[_vm._v(\"K = 2, , 20 clusters\")])]),_c('p',[_c('strong',[_vm._v(\"WCSS\")]),_vm._v(\" captures extent to which data points within a cluster are at a close distance to each other, ideally we want this to be low, but not too low. Minimizing WCSS is the same as maximizing the distance between data points in different clusters. \")])])])]),_c('div',{staticClass:\"pb-3\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"elbow-plot\",\"src\":require(\"../../../../public/assets/elbow-plot.png\"),\"alt\":\"Second slide\"}})]),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"2\"}},[_vm._v(\" We selected a subset of combinations which have \"),_c('strong',[_vm._v(\"relatively lower WCSS scores\")]),_vm._v(\" across all K values and \"),_c('strong',[_vm._v(\"have an elbow in the elbow plot\")]),_vm._v(\" to qualitatively investigate further. We call this the \"),_c('strong',[_vm._v(\"query subset\")]),_vm._v(\". \")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering Qualitative Evaluation & Results\")]),_c('h5',[_vm._v(\"Preliminary Qualitative Assessment (in EN): Investigating the Query Subset\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-5\",attrs:{\"id\":\"workflow-and-configs\"}},[_c('h6',[_vm._v(\"Preliminary Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-evaluation-workflow\",\"src\":require(\"../../../../public/assets/preliminary-assessment.png\"),\"alt\":\"Second slide\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Query Subset\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"query-subset\",\"src\":require(\"../../../../public/assets/query-subset.png\"),\"alt\":\"Configuration Combinations in the Query Subset\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Qualitative Summaries of Clusters and Possible Labels\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-summaries\",\"src\":require(\"../../../../public/assets/qualitative-summaries.png\"),\"alt\":\"Qualitative Summaries of Resultant Clustering for Each Cluster\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-1\",attrs:{\"id\":\"workflow\"}},[_c('h6',[_vm._v(\"Preliminary Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-evaluation-workflow\",\"src\":require(\"../../../../public/assets/preliminary-assessment.png\"),\"alt\":\"Second slide\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-2 pb-md-5\"},[_c('p',[_c('ol',[_c('li',[_c('p',[_c('strong',[_vm._v(\"For each combination in the subset, we determine an elbow\")]),_vm._v(\" from the corresponding elbow plot as the K value to use in our qualitative analysis. \")])])])]),_c('img',{staticClass:\"img-fluid pb-3\",attrs:{\"id\":\"identified-elbow\",\"src\":require(\"../../../../public/assets/identified-elbow.png\"),\"alt\":\"Second slide\"}}),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"2\"}},[_c('p',[_vm._v(\" For each cluster, we investigate the \"),_c('strong',[_vm._v(\"top 20 reports within a cluster which were closest to the cluster center\")]),_vm._v(\". We note that for the \"),_c('strong',[_vm._v(\"preliminary assessment\")]),_vm._v(\", we used \"),_c('strong',[_vm._v(\"English translations of the reports given by DeepL neural translation.\")]),_c('sup',[_c('a',{attrs:{\"href\":\"#fnE\",\"id\":\"refE\"}},[_vm._v(\"1\")])])]),_c('p',[_vm._v(\" When investigating the representative reports in each cluster, we answered the question: \")])])])]),_c('p',{attrs:{\"id\":\"research-question\"}},[_c('strong',[_vm._v(\"When looked at together, does the content of the representative reports in a cluster elicit an interpretable label? If so, what is it?\")])]),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"3\"}},[_c('p',[_vm._v(\" We then identified the configuration combination in the query subset which yielded \"),_c('strong',[_vm._v(\"the most interpretable labels across combinations\")]),_vm._v(\", i.e. selected the combination which had the highest number of clusters that had representative documents which elicited an interpretable label. \")]),_c('p',[_vm._v(\" In the neighboring graphics, we showcase the configuration combinations in the query subset and we report qualitative summaries for each of the cluster configurations. We note that the configuration combination which gave the highest number of clusters which had a coherent, interpretable label (9 in total) was the \"),_c('strong',[_vm._v(\"BERT embedding, t-SNE (2 components), and K-medoids clustering\")]),_vm._v(\" combination, which is \"),_c('strong',[_vm._v(\"bolded\")]),_vm._v(\". \")])])])])]),_c('div',{staticClass:\"col-12 pt-3 pb-5\",attrs:{\"id\":\"configs\"}},[_c('h6',[_vm._v(\"Query Subset\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"query-subset\",\"src\":require(\"../../../../public/assets/query-subset.png\"),\"alt\":\"Configuration Combinations in the Query Subset\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Qualitative Summaries of Clusters and Possible Labels\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-summaries\",\"src\":require(\"../../../../public/assets/qualitative-summaries.png\"),\"alt\":\"Qualitative Summaries of Resultant Clustering for Each Cluster\"}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnE\"}},[_vm._v(\"1. \"),_c('a',{attrs:{\"href\":\"https://www.deepl.com/en/translator\",\"target\":\"_blank\"}},[_vm._v(\"Link to DeepL.\")]),_vm._v(\" We acknowledge Saeko Baird of the Urban Risk Lab at MIT who cleaned these translations of their inaccuracies.\"),_c('a',{attrs:{\"href\":\"#refE\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around pb-4\"},[_c('div',{staticClass:\"col-12\"},[_c('h5',[_vm._v(\"Clustering Qualitative Evaluation & Results\")]),_c('h5',[_vm._v(\"Final Qualitative Assessment (in JA): Investigating the Optimal Configuration Combination determined from the Preliminary Assessment\")])]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-0\",attrs:{\"id\":\"workflow-and-clusters\"}},[_c('h6',[_vm._v(\"Final Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-final-evaluation-workflow\",\"src\":require(\"../../../../public/assets/final-assessment.png\"),\"alt\":\"Final Qualitative Assessment Workflow\"}}),_c('br'),_c('br'),_c('h6',[_vm._v(\"Unlabeled Clusters\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"unlabled-clusters-img\",\"src\":require(\"../../../../public/assets/unlabeled-clusters.png\"),\"alt\":\"Unlabeled Clusters\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-3 pb-1\",attrs:{\"id\":\"final-assessment-workflow\"}},[_c('h6',[_vm._v(\"Final Assessment Workflow\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"qualitative-final-evaluation-workflow\",\"src\":require(\"../../../../public/assets/final-assessment.png\"),\"alt\":\"Final Qualitative Assessment Workflow\"}})]),_c('div',{staticClass:\"col-12 col-md-6 pt-2\",attrs:{\"id\":\"steps-with-unlabeled-clusters\"}},[_c('p',[_c('ol',[_c('li',[_c('p',[_vm._v(\" Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \"),_c('strong',[_vm._v(\"top 20 closest reports\")]),_vm._v(\" within the cluster to the cluster center & the \"),_c('strong',[_vm._v(\"top 20 unigrams by TF-IDF score for each cluster document in the cluster-level document corpus\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters. \")])])])]),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('h6',[_vm._v(\"Unlabeled Clusters\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"unlabeled-clusters-img\",\"src\":require(\"../../../../public/assets/unlabeled-clusters.png\"),\"alt\":\"Unlabeled Clusters\"}})]),_c('p',[_c('ol',[_c('li',{attrs:{\"value\":\"2\"}},[_c('p',[_vm._v(\" Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnF\",\"id\":\"refF\"}},[_vm._v(\"1\")])]),_vm._v(\" investigated each cluster and assigned an interpretable label to it. \")]),_c('p',[_vm._v(\" The interpretable label given for each cluster is depicted in the figures below: \")])])])]),_c('div',{staticClass:\"col-12\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"cluster-labels\",\"src\":require(\"../../../../public/assets/cluster-labels.png\"),\"alt\":\"Cluster Labels\"}})])]),_c('div',{staticClass:\"col-12 col-md-6\",attrs:{\"id\":\"steps-without-unlabeled-clusters\"}},[_c('p',[_c('ol',[_c('li',[_c('p',[_vm._v(\" Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \"),_c('strong',[_vm._v(\"top 20 closest reports\")]),_vm._v(\" within the cluster to the cluster center & the \"),_c('strong',[_vm._v(\"top 20 unigrams by TF-IDF score for each cluster document in the cluster-level document corpus\")]),_vm._v(\". \")]),_c('p',[_vm._v(\" Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters. \")])]),_c('li',[_c('p',[_vm._v(\" Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA\"),_c('sup',[_c('a',{attrs:{\"href\":\"#fnF\",\"id\":\"refF\"}},[_vm._v(\"1\")])]),_vm._v(\" investigated each cluster and assigned an interpretable label to it. \")]),_c('p',[_vm._v(\" The interpretable label given for each cluster is depicted in the figures below: \")])])])]),_c('div',{staticClass:\"col-12\"},[_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"cluster-labels\",\"src\":require(\"../../../../public/assets/cluster-labels.png\"),\"alt\":\"Cluster Labels\"}})])]),_c('div',{staticClass:\"col-12 col-md-7 pb-3\",attrs:{\"id\":\"labeled-clusters\"}},[_c('p',{attrs:{\"id\":\"process-arrow\"}},[_vm._v(\"\")]),_c('h6',[_vm._v(\"Labeled Clusters\")]),_c('img',{staticClass:\"img-fluid\",attrs:{\"id\":\"labeled-clusters-img\",\"src\":require(\"../../../../public/assets/labeled-clusters.png\"),\"alt\":\"Labeled Clusters\"}})]),_c('div',{staticClass:\"col-12 col-md-7\"},[_c('hr'),_c('p',[_c('sup',{attrs:{\"id\":\"fnF\"}},[_vm._v(\"1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who assigned an interpretable label to each cluster.\"),_c('a',{attrs:{\"href\":\"#refF\",\"title\":\"Jump back to footnote 1 in the text.\"}},[_vm._v(\"\")])])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around\"},[_c('div',{staticClass:\"carousel-text col-12 col-md-7\"},[_c('h5',[_vm._v(\"Clustering Firefighter Flood Crisis Reports - Discussion\")]),_c('p',[_c('ul',[_c('li',[_vm._v(\" Clustering results suggest what is deemed important to report during flood crisis by FC firefighters on-the-ground. \")]),_c('li',[_vm._v(\" Since we applied the clustering on-the-ground firefighter reports, these results can be used to devise \"),_c('strong',[_vm._v(\"classification tasks with labels which better embed the information needs of crisis managers\")]),_vm._v(\" & can be cross-referenced with crisis managers. \")]),_c('li',[_vm._v(\" Experiment can also be applied on Japanese RiskMap reports or crisis tweets to see if similar cluster labels are unveiled by resident reporting. \")]),_c('li',[_vm._v(\" This method has the \"),_c('strong',[_vm._v(\"drawback of permitting data points to be in only one cluster (hard clustering)\")]),_vm._v(\", and we observed that some of the data points have content which is indicative of multiple of the unveiled interpretable labels. \")])])])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h3>Text Analysis Module</h3>\n        </div>\n        <div class=\"col-12 col-md-8\">\n            <div id=\"TextAnalysisCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"2\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"3\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"4\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"5\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"6\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"7\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"8\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"9\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"10\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"11\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"12\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"13\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"14\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"15\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"16\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"17\"></li>\n                    <li data-bs-target=\"#TextAnalysisCarousel\" data-bs-slide-to=\"18\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item cc-carousel-item active\">\n                        <div class=\"row align-items-center justify-content-around pb-4\">\n                            <div class=\"col-12 col-md-7 pt-3 pb-3\">\n                                <img id=\"text-analysis-module\" class=\"img-fluid\" src=\"../../../../public/assets/text-analysis-module.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Text Analysis Methodology Overview</h5>\n                                <p> \n                                    The Text Analysis Module aims to provide accurate and efficient classifications of\n                                    crisis reports using the text modality that is often present in crowdsourced reports. Another aim\n                                    was to incorporate the insights we gained from the results of our qualitative analysis\n                                    of the Image Analysis Module that were transferable\n                                    between the data modalities, i.e. the importance of identifying potential for human\n                                    casualty or risk to humans. We did this to exemplify our frameworks intention of\n                                    producing iteratively developed ML methodologies and AI systems to enhance crisis\n                                    awareness and response using insights gained from crisis managers.\n                                </p>\n                                <h6>Incorporating Crisis Expert Insights into Model Development & Performance Metric Selection</h6>\n                                <p>\n                                    To incorporate the insights of the crisis managers into the design and development\n                                    of a new text classification model, we first created a classification task and associated\n                                    <strong>classes that align with the expressed information needs of crisis managers during a\n                                    crisis event</strong>. Then, we <strong>selected a performance evaluation metric that aligns with the\n                                    priorities of the crisis managers</strong> for that task, finally developing a model that is\n                                    evaluated using the selected performance metric.\n                                </p>\n                                <h6>Human Risk Text Classification Experiments</h6>\n                                <p>\n                                    In the process of conducting this exercise, we performed various classification \n                                    experiments, experimenting with various text featurizations and classical machine learning\n                                    algorithms.\n                                </p>\n                                <h6>Preprocessing & Featurization of Japanese Text</h6>\n                                <p>\n                                    We note that since this study focused exclusively on <strong>Japanese crisis text</strong>, we\n                                    constructed a preprocessing pipeline that uses open-source Japanese tokenizers, \n                                    stop-words, and a lemmatizer to preprocess the Japanese text. Additionally, we \n                                    investigated the use of text embeddings of the Japanese crisis text that are created by\n                                    applying CLS pooling, a process which creates a contextualized numerical embedding\n                                    of inputted text, using a pretrained Japanese Masked Language Modeling (MLM)\n                                    BERT model in both our supervised and unsupervised learning experiments.\n                                </p>\n                                <h6>Clustering Data to Uncover Semantically-similar Groupings</h6>\n                                <p>\n                                    Finally, we conclude the development of this ML module on an exploratory note,\n                                    devising a pipeline that evaluates a combination of text featurizations, dimensionality\n                                    reduction techniques, and clustering algorithms to provide intuitive groupings of text\n                                    to help inform the development of text classification tasks in future work.\n                                </p>\n                                <h6>Evaluation</h6>\n                                <p>\n                                    In our evaluation of our text classification experiments, we perform quantitative\n                                    evaluation, assessing the performance of the model for the task based on the \n                                    determined performance evaluation metric mentioned above in addition to other metrics, e.g. per-class\n                                    performance metrics. For our unsupervised experiments, we include both \n                                    quantitative and qualitative evaluation. Using the Within-Cluster Sum of Squares (WCSS)\n                                    metric, we determine a set of optimal clustering pipeline configurations and their\n                                    corresponding optimal number of clusters to use for further investigation. We assess\n                                    qualitatively by investigating the resulting clusters and determining for each cluster,\n                                    whether or not the representative documents within that cluster have a cohesive,\n                                    interpretable label, and if they do, what that label is.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-5\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Summary of Results</h5>\n                                <h5>Iterating on ML Methodology based on Crisis Managers Insights</h5>\n                                <p>\n                                    Our framework was developed to both highlight the importance of involving crisis\n                                    managers in the process of developing a ML methodology and contextualize model\n                                    performance among other measures of efficacy for the ML methodology. Since our\n                                    framework seeks to be used in the development and iteration of an ML methodology\n                                    based on the insights gained from crisis managers, we iterated on the Text Analysis\n                                    Module using insights we had gained from our results on the Image Analysis Module.\n                                </p>\n                                <h5>Developing the Human Risk Task</h5>\n                                <p>\n                                    Using labels provided directly to us by crisis managers, we created a new text\n                                    classification task in an effort to better fulfill their information needs during a crisis\n                                    event. <strong>Using insights gained from the results of image annotation workshops of the\n                                    Image Analysis Module, we determined F2 score to be an appropriate\n                                    performance metric for model performance evaluation as false negatives are considered more\n                                    costly than false positives for assessing human risk from text reports</strong>. \n                                </p>\n                                <p>\n                                    <strong>To the best\n                                    of our knowledge, the exercises of creating a classification task from labels provided\n                                    directly by crisis managers and formulating an appropriate model performance metric informed\n                                    from crisis expert insights are novel contributions of this work.</strong> These exercises follow\n                                    directly inline with our framework, <strong>using the results\n                                    from the Image Analysis Module to iteratively design and develop ML models for the\n                                    Text Analysis Module.</strong>\n                                </p>\n                                <h5>Human Risk Task Model Evaluation</h5>\n                                <p>\n                                    Using F2 as the metric to optimize for during 5 x 5 Nested Cross Validation (Nested CV), we were able to\n                                    identify the Support Vector Machine (SVM) algorithm and its corresponding hyperparameter grid as achieving a\n                                    relatively high mean F2 performance with low variance. To assess the models ability\n                                    to perform the human risk classification task, we found the tuned <strong>SVM model to\n                                    achieve an F2 score of 92.8%, which is a substantial improvement over the baseline\n                                    models F2 score of 43.4%</strong>. Having a baseline is an important aspect of our framework\n                                    as it enables us the ability to determine if a developed model is performing the task\n                                    well, i.e. if it does not perform the task better than the baseline, it is not a useful\n                                    model for the task. This <strong>suggests the tuned SVM model is a useful classifier for\n                                    the task and performs the task reasonably well</strong>. This is further evidenced from the\n                                    Precision-Recall curve, where the tuned SVM model achieved an Area Under the Precision-Recall Curve (AUCPR) score of\n                                    0.919, which is a significant improvement over the typical baseline classifier used for\n                                    that metric which achieves an AUCPR of 0.133. We note that recall for the \"Human\n                                    Risk\" class is higher than precision likely being a result of using F2 as the performance\n                                    metric to optimize in the classification experiments. Lastly, when looking at the \n                                    per-class performance metrics for each class, we see that the model performs reasonably\n                                    well on both classes achieving scores at or above 0.857 for the \"Human Risk\" class\n                                    and at or above 0.976 for the \"Not Human Risk\" class.\n                                </p>\n                                <h5>Clustering of Firefighter Crisis Text Reports</h5>\n                                <p>\n                                    From our preliminary clustering assessments, we observed that clustering using\n                                    <strong>K-medoids rather than K-means with all else equal (i.e. text featurization and dimensionality reduction technique), \n                                    typically yielded lower WCSS scores across all  values between 2-20</strong>. This is likely due to the K-medoids algorithms robustness to\n                                    outliers and noise, suggesting that there may exist some reports in the corpus which\n                                    are quite different from the rest.\n                                </p>\n                                <p>\n                                    We note that since the corpus we studied was specific to flood and typhoon crisis events, \n                                    it is no surprise that <strong>many of the identified cluster labels are geared\n                                    towards flood-related information such as \"Areas with Flood Risk\", \"River Water\n                                    Level and Corresponding Warning for Emergency Operation Center (EOC)/Fire Department (FD)\", \"Residential Areas/Buildings in\n                                    Flood (Risk)\", and \"Landslide/Fallen Tree\"</strong>. Although some of the categories are\n                                    <strong>quite general such as \"Rescue (Activities/Requests)\", \"Closed Roads by the City\",\n                                    and \"Impassable Roads (due to Flood/Obstacles/Damage)\"</strong>, we also see that some\n                                    of the cluster labels are <strong>specific to the fire department</strong> such as <strong>\"Areas where FD is\n                                    active\" and \"FD Activities/Weather Warning/Flood Control Alert\"</strong>.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Fukuchiyama Flood Text Reports Data Collection</h5>\n                        <div class=\"row align-items-center justify-content-center\">\n                            <div class=\"col-12 col-md-4 pt-1 pb-md-5\">\n                                <img id=\"text-data-collection\" class=\"img-fluid\" src=\"../../../../public/assets/txt-data-collection.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 pt-2 pb-4 col-md-6\">\n                                <p>\n                                    Our crisis management partners in Fukuchiyama City (FC) compiled <strong>716 Japanese (JA) text transcripts</strong>\n                                    of radio communications from <strong>on-the-ground firefighters</strong> which occurred during the following past FC flood events:\n                                </p>\n                                    <div class=\"centered-list-parent\">\n                                        <ul class=\"centered-list\">\n                                            <li>Typhoon Manyi in 2013</li>\n                                            <li>Heavy Rain Event in August 2014</li>\n                                            <li>Typhoon Lan in 2017</li>\n                                            <li>Heavy Rain Event in July 2018</li>\n                                        </ul>\n                                    </div>\n                                <p>\n                                    The data collection process which took place during these events is depicted in the adjacent figure.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Fukuchiyama Flood Text Reports Dataset Characteristics</h5>\n                        <div class=\"row align-items-center justify-content-center\">\n                            <div class=\"col-12 col-md-7 pt-3 pb-3\">\n                                <table id=\"text-characteristics-table\">\n                                        <tr>\n                                            <th><strong>Total Number of Reports</strong></th>\n                                            <th><strong>Reports Labeled for Human Risk</strong></th>\n                                            <th><strong>Reports Labeled for Emergency Operation Center (EOC) Humanitarian Categories</strong></th>\n                                            <th><strong>Unique EOC Humanitarian Categories</strong></th>\n                                        </tr>\n                                        <tr>\n                                            <td>716</td>\n                                            <td>715</td>\n                                            <td>584</td>\n                                            <td>108</td>\n                                        </tr>\n                                    </table>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    We use all 716 reports in our clustering experiments. Since 715 out of the total 716 reports are labeled for Human Risk, we \n                                    use those labeled reports in our classification experiments. \n                                    <br>\n                                    <br>\n                                    To understand how these firefighter crisis text reports compare to other Japanese crisis reports, we compare the FC firefighter reports character length distribution against another Japanese\n                                    crisis text report corpus, the text of Tokyo crisis reports received by RiskMap (RM)\n                                    during Typhoon Hagibis in 2019. We note that there are 68 reports in total for the\n                                    Typhoon Hagibis RM reports dataset. \n                                </p>\n                            </div>\n                            <div class=\"col-10\">\n                                <img id=\"dataset-comparison\" src=\"../../../../public/assets/character_box_and_whisk_fc_rm.png\" class=\"img-fluid\" alt=\"First slide\">\n                            </div>\n                            <div class=\"row justify-content-center\">\n                                <div class=\"col-6 col-lg-4 pt-3\">\n                                    <h6>FC Firefighter Reports Characteristics</h6>\n                                    <div class=\"centered-list-parent\">\n                                        <ul class=\"centered-list\">\n                                            <li>N = 716 Reports</li>\n                                            <li>Median = 22 Characters</li>\n                                        </ul> \n                                    </div>\n                                </div>\n                                <div class=\"col-6 col-lg-4 pt-3\">\n                                    <h6>Typhoon Hagibis RM Reports Characteristics</h6>\n                                    <div class=\"centered-list-parent\">\n                                        <ul class=\"centered-list\" id=\"hagibis-details\">\n                                            <li>N = 68 Reports</li>\n                                            <li>Median = 14 Characters</li>\n                                        </ul> \n                                    </div>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    Most Japanese (JA) crisis text reports are between only a <strong>few characters</strong> to about <strong>50 characters</strong>. We also observe that each distribution is right-skewed. This is further seen by Twitter research, which \n                                    finds that <strong>JA tweets</strong> have a mode of <strong>15 characters</strong>, with a character distribution exhibiting right-skew. It is noted\n                                    that <strong>English (EN) tweets</strong> have a mode of <strong>34 characters</strong>, which as the authors state,\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <h6>\n                                    This is because in languages like Japanese, Korean, \n                                    and Chinese you can convey about double the amount of \n                                    information in one character as you can in many other languages, \n                                    like English, Spanish, Portuguese, or French\"<sup><a href=\"#fnA\" id=\"refA\">1</a></sup>\n                                </h6>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    <strong>Comparing these various dataset distributions suggests that the FC firefighter text reports are of similar character length to RM JA reports and JA tweets.</strong>\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                <sup id=\"fnA\">1. A. Rosen and I. Ihara, Giving you more characters to express yourself. <a href=\"https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself\" target=\"_blank\">https://blog.twitter.com/en_us/topics/product/2017/Giving-you-more-characters-to-express-yourself</a>, Sept. 2017.<a href=\"#refA\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                                </p>\n                                <br>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Text Preprocessing & Featurization Pipeline</h5>\n                        <div class=\"row justify-content-center align-items-center pb-3\">\n                            <div class=\"col-12 col-md-9 pb-4\">\n                                <img id=\"text-pipeline\" class=\"img-fluid\" src=\"../../../../public/assets/text-preprocessing.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-9 pb-4\">\n                                <p>\n                                    In order to use the FC firefighter report text data in our classification and clustering experiments, we needed to featurize, or construct\n                                    numerical representations (feature vectors) of the text to use as input to ML models. In the pipeline we developed for the featurization of Japanese text, <strong>we investigate 4 different featurizations of the text</strong>, namely,\n                                    <strong>Bag-of-Words (BOW)</strong> based on unigram & bigram representations, <strong>Term-Frequency Inverse-Document-Frequency (TF-IDF)</strong> based on unigram, and finally <strong>pretrained BERT Masked Language Model (MLM) embeddings using CLS Pooling</strong>.\n                                </p>\n                                <p>\n                                    Depending on the featurization, we integrate various preprocessing steps in order to perform commonplace Natural Language Processing (NLP) preprocessing\n                                    steps. Due to our limited knowledge of the Japanese language, we make use of popular tokenizers & a lemmatizer pretrained on Japanese text as well as an open-source Japanese stopwords list for preprocessing the JA text.\n                                </p>\n                                <p>\n                                    In the following slide we describe the preprocessing and featurization steps of our pipeline as applied to the input data in order to yield the featurizations we have mentioned. Additionally,\n                                    we note the associated pros and cons of the featurizations.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Text Preprocessing & Featurization Pipeline</h5>\n                        <h5><u>BOW & TF-IDF Preprocessing & Featurization</u></h5>\n                        <div class=\"row align-items-center justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7 pb-2 pt-2\">\n                                <img id=\"n-gram-preprocessing\" class=\"img-fluid\" src=\"../../../../public/assets/n-gram-preprocessing.png\" alt=\"N-gram Preprocessing\">\n                            </div>\n                            <div class=\"col-12\">\n                                <h6>Preprocessing</h6>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    For the BOW based on unigram, BOW based on bigram, and TF-IDF (based on unigram) featurizations, we leverage a popular, open-source JA tokenizer,\n                                    stopwords list, and lemmatizer to <strong>preprocess the raw input text</strong><sup><a href=\"#fnB\" id=\"refB\">1</a></sup>, these steps can be seen in the figure above:\n                                    <ol>\n                                        <li>\n                                            Break up raw report text into <strong>word tokens</strong>, i.e. tokenize:\n                                            <ul>\n                                                <li>E.g. the road is submerged.  [the, road, is, submerged, .]</li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            We remove stopwords (e.g. the, as, it, is, .)  otherwise could add noise to the input:\n                                            <ul>\n                                                <li>E.g. [<strong>the</strong>, road, <strong>is</strong>, submerged, <strong>.</strong>]  [road, submerged]</li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            We <strong>lemmatize</strong> word tokens, i.e. convert word to its lemma, or dictionary form:\n                                            <ul>\n                                                <li>E.g. [road, <strong>submerged</strong>]  [road, <strong>submerge</strong>] </li>\n                                            </ul>\n                                        </li>\n                                    </ol>\n                                </p>\n                            </div>\n                            <h6>Featurization</h6>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    After preprocessing the raw text, we then use the preprocessed input to form an n-gram representation, which in our work was limited to <strong>unigram</strong> and <strong>bigram</strong> representations, but we note our pipeline generalizes to produce n-gram representations. For example, \n                                    the unigram representation of the processed example used above [\"road\", \"submerge\"] would be\n                                    as [\"road\", \"submerge\"] and the bigram representation would be [\"road submerge\"]. Once the n-gram representation is computed from the preprocessed input, we convert the preprocessed word tokens into the BOW or TF-IDF feature vector representations, or featurizations, which can be used as inputs to ML models.\n                                    The values of the <strong>BOW n-gram features</strong> are simply their associated <strong>frequency</strong> in a text report. We show the resulting feature vector for BOW based on unigram for the [\"road\", \"submerge\"] example below:\n                                </p> \n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"bow-unigram-ex\" class=\"img-fluid\" src=\"../../../../public/assets/bag-of-word-features.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    For the TF-IDF featurizations based on unigrams, the feature values are computed by considering both the <strong>frequency</strong> of the unigrams in the report as well as the <strong>occurence\n                                    of the unigram across all reports</strong>. This value gives a relative importance to a unigram that considers the unigram in a specific report and across all reports. We show the resulting feature vector for TF-IDF based on unigram for the [\"road\", \"submerge\"] example below as if it were part of a collection of reports, or a text corpus:\n                                </p>\n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"tfidf-unigram-ex\" class=\"img-fluid\" src=\"../../../../public/assets/tfidf-features.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"row justify-content-center\">\n                                <div class=\"col-12 col-md-3 pt-3\">\n                                    <h6><u>Pros:</u></h6>\n                                    <div class=\"centered-list-parent\">\n                                        <ul class=\"centered-list\">\n                                            <li>Interpretable</li>\n                                            <li><strong>Language-agnostic</strong></li>\n                                        </ul>\n                                    </div>\n                                </div>\n                                <div class=\"col-12 col-lg-4 pt-3\">\n                                    <h6><u>Cons:</u></h6>\n                                    <div class=\"centered-list-parent\">\n                                        <ul class=\"centered-list\">\n                                            <li>Sparse (many 0's) & High-Dimensional</li>\n                                            <li>Doesn't do well for <strong>Out-of-Vocab (OOV)</strong> word tokens</li>\n                                            <li><strong>Language-agnostic</strong> (i.e. inability to capture specificity to a particular language)</li>\n                                            <li><strong>Severely limited ability</strong> to capture <strong>token similarity, long-range dependencies, and understanding of a language</strong></li>\n                                        </ul> \n                                    </div>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    Although these n-gram-based featurizations have the benefit of being language-agnostic, we note that they have the limitations\n                                    of being high-dimensional and sparse in which most entries of the feature vector are zero, an inability to model long-range dependencies between tokens in the context of a\n                                    document, and a severely limited ability to capture token similarity and understanding of a language. Therefore, we investigate a featurization strategy that yields\n                                    dense, contextualized document representations specific to Japanese text documents. This strategy uses a pretrained Japanese Masked Language Modeling (MLM) Bidirectional Encoder Representations from Transformers (BERT) model and the CLS pooling\n                                    technique.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                    <sup id=\"fnB\">1. We note that in this work we make use of the <a href=\"https://pypi.org/project/fugashi/1.1.2/\" target=\"_blank\">fugashi</a> (version: 1.1.2) open-source morphological tool for <strong>tokenizing and lemmatizing</strong> Japanese text. Since fugashi requires a dictionary to operate, we use the <a href=\"https://pypi.org/project/unidic-lite/1.0.8/\" target=\"_blank\">Unidic Lite</a> dictionary (version: 1.0.8). Finally, for the stopwords list, we use a versioned, open-source list available <a href=\"https://raw.githubusercontent.com/stopwords-iso/stopwords-ja/5a000f6a62f9e3a12f436f36d168e2fcd2fb1878/stopwords-ja.json\" target=\"_blank\">here</a>.<a href=\"#refB\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Text Preprocessing & Featurization Pipeline</h5>\n                        <h5><u>Pretrained Japanese MLM BERT Model Embeddings Preprocessing & Featurization</u></h5>\n                        <div class=\"col-12 pb-2 pt-2\">\n                                <img id=\"bert-features-preprocessing\" class=\"img-fluid\" src=\"../../../../public/assets/bert-preprocessing.png\" alt=\"Second slide\">\n                            </div>\n                        <div class=\"row justify-content-center\">\n                            <div class=\"col-12\">\n                                <h6>Preprocessing</h6>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    The BERT MLM Deep learning (NN) model is optimized to <strong>predict</strong> a randomly <strong>masked words</strong> by using the <strong>context</strong>, \n                                    or the words that surround them. \n                                </p>\n                                <p>\n                                    Tohoku University Researchers <strong>pretrained a MLM BERT</strong> model using a dataset of approx. <strong>30M sentences</strong> of the \n                                    <strong>Japanese version of Wikipedia</strong>. \n                                </p>\n                                <p>\n                                    Raw text data is word tokenized using Fugashi & Unidic Lite dictionary. Word tokens are further split into subwords \n                                    using the <strong>WordPiece algorithm</strong>, yielding a token vocabulary of <strong>32768 unique tokens.</strong><sup><a href=\"#fnC\" id=\"refC\">1</a></sup>\n                                </p>\n                            </div>\n                            <h6>Featurization</h6>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    The text input tokens resulting from the preprocessing steps are prepended with a <strong>[CLS]</strong> token, \n                                    the Classification token. Deep in the BERT model, we extract a <strong>contextualized numerical \n                                    representation</strong> of the report for classification tasks by grabbing the <strong>final hidden state</strong> \n                                    corresponding to this token. This is <strong><i>CLS Pooling</i></strong>, yielding a dense, contextualized feature vector of 768 dimensions. \n                                    For brevity, we refer to these feature vectors as <strong>BERT embeddings</strong>. We show an example of a featurized text report below:\n                                </p> \n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"bert-features-ex\" class=\"img-fluid\" src=\"../../../../public/assets/bert-features.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"row justify-content-center\">\n                                <div class=\"col-12 col-md-5 pt-3\">\n                                    <h6><u>Pros:</u></h6>\n                                    <div>\n                                        <p>\n                                            <ul>\n                                                <li><strong>Optimized for the Japanese language</strong></li>\n                                                <li><strong>Dense features </strong> (i.e. lower dimensions than previous n-gram based features & not sparse)</li>\n                                                <li>\n                                                    <strong>Contextualized representations</strong>, which can better capture long-range dependencies between words in a \n                                                    report & State-of-the-art <strong>language understanding</strong>\n                                                </li>\n                                                <li>\n                                                    Better-equipped to handle <strong>OOV tokens</strong> due to the use of the WordPiece algorithm\n                                                </li>\n                                            </ul>\n                                        </p>\n                                    </div>\n                                </div>\n                                <div class=\"col-12 col-lg-4 pt-3\">\n                                    <h6><u>Cons:</u></h6>\n                                    <p>\n                                        <ul>\n                                            <li>Features are <strong>NOT interpretable</strong></li>\n                                            <li><strong>Specific to JA</strong></li>\n                                            <li><strong>Not finetuned to crisis text corpus</strong>, although we note this could be done in a future work</li>\n                                        </ul> \n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    Having preprocessed the text and produced various featurizations with their own pros and cons, we utilized all of the featurizations in our \n                                    classification experiments discussed in the next slides and TF-IDF based on unigram features as well as BERT embeddings in our clustering experiments.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                <sup id=\"fnC\">1. <a href=\"https://huggingface.co/cl-tohoku/bert-base-japanese-v2\" target=\"_blank\">Link to Pretrained Japanese BERT MLM Model</a><a href=\"#refC\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                                </p>\n                                <br>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <h5>Human Risk Text Classification</h5>\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    Since 715 out of the 716 firefighter text reports were labeled for the binary <strong>Human Risk/No Human Risk classes</strong>, we chose to focus our text classification\n                                    experiments on this classification task. Additionally, with the text analysis module, we aimed to develop a task that <strong>better met the information needs</strong> of crisis managers during\n                                    a crisis event. We did this by using the labels our crisis management partners in Fukuchiyama provided to us directly.\n                                </p>\n                            </div>\n                            <h6>Task Description</h6>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    <i>\n                                        The Human Risk text classification task \n                                        <strong>determines whether or not a crisis text report indicates if \n                                        there are people in need of rescue from a crisis.</strong> This includes people \n                                        being unable to evacuate due to physical disability (such as unable to use stairs), surrounding conditions (such as being trapped in a submerged car), \n                                        and/or being in need of life-saving emergency medical care.\n                                    </i>\n                                </p>\n                                <p>\n                                    In an effort to transition from abstract class descriptions to something more specific, e.g. a checklist, we choose to both provide a definition of the task and further detail the Human Risk classes as \n                                    bulleted lists containing the specific traits/descriptors contained in a text report which are characteristic of each class with the aim of enhancing the clarity of each class and the task overall:\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7 pt-3 pl-md-5\">\n                                <h6><strong>Human Risk</strong> Class Descriptors</h6>\n                                <div>\n                                    <p>\n                                        <ul>\n                                            <li>Rescue being requested (to the Fire Department (FD))</li>\n                                            <li>Evacuation support being requested (to the FD)</li>\n                                            <li>Human missed the chance to evacuate from their own house, at work, shopping center, etc.</li>\n                                            <li>Vulnerable population (elderly, disabled, small children) being left in the house in the flooding area</li>\n                                            <li>Water rising inside the house above the floor (human inside)</li>\n                                            <li>Water current is fast inside the house and hard to move upstairs (human inside)\n                                            <li>Sediment flowing into the house (human inside)</li>\n                                            <li>Human being trapped in elevator, submerged car, or a car which is not submerged yet</li>\n                                            <li>Human being washed away in a river</li>\n                                            <li>Rescue team dispatched</li>\n                                            <li>Rescue team in activity (such as helping evacuation, rescuing, etc.)</li>\n                                            <li>Rescue activity completed</li>\n                                            <li>Landslide occurrence on the highway - possible vehicle being involved</li>\n                                        </ul>\n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-4 pt-3\">\n                                <h6><strong>No Human Risk</strong> Class Descriptors</h6>\n                                <p>\n                                    <ul>\n                                        <li>Dam Discharge</li>\n                                        <li>Meteorological Information</li>\n                                        <li>River Water Level Information</li>\n                                        <li>Weather Alert</li>\n                                        <li>Road Closure</li>\n                                        <li>Road Flood Risk (not flooded yet)</li>\n                                        <li>Area Flood Risk (not flooded yet)</li>\n                                    </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12 pb-2\">\n                                <img id=\"human-risk-classification\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-diagram.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    In addition to using labels/classes which better meet the information needs of crisis managers during crisis, \n                                    we aimed to develop the classification model for this task <strong>using a performance metric which better aligns with the priorities of the crisis managers</strong>\n                                    as it pertains to this task and also <strong>considers the nature of the data (i.e. class imbalance)</strong>. We discuss these considerations and how we <strong>determined the\n                                    performance metric</strong> for this task in the next slides. We note that <strong>this process</strong> of constructing a task from <strong>crisis manager's labels</strong> to determining the appropriate \n                                    performance metric to utilize for developing a model which considers both the <strong>properties of the data</strong> and the <strong>insights we gained from crisis managers</strong> is a <strong>novel contribution of this research.</strong>\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                    <sup id=\"fnD\">1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who determined these class definitions from examining the original Japanese reports.<a href=\"#refD\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <h5>Human Risk Classification - Determination of the Performance Evaluation Metric</h5>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    In our determination of the performance metric to use for assessing the performance of the Human Risk classifier we consider both the class imbalance of the task\n                                    and insights we gained from the workshops conducted in the image analysis module.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-md-5\">\n                                <img id=\"human-riks-label-distribution\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-label-distribution.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 pt-3 col-md-6\">\n                                <h5>Substantial Class Imbalance</h5>\n                                <p>\n                                   <ul>\n                                        <li>\n                                            Across the 715 reports labeled for Human Risk, we observe that there are disporportionately more \n                                            \"No Human Risk\" data points than \"Human Risk\" data points, thus the <strong>\"Human Risk\" class is the minority class</strong> for this task.\n                                        </li>\n                                        <li>\n                                            <strong>Accuracy</strong> of the classifier which always predicts \"No Human Risk\": <strong>86.7%</strong>, i.e. the percentage of\n                                            \"No Human Risk\" labels in the dataset \n                                        </li>\n                                        <li>\n                                            Need to account for this imbalance in the metric, otherwise conclusions about the model's ability to perform the task can be\n                                            misleading, as seen with using accuracy as the performance metric\n                                        </li>\n                                   </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-md-3 pb-md-5\">\n                                <img id=\"insights-from-workshops\" class=\"img-fluid\" src=\"../../../../public/assets/workshop-insights.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"carousel-text col-12 col-md-6\">\n                                <h5>Insights from Image Annotation Workshops</h5>\n                                <p>\n                                   <ul>\n                                        <li>\n                                            From the workshops, we understand that in assessing the potential for human casualities, the cost associated with \n                                            <strong>NOT investigating the potential for human casualities when there ARE human casualities (False Negative (FN))</strong> is considered <strong>higher</strong> than the cost\n                                            of <strong>investigating potential human casualities when there ARE NONE (False Positive (FP))</strong>  <strong>Performance on \"Human Risk\" class is paramount</strong>.\n                                        </li>\n                                        <li>\n                                            <strong>Accuracy</strong> does NOT tell us how well the model performs on the <strong>\"Human Risk\"</strong> class \n                                            <strong> Although Recall (Minimizing FNs)</strong> & <strong>Precision (Minimizing FPs)</strong> have different priorities, both focus\n                                            on the performance of the \"Human Risk\" class\n                                        </li>\n                                        <li>\n                                            Ideally, we'd like to minimize both FN & FP, which is captured in the <strong>F1 score</strong>, however F1 score treats recall as equally as important as precision\n                                        </li>\n                                        <li>\n                                            <strong>F2 score</strong> treats recall as 2x as important as precision, i.e. <strong>the relative cost of FN \n                                                is twice as much as the cost of FP</strong>\n                                        </li>\n                                   </ul> \n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-5\">\n                                <p>\n                                     We thus used the <strong>properties of the data</strong> (i.e. class imbalance) & \n                                    <strong>the insights we gained from crisis experts</strong> to determine the performance metric to evaluate the model we develop for the Human Risk task \n                                     the <strong><u>F2 score</u></strong>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <h5>Human Risk Classification - Data Splits & Algorithm Selection</h5>\n                            <div class=\"row align-items-center justify-content-center pb-3\">\n                                <div class=\"col-12 col-md-6 pt-3\">\n                                    <h5>Train/Test Splits</h5>\n                                    <p>\n                                        We split the full dataset of 715 reports into non-overlapping train and test splits in percentages of 80%/20%, respectively. Additionally, we preserve\n                                        the class imbalance using stratified splitting. \n                                    </p>\n                                </div>\n                                <div class=\"col-12 col-md-4 pt-3 pb-md-5\">\n                                    <img id=\"data-splitting\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-data-splits.png\" alt=\"Second slide\">\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Nested Cross Validation for Algorithm Selection</h5>\n                                <p>\n                                    We were interested in investigating multiple ML algorithms for the Human Risk classification task, each with their own set of tunable hyperparameters. We aimed to determine which\n                                    algorithm paired with a corresponding hyperparameter grid search procedure (e.g. Grid Search, i.e. fitting a model to each unique hyperparameter combination in the grid), had the best estimated generalization\n                                    performance and would use that algorithm for the final model evaluation. We note that since we had insufficient data to use train/dev/test splits, we used a variation of K-fold Cross Validation (CV).\n                                </p>\n                                <p>\n                                    Since using the same K-fold CV procedure\n                                    for both performing hyperparameter tuning and estimating generalization performance can yield an estimated generalization performance that is biased and overly-optimistic, we elected\n                                    to use <strong>Nested CV</strong>. Nested CV is typically inpractical in large data settings as it is substantially more computationally expensive to perform as compared to K-fold CV; for our low-data setting it was feasible to use. Nested CV is a useful variation of CV as it mitigates the bias in the\n                                    generalization performance estimate of the algorithm and its corresponding search procedure by nesting the hyperparameter optimization within the generalization performance estimation procedure. \n                                </p>\n                                <p>\n                                    For the algorithm selection procedure, we investigated the following classification algorithms:\n                                </p>\n                            </div>\n                            <div class=\"col-12\">\n                                <div class=\"centered-list-parent\">\n                                    <ul class=\"centered-list\">\n                                        <li>Logistic Regression</li>\n                                        <li>Decision Tree</li>\n                                        <li>Random Forest</li>\n                                        <li>Support Vector Machine</li>\n                                        <li>Multinomial Naive Bayes</li>\n                                        <li>K-Nearest Neighbors</li>\n                                    </ul>\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    We note that we perform 5 x 5 Nested CV on the train split data & treat the various text featurizations offered by our featurization pipeline as hyperparameters in the hyperparameter grid for each\n                                    algorithm.\n                                </p>\n                                <p>\n                                    For the model evaluation on the test set, we select the algorithm (and corresponding search procedure) which \n                                    had the highest relative mean F2 score with the lowest variance across folds (low standard deviation) from the nested CV procedure.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-8 pt-3 pb-5\">\n                                <img id=\"nested-cv\" class=\"img-fluid\" src=\"../../../../public/assets/nested-cv.png\" alt=\"Second slide\">\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-4\">\n                            <div class=\"col-12\">\n                                <h5>Human Risk Classification</h5>\n                                <h5>Algorithm Selection Results</h5>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-md-5\">\n                                <img id=\"algo-selection-table\" class=\"img-fluid\" src=\"../../../../public/assets/nested-cv-table.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-md-5 pb-3\">\n                                <img id=\"algo-selection-graph\" class=\"img-fluid\" src=\"../../../../public/assets/nested-cv-graph.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    The results presented above are determined from the generalization performance estimation found from \n                                    the performance (by F2 score) on the outer loop 5-fold CV in Nested CV. We make available the intermediate and final results of Nested CV for each algorithm and\n                                    corresponding hyperparameter grid.<sup><a href=\"#fnD\" id=\"refD\">1</a></sup>\n                                </p>\n                                <p>\n                                    From the results, we determined that the performance of the <strong>Support Vector Machine (SVM)</strong> algorithm with its\n                                    corresponding hyperparameter search procedure yielded the highest mean F2 score,\n                                    82.0%, and the lowest standard deviation, 4.22%. <strong>We therefore select the SVM algorithm and its corresponding hyperparameter grid for the\n                                    final human risk model evaluation on the test set.</strong>\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                    <sup id=\"fnD\">1. <a href=\"https://github.com/dyllew/towards-automated-assessment-of-crowdsourced-crisis-reporting/tree/main/Text%20Analysis%20Module/Classification/Nested%20CV\" target=\"_blank\">Link to Results & Hyperparameters of Nested CV</a><a href=\"#refD\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around pb-5\">\n                            <div class=\"col-12\">\n                                <h5>Human Risk Classification</h5>\n                                <h5>Final Model Evaluation</h5>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Tuning the SVM Model</h5>\n                                <p>\n                                    Prior to performing the final evaluation of the SVM on the test split data, we performed 5-fold CV on the full train split data, applying grid search with the hyperparameter\n                                    grid associated with the SVM algorithm to find optimal hyperparameter values. The optimal hyperparameter values found for the SVM are shown in the table below:\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3 pb-3\">\n                                <img id=\"svm-hyperparameters\" class=\"img-fluid\" src=\"../../../../public/assets/svm-hyperparameters.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    We report the estimated generalization performance of the tuned SVM found\n                                    from the 5-fold CV mentioned above, noting that this is <strong>likely a biased estimate of\n                                    generalization performance</strong> as the 5-fold CV procedure was also used to tune the\n                                    model. The <strong>tuned SVM model</strong> achieves a <strong>mean F2 score of 85.0%</strong> and has\n                                    a <strong>standard deviation of 7.40%</strong>.\n                                </p>\n                                <p>\n                                    After tuning the SVM model to find the optimal hyperparameters above, we fit the SVM algorithm using those optimal hyperparameters on the <strong>entire train split data</strong>.\n                                    We then use this <strong>fitted SVM model to predict on the unseen test split data</strong>. The model's predictions on the test split yield the final evaluation of \n                                    the model's generalization performance. As part of our framework, we report aggregate metrics including the F2 and Area Under the Precision-Recall Curve (AUCPR) score and compare our trained model to baseline scores.\n                                    Lastly, we report per-class performance metrics and the confusion matrix of the model's predictions.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-3\">\n                                <img id=\"model-evalution-diagram\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-model-evaluation.png\" alt=\"Second slide\">\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Human Risk Classification</h5>\n                                <h5>Final Model Evaluation - Results</h5>\n                            </div>\n                            <div class=\"col-12\">\n                                <h5>Aggregate Metrics & Comparison to Baseline Scores</h5>\n                            </div>\n                            <div class=\"col-12 col-md-8\">\n                                <p>\n                                    We report an <strong>F2 score of 92.8% on the test split data.</strong> <strong>The baseline classifier which always predicts \"Human Risk\", has an F2 score of\n                                    43.4%</strong>, so the <strong>trained classifier is a significant improvement over the baseline classifier</strong>.\n                                </p>\n                                <p>\n                                    In addition to F2, we plot the Precision-Recall curve, visualizing the tradeoff between precision and recall for different classification thresholds used by the classifier\n                                    when classifying the data. It is advised to use the PR curve over the Receiver Operating Characteristic (ROC) curve in the case of imbalanced data, as ROC can give an optimistic estimate of the classifiers output\n                                    quality by considering true negatives in the computation, which in high quantity can dramatically lessen the effect of the false positives, false negatives, and true positives\n                                    in the performance estimate, giving a misleadingly high estimate of performance.\n                                </p>\n                            </div>\n                            <div class=\"col-12\"></div>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-4 pt-3 pb-3\">\n                                    <img id=\"aucpr-curve\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-aucpr.png\" alt=\"Second slide\">\n                                </div>\n                                <div class=\"col-12 col-md-5\">\n                                    <p>\n                                        The better the classifier (higher recall and higher precision), the closer the AUCPR score is to 1.\n                                        For AUCPR, the performance of a <strong>baseline model has a score which is given by the proportion of positive samples to the total number\n                                        of samples</strong> in the test dataset, which in this case is <strong>0.133</strong>.\n                                    </p>\n                                    <p>\n                                        We report an <strong>AUCPR of 0.919 for the SVM model</strong> on the test split data, a significant improvement over the baseline score.\n                                    </p>\n                                </div>\n                            </div>\n                            <div class=\"col-12\">\n                                <h5>Confusion Matrix & Per-Class Metrics</h5>\n                            </div>\n                            <div class=\"row align-items-center justify-content-center\">\n                                <div class=\"col-12 col-md-4 pt-3 pb-5\">\n                                    <img id=\"human-risk-confusion-matrix\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-cm-svm.png\" alt=\"Second slide\">\n                                </div>\n                                <div class=\"col-12 col-md-4 pt-3 pb-5\">\n                                    <img id=\"human-risk-per-class\" class=\"img-fluid\" src=\"../../../../public/assets/human-risk-per-class-metric.png\" alt=\"Second slide\">\n                                </div>\n                            </div>\n                            <div class=\"col-12 col-md-8 pb-5\">\n                                <p>\n                                    From the confusion matrix, we see that the model made very few\n                                    misclassifications on the test split data. Specifically, the model only misclassified one\n                                    data point which was labeled as \"Human Risk\" as \"No Human Risk\" out of all 19\n                                    data points labeled as \"Human Risk\", thus the model had <strong>low false negatives</strong>. The\n                                    model misclassified 3 \"No Human Risk\" data points as \"Human Risk\" out of a total\n                                    of 124 \"No Human Risk\" data points, thus the model predicted <strong>3 false positives</strong>. We\n                                    note that the model had more false positives than false negatives, but <strong>few of each</strong>.\n                                </p>\n                                <p>\n                                    The model performs well by all per-class metrics on the \"No\n                                    Human Risk\" class achieving scores at and above 0.976. Comparatively lower\n                                    performance is observed across all metrics for the \"Human Risk\" class with precision,\n                                    recall, and F1 scores of 0.857, 0.947, and 0.9, respectively.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Human Risk Text Classification - Discussion</h5>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            When determing the performance metric for the human risk task, we asked the following technical questions:\n                                            <ol>\n                                                <li><strong>Does the metric account for imbalance present in the data distribution?</strong></li>\n                                                <li><strong>Once the metric is determined, what is the performance of the baseline model for the task?</strong></li>\n                                            </ol>\n                                        </li>\n                                    </ul>\n                                </p>\n                                <p>\n                                    While these technical questions are no doubt important for assessing model efficacy for the task, \n                                    we underscore that there were other questions we asked which <strong><u>could only be answered by our engagement with crisis managers.</u></strong>\n                                </p>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            Before we began developing the human risk model, we asked a question about the task itself: \n                                            <strong>Do the classes for the task sufficiently capture the expressed information needs of crisis managers during a crisis?</strong>\n                                            <ul>\n                                                <li>\n                                                    Since these <strong>labels were provided directly by crisis managers</strong> and given the <strong>crisis manager's insight into \n                                                    the importance of assessing the potential of human casualty</strong>, we determined that these classes sufficiently capture\n                                                    an important information need of crisis managers during crisis.\n                                                </li>\n                                            </ul>\n                                        </li>\n                                        <li>\n                                            For the determining the performance metric for the task, questions which required crisis manager engagement included:\n                                            <ul>\n                                                <li>\n                                                    <strong>\n                                                        Does the metric incorporate the priorities of the crisis managers as\n                                                        it relates to the task, e.g. the cost of a false negative is significantly higher than\n                                                        the cost of a false positive for assessing human risk?\n                                                    </strong>\n                                                </li>\n                                                <li>\n                                                    <strong>\n                                                        Are there multiple metrics that should considered in assessing model efficacy in performing the classification task,\n                                                        e.g. precision and recall, or F2?\n                                                    </strong>\n                                                </li>\n                                            </ul>\n                                        </li>\n                                    </ul>\n                                </p>\n                                <p>\n                                    Asking these questions allowed us to both consider the technical intricacies for\n                                    the task (i.e. data imbalance and baseline performance) and directly embed the\n                                    information needs and priorities of crisis managers into our text ML methodology\n                                    and evaluation, which are important aims of our framework.\n                                </p>\n                                <p>\n                                    The <strong>significant improvement over the baseline classifier by F2</strong> suggests the tuned <strong>SVM model is a useful classifier for\n                                    the human risk task and performs the task reasonably well</strong>. This is further evidenced from the\n                                    Precision-Recall curve, where the tuned SVM model achieved an AUCPR score of\n                                    0.919, a significant improvement over the typical baseline classifier used for\n                                    that metric which achieves an AUCPR of 0.133\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Clustering of Crowdsourced Japanese Crisis Text Data</h5>\n                                <h5>Overview</h5>\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <p>\n                                    Beyond investigating the human risk classification task, we aimed to explore the\n                                    Fukuchiyama firefighter flood crisis report corpus to see if we could <strong>uncover other\n                                    coherent categories which may exist in the data</strong>. These <strong>uncovered categories could\n                                    inform the development of classification tasks in future work</strong>, in addition to any\n                                    of the humanitarian categories provided by crisis managers.\n                                </p>\n                                <p>\n                                    This exploration is powered by a series of unsupervised learning techniques including dimensionality reduction and clustering. \n                                    We developed a pipeline that utilizes these unsupervised techniques to perform the clustering experiments we conducted\n                                    to uncover coherent categories in the data.\n                                </p>\n                                <p>\n                                    For our clustering experiments, we note that we use <strong>all 716 FC firefighter crisis reports</strong>. Additionally, we focus on using the <strong>TF-IDF based on unigrams embeddings</strong> and \n                                    <strong>Pretrained Japanese BERT embeddings</strong>, which we refer to as BERT embeddings for brevity. Since these featurizations are 1489 and 768 dimensions respectively, this\n                                    motivated our incorporation of dimensionality reduction techniques into our clustering pipeline.\n                                </p>\n                                <div class=\"col-12 pt-3 pb-3\">\n                                    <img id=\"clustering-features\" class=\"img-fluid\" src=\"../../../../public/assets/text-features-for-clustering.png\" alt=\"Second slide\">\n                                </div>\n                            </div>\n                            <div class=\"col-12\">\n                                <h6>Evaluation Overview</h6>\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-5\">\n                                <p>\n                                    The evaluation of our clustering experiments consisted of multiple stages. <strong>First, we perform quantitative analysis</strong>, producing <strong>Within-Cluster Sum of Squares (WCSS)</strong>, or \"Elbow\" plots for each combination of\n                                    featurization type, dimensionality reduction technique, and clustering algorithm (12 combos in total). We refer to these as configuration combinations. Using these plots, we identify <strong>a query subset</strong> of the combinations to further investigate for our\n                                    <strong>qualitative evaluation</strong>. In the first stage of our qualitative evaluation we used the <strong>english translations of the closest documents to each cluster center</strong> to select a configuration combination from the query subset \n                                    for the <strong>final stage of qualitative assessment</strong>. In the final stage, a <strong>fluent Japanese speaker</strong> investigated the raw Japanese reports for the clusters found by the selected configuration combination and \n                                    <strong>determined a human-interpretable label to describe the cluster overall</strong> for each cluster. \n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row align-items-center justify-content-around\">\n                            <div class=\"col-12\">\n                                <h5>Clustering Pipeline & Experiments</h5>\n                            </div>\n                            <div class=\"col-12 col-md-5 pt-3 pb-3\">\n                                    <img id=\"clustering-pipeline\" class=\"img-fluid\" src=\"../../../../public/assets/clustering-pipeline.png\" alt=\"Pipeline for Clustering\">\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-5\">\n                                <h6>Featurize, Reduce Dimensions, and Cluster</h6>\n                                <p>\n                                    Using the devised clustering pipeline, we can sequentially reduce the dimensions of the input features to <strong>2-dimensions</strong>, using\n                                    <strong>Principle Component Analysis (PCA)</strong>, <strong>t-distributed Stochastic Neighbor Embedding (t-SNE)</strong>, or we <strong>do not apply dimensionality reduction</strong> at all. \n                                    We finally cluster the reduced data using either <strong>K-means</strong> or <strong>K-medoids</strong>, which is more robust to outliers present in the data. These hyperparameters to \n                                    the clustering pipeline are summarized in the adjacent figure.\n                                </p>\n                                <div class=\"row justify-content-center pb-3\">\n                                    <img id=\"clustering-hyperparameters\" class=\"img-fluid\" src=\"../../../../public/assets/clustering-hyperparameters.png\" alt=\"Hyperparameters for Clustering\">\n                                </div>\n                                <h6>Outputs of Clustering Pipeline</h6>\n                                <p>\n                                    <strong>Having selected a hyperparameter configuration and a K-value to use</strong>, our clustering pipeline produces the clustered data points, the text associated with\n                                    the <strong>20 closest documents to the cluster center (in raw JA text & EN translations)</strong> for each cluster, and the top 20 unigrams (in JA) by TF-IDF score for the \n                                    document formed from concatenating the documents in a cluster, for each cluster, forming a cluster-level document corpus. We note that our pipeline can take any positive values x & y for displaying the \n                                    top x unigrams or top y documents in a cluster.\n                                </p>\n                            </div>\n                            <div class=\"col-12 col-md-7 pb-5\">\n                                <h5>Clustering Experiments - Identifying the Query Subset</h5>\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                We investigated all 12 featurization, dimensionality reduction, and clustering algorithm combinations by investigating the corresponding \n                                                <strong>WCSS or Elbow plot</strong> for <strong>K = 2, , 20 clusters</strong>\n                                            </p>\n                                            <p>\n                                                <strong>WCSS</strong> captures extent to which data points within a cluster are at a close distance to each other, ideally we want this to be low, \n                                                but not too low. Minimizing WCSS is the same as maximizing the distance between data points in different clusters.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"pb-3\">\n                                    <img id=\"elbow-plot\" class=\"img-fluid\" src=\"../../../../public/assets/elbow-plot.png\" alt=\"Second slide\">\n                                </div>\n                                <p>\n                                    <ol>\n                                        <li value=\"2\">\n                                            We selected a subset of combinations which have <strong>relatively lower WCSS scores</strong> across \n                                            all K values and <strong>have an elbow in the elbow plot</strong> to qualitatively investigate further. We call this the <strong>query subset</strong>.\n                                        </li>\n                                    </ol>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around pb-4\">\n                            <div class=\"col-12\">\n                                <h5>Clustering Qualitative Evaluation & Results</h5>\n                                <h5>Preliminary Qualitative Assessment (in EN): Investigating the Query Subset</h5>\n                            </div>\n                            <div id=\"workflow-and-configs\" class=\"col-12 col-md-6 pt-3 pb-5\">\n                                <h6>Preliminary Assessment Workflow</h6>\n                                <img id=\"qualitative-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/preliminary-assessment.png\" alt=\"Second slide\">\n                                <br>\n                                <br>\n                                <h6>Query Subset</h6>\n                                <img id=\"query-subset\" class=\"img-fluid\" src=\"../../../../public/assets/query-subset.png\" alt=\"Configuration Combinations in the Query Subset\">\n                                <br>\n                                <br>\n                                <h6>Qualitative Summaries of Clusters and Possible Labels</h6>\n                                <img id=\"qualitative-summaries\" class=\"img-fluid\" src=\"../../../../public/assets/qualitative-summaries.png\" alt=\"Qualitative Summaries of Resultant Clustering for Each Cluster\">\n                            </div>\n                            <div id=\"workflow\" class=\"col-12 col-md-6 pt-3 pb-1\">\n                                <h6>Preliminary Assessment Workflow</h6>\n                                <img id=\"qualitative-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/preliminary-assessment.png\" alt=\"Second slide\">\n                            </div>\n                            <div class=\"col-12 col-md-6 pt-2 pb-md-5\">\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                <strong>For each combination in the subset, we determine an elbow</strong> from the corresponding elbow \n                                                plot as the K value to use in our qualitative analysis.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <img id=\"identified-elbow\" class=\"img-fluid pb-3\" src=\"../../../../public/assets/identified-elbow.png\" alt=\"Second slide\">\n                                <p>\n                                    <ol>\n                                        <li value=\"2\">\n                                            <p>\n                                                For each cluster, we investigate the <strong>top 20 reports within a cluster which were closest to the cluster center</strong>. \n                                                We note that for the <strong>preliminary assessment</strong>, we used <strong>English translations of the reports given by DeepL neural translation.</strong><sup><a href=\"#fnE\" id=\"refE\">1</a></sup>\n                                            </p>\n                                            <p>\n                                                When investigating the representative reports in each cluster, we answered the question: \n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <p id=\"research-question\">\n                                    <strong>When looked at together, does the content of the representative reports in a cluster elicit an interpretable label? If so, what is it?</strong>\n                                </p>\n                                <p>\n                                    <ol>\n                                        <li value=\"3\">\n                                            <p>\n                                                We then identified the configuration combination in the query subset which yielded <strong>the most interpretable labels across combinations</strong>,\n                                                i.e. selected the combination which had the highest number of clusters that had\n                                                representative documents which elicited an interpretable label.\n                                            </p>\n                                            <p>\n                                                In the neighboring graphics, we showcase the configuration combinations in the query subset and\n                                                we report qualitative summaries for each of the cluster configurations. We note that the configuration combination which gave the highest number of clusters which had a coherent, interpretable label (9 in total)\n                                                was the <strong>BERT embedding, t-SNE (2 components), and K-medoids clustering</strong> combination, which is <strong>bolded</strong>.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                            </div>\n                            <div id=\"configs\" class=\"col-12 pt-3 pb-5\">\n                                <h6>Query Subset</h6>\n                                <img id=\"query-subset\" class=\"img-fluid\" src=\"../../../../public/assets/query-subset.png\" alt=\"Configuration Combinations in the Query Subset\">\n                                <br>\n                                <br>\n                                <h6>Qualitative Summaries of Clusters and Possible Labels</h6>\n                                <img id=\"qualitative-summaries\" class=\"img-fluid\" src=\"../../../../public/assets/qualitative-summaries.png\" alt=\"Qualitative Summaries of Resultant Clustering for Each Cluster\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                    <sup id=\"fnE\">1. <a href=\"https://www.deepl.com/en/translator\" target=\"_blank\">Link to DeepL.</a> We acknowledge Saeko Baird of the Urban Risk Lab at MIT who cleaned these translations of their inaccuracies.<a href=\"#refE\" title=\"Jump back to footnote 1 in the text.\"></a></sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around pb-4\">\n                            <div class=\"col-12\">\n                                <h5>Clustering Qualitative Evaluation & Results</h5>\n                                <h5>Final Qualitative Assessment (in JA): Investigating the Optimal Configuration Combination determined from the Preliminary Assessment</h5>\n                            </div>\n                            <div id=\"workflow-and-clusters\" class=\"col-12 col-md-6 pt-3 pb-0\">\n                                <h6>Final Assessment Workflow</h6>\n                                <img id=\"qualitative-final-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/final-assessment.png\" alt=\"Final Qualitative Assessment Workflow\">\n                                <br>\n                                <br>\n                                <h6>Unlabeled Clusters</h6>\n                                <img id=\"unlabled-clusters-img\" class=\"img-fluid\" src=\"../../../../public/assets/unlabeled-clusters.png\" alt=\"Unlabeled Clusters\">\n                            </div>\n                            <div id=\"final-assessment-workflow\" class=\"col-12 col-md-6 pt-3 pb-1\">\n                                <h6>Final Assessment Workflow</h6>\n                                <img id=\"qualitative-final-evaluation-workflow\" class=\"img-fluid\" src=\"../../../../public/assets/final-assessment.png\" alt=\"Final Qualitative Assessment Workflow\">\n                            </div>\n                            <div id=\"steps-with-unlabeled-clusters\" class=\"col-12 col-md-6 pt-2\">\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \n                                                <strong>top 20 closest reports</strong> within the cluster to the cluster center & the <strong>top 20 unigrams by TF-IDF score for each cluster document in the cluster-level document corpus</strong>.\n                                            </p>\n                                            <p>\n                                                Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters.\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"row justify-content-center pb-4\">\n                                    <h6>Unlabeled Clusters</h6>\n                                    <img id=\"unlabeled-clusters-img\" class=\"img-fluid\" src=\"../../../../public/assets/unlabeled-clusters.png\" alt=\"Unlabeled Clusters\">\n                                </div>\n                                <p>\n                                    <ol>\n                                        <li value=\"2\">\n                                            <p>\n                                                Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA<sup><a href=\"#fnF\" id=\"refF\">1</a></sup> investigated each cluster and assigned an interpretable label to it.\n                                            </p>\n                                            <p>\n                                                The interpretable label given for each cluster is depicted in the figures below:\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"col-12\">\n                                    <img id=\"cluster-labels\" class=\"img-fluid\" src=\"../../../../public/assets/cluster-labels.png\" alt=\"Cluster Labels\">\n                                </div>\n                            </div>\n                            <div id=\"steps-without-unlabeled-clusters\" class=\"col-12 col-md-6\">\n                                <p>\n                                    <ol>\n                                        <li>\n                                            <p>\n                                                Using the selected optimal combination, for each resulting cluster found, we produce auxiliary outputs showing the \n                                                <strong>top 20 closest reports</strong> within the cluster to the cluster center & the <strong>top 20 unigrams by TF-IDF score for each cluster document in the cluster-level document corpus</strong>.\n                                            </p>\n                                            <p>\n                                                Since dimensionality reduction was applied to 2-dimensions, we can visualize the clusters.\n                                            </p>\n                                        </li>\n                                        <li>\n                                            <p>\n                                                Using the auxiliary outputs in JA, a member of the Urban Risk Lab who is fluent in EN & JA<sup><a href=\"#fnF\" id=\"refF\">1</a></sup> investigated each cluster and assigned an interpretable label to it.\n                                            </p>\n                                            <p>\n                                                The interpretable label given for each cluster is depicted in the figures below:\n                                            </p>\n                                        </li>\n                                    </ol>\n                                </p>\n                                <div class=\"col-12\">\n                                    <img id=\"cluster-labels\" class=\"img-fluid\" src=\"../../../../public/assets/cluster-labels.png\" alt=\"Cluster Labels\">\n                                </div>\n                            </div>\n                            <div id=\"labeled-clusters\" class=\"col-12 col-md-7 pb-3\">\n                                <p id=\"process-arrow\">&#8595;</p>\n                                <h6>Labeled Clusters</h6>\n                                <img id=\"labeled-clusters-img\" class=\"img-fluid\" src=\"../../../../public/assets/labeled-clusters.png\" alt=\"Labeled Clusters\">\n                            </div>\n                            <div class=\"col-12 col-md-7\">\n                                <hr>\n                                <p>\n                                    <sup id=\"fnF\">1. We acknowledge Saeko Baird of the Urban Risk Lab at MIT who assigned an interpretable label\n                                        to each cluster.<a href=\"#refF\" title=\"Jump back to footnote 1 in the text.\"></a>\n                                    </sup>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-around\">\n                            <div class=\"carousel-text col-12 col-md-7\">\n                                <h5>Clustering Firefighter Flood Crisis Reports - Discussion</h5>\n                                <p>\n                                    <ul>\n                                        <li>\n                                            Clustering results suggest what is deemed important to report during flood crisis by FC firefighters on-the-ground.\n                                        </li>\n                                        <li>\n                                            Since we applied the clustering on-the-ground firefighter reports, these results can be used to devise <strong>classification tasks with labels which better embed the information needs of crisis managers</strong> & can be cross-referenced with crisis managers.\n                                        </li>\n                                        <li>\n                                            Experiment can also be applied on Japanese RiskMap reports or crisis tweets to see if similar cluster labels are unveiled by resident reporting.\n                                        </li>\n                                        <li>\n                                            This method has the <strong>drawback of permitting data points to be in only one cluster (hard clustering)</strong>, and \n                                            we observed that some of the data points have content which is indicative of multiple of the unveiled interpretable labels.\n                                        </li>\n                                    </ul>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../../constants';\n\nexport default {\n  name: 'TextAnalysisCarousel',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('TextAnalysisCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h5 {\n    color: white;\n}\n\n.characteristics {\n    text-align: center;\n}\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.centered-list-parent {\n    text-align: center;\n}\n\n.centered-list {\n    display: inline-block;\n    text-align: left;\n}\n\n#hagibis-details {\n    color: palevioletred;\n}\n\n#twitter-quote {\n    text-align: left;\n}\n\n#pika-gif {\n    margin-top: 10px;\n    margin-bottom: 40px;\n    width: 60vh;\n    height: 40vh;\n}\n\n#identified-elbow {\n    max-height: 40vh;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n#workflow-and-configs {\n    display: none;\n}\n\n#workflow-and-clusters {\n    display: none;\n}\n\n#steps-without-unlabeled-clusters {\n    display: none;\n}\n\n#process-arrow {\n    font-size: 6vh;\n    text-align: center;\n}\n\n@media (min-width: 768px) {\n    \n    #clustering-hyperparameters {\n        max-width: 40vw;\n    }\n\n    #workflow-and-configs {\n        display: inline;\n    }\n\n    #workflow {\n        display: none;\n    }\n\n    #configs {\n        display: none;\n    }\n\n    #workflow-and-clusters {\n        display: inline;\n    }\n\n    #final-assessment-workflow {\n        display: none;\n    }\n\n    #steps-without-unlabeled-clusters {\n        display: inline;\n    }\n\n    #steps-with-unlabeled-clusters {\n        display: none;\n    }\n\n    #process-arrow {\n        font-size: 10vh;\n        text-align: center;\n    }\n\n}\n\n@media (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n#text-characteristics-table {\n  font-family: Arial, Helvetica, sans-serif;\n  border-collapse: collapse;\n  color: black;\n  width: 100%;\n}\n\n#text-characteristics-table td, #text-characteristics-table th {\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n\n#text-characteristics-table tr:nth-child(even){background-color: #f2f2f2;}\n#text-characteristics-table tr:hover{background-color: #ddd;}\n#text-characteristics-table tr:nth-child(odd) {background-color: #ddd;}\n\n#text-characteristics-table th {\n  padding-top: 12px;\n  padding-bottom: 12px;\n  text-align: center;\n  background-color: darkturquoise;\n  color: white\n}\n\n</style>","import mod from \"-!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../../node_modules/thread-loader/dist/cjs.js!../../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./TextAnalysisCarousel.vue?vue&type=template&id=ec77afde&scoped=true&\"\nimport script from \"./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"\nexport * from \"./TextAnalysisCarousel.vue?vue&type=script&lang=js&\"\nimport style0 from \"./TextAnalysisCarousel.vue?vue&type=style&index=0&id=ec77afde&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"ec77afde\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12 col-md-8\"},[_c('h3',[_vm._v(\"Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"NLPIntLitDevCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#NLPIntLitDevCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#NLPIntLitDevCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#NLPIntLitDevCarousel\",\"data-bs-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-bs-target\":\"#NLPIntLitDevCarousel\",\"data-bs-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-bs-target\":\"#NLPIntLitDevCarousel\",\"data-bs-slide-to\":\"4\"}}),_c('li',{attrs:{\"data-bs-target\":\"#NLPIntLitDevCarousel\",\"data-bs-slide-to\":\"5\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/int-dev-results.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Project Presentation, Report, and Code\")]),_c('p',[_vm._v(\" This was a final project for 6.864: Advanced Natural Language Processing. This presentation focuses on introducing the project, the specific parts I worked on, and the main results from our analysis. \"),_c('br'),_vm._v(\" A brief overview of the motivation, methods, and results is available in this \"),_c('a',{attrs:{\"href\":\"./assets/int-dev-gray-lit.pdf\",\"target\":\"_blank\"}},[_vm._v(\"presentation PDF.\")]),_c('br'),_vm._v(\" The more thorough report of our methodology, visualizations, and findings is \"),_c('a',{attrs:{\"href\":\"./assets/6_864_Project.pdf\",\"target\":\"_blank\"}},[_vm._v(\"here.\")]),_c('br'),_vm._v(\" The code for this project was written in Python. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/6.864-fp\",\"target\":\"_blank\"}},[_vm._v(\"Here's the GitHub Repo.\")])]),_c('p',[_vm._v(\" For this project, my main contributions were the supervised methodology/Named Entity Recognition stream discussed in the following slides. \")])])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('div',{staticClass:\"row justify-content-center pb-5\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Abstract\")]),_c('p',[_vm._v(\" In fields like international development, decision-makers prioritize making evidence-based decisions for funding and implementing future projects. This aim is made difficult because of the plethora of information being published each year, and the nature of the research corpus as unstructured text or gray literature. To make informed decisions and understand the growing corpus of research available, researchers have turned to evidence synthesis - the process of compiling information and knowledge from many sources and disciplines to inform decisions. However, the manual evidence synthesis process takes extensive time (often 18 months to 3 years) and effort, and may soon be impossible at the worlds increasing rate of research output. To address these problems, we employ natural language processing techniques on a international development literature corpus of 244 documents to extract information from the title and abstract of international development documents, and to automatically cluster documents based on their content. We classify documents by Country of Study using a pretrained transformer Named Entity Recognition model and achieve an accuracy of 91.0%. Using K-Means clustering, we uncover informative and distinctive groupings of the documents which share similar semantic content. These methods reduce the time it takes for manual evidence synthesis for international development grey literature by enabling country of study filtering and clustering documents by semantic similarity. \")]),_c('h5',[_c('strong',[_vm._v(\"Research Question\")])]),_c('strong',{attrs:{\"id\":\"research-question\"}},[_vm._v(\" Can we use Natural Language Processing to expedite manual evidence synthesis and uncover broad classes of documents in the international development field? \")])])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Named Entity Recognition (NER) for Country of Study (CoS) Classification - Methodology\")]),_c('p',[_vm._v(\" The country of study (CoS) associated with each paper is quite pertinent to the international development domain. Our dataset is labeled with the CoS of each paper in our corpus. However, since our dataset is rather small (244 documents), we sought to evaluate whether pretrained NER models which extract a variety of entity types from text, could accurately extract the CoS for the papers in our corpus, which have a variety of text fields. \")]),_c('h6',[_vm._v(\"Country of Study Extraction and Classification\")]),_c('p',[_vm._v(\" We create a lower-cased, alphabetically-ordered, list of countries, which we construct using countryinfo \"),_c('a',{attrs:{\"href\":\"https://pypi.org/project/countryinfo/\",\"target\":\"_blank\"}},[_vm._v(\"(Link to CountryInfo PyPI page)\")]),_vm._v(\", a Python package which contains a large dictionary of countries, their alternative names, and ISO information. We ensure the strings of the countries present in our corpus match their respective string in the alphabetically-sorted list of countries. We note that Myanmar and Kosovo are countries present in our corpus, but are not present in the countryinfo dictionary, so we add them to the final list of alphabetically-sorted countries. Since nationality is a type of named entity that NER models typically extract in addition to countries, using a comprehensive, open-source nationality-country mapping \"),_c('a',{attrs:{\"href\":\"https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv\",\"target\":\"_blank\"}},[_vm._v(\"(Demonym-Country Mapping Link)\")]),_vm._v(\", we construct a lower-cased, alphabetically-ordered list of nationalities as well as a dictionary mapping nationality to country. We note that we use the words nationality and demonym interchangeably. These lists and dictionary are useful for performing the country of study (CoS) classification using extracted entities from input text or determining if a nationality or country is a substring contained in the input text string. \")]),_c('h6',[_vm._v(\"Simple Substring Matcher (SSM) Algorithm Baseline  CoS Extraction & Classification\")]),_c('p',[_vm._v(\" As a baseline to our CoS prediction task, we devise a simple, non-ML, deterministic algorithm, called the Simple Substring Matcher (SSM) Algorithm. This method begins by making the input text lower-cased. To predict a CoS, it then scans through the alphabetically-sorted list of countries and classifies the first country which is a substring in the input text as the CoS. If no country is found as a substring in the text, the method then scans the alphabetically-sorted list of nationalities. If a nationality is found as a substring of the input, the method maps the nationality to the corresponding country and classifies the paper as having that country as the CoS. If neither country nor nationality is found as a substring in the text, the method classifies the paper's CoS as a \"),_c('i',[_vm._v(\"None\")]),_vm._v(\" value. We refer to this classification model as the Simple Substring Matcher (SSM) model. \")]),_c('h6',[_vm._v(\"CoS Extraction & Classification by Pretrained NER Models\")]),_c('p',[_vm._v(\" Although we utilize different pretrained NER models in our experiments as shown in the next slide, the process for classifying CoS using predicted entities is the same. Each model takes the raw text as input, predicts various non-overlapping entities present in the text into one of several entity categories. For the CoS classification task, we only consider the predicted \"),_c('b',[_vm._v(\"NORP\")]),_vm._v(\" (nationalities, religious, or political groups) entities and the \"),_c('b',[_vm._v(\"GPE\")]),_vm._v(\" (countries, cities, or states) entities as we assume that these categories are the only ones which would contain the country or relevant demonym associated with the CoS. We now begin our discussion of the classification procedure for the pretrained NER models. First, we make all NORP and GPE entities lower-cased. Next, we map any demonyms present among the NORP entities to their corresponding country. We then combine the resulting unique NORP and GPE entities into an alphabetically-ordered list. We scan this list of NORP and GPE entities checking if any of them exist in the countries list mentioned above, classifying the CoS as the first entity-country match found. If no match is found, we make a final attempt to determine the CoS by providing each of the entities as input to the GeoPy Geocoder \"),_c('a',{attrs:{\"href\":\"https://geopy.readthedocs.io/en/stable/\",\"target\":\"_blank\"}},[_vm._v(\"(Link to GeoPy API)\")]),_vm._v(\" object, which provides an address-location object if a location is found for the provided entity or no value otherwise. We do this for each entity, and if a location is found for a particular entity, we classify the paper's CoS as the country associated with the found address-location. If no country is found for all the entities, we classify the paper's CoS as \"),_c('i',[_vm._v(\"None\")]),_vm._v(\". \")])])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/ner-results.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"NER for Country of Study Classification - Results\")]),_c('p',[_vm._v(\" In addition to testing different classification models, I experimented with different input strings to see how results change with various text fields and concatenations between them. These various inputs to the models include the title, abstract, intervention description, outcome description, and various concatenations of these text fields. \")]),_c('p',[_vm._v(\" All of the pretrained spaCy NER models have 0.0% accuracy when using the just the intervention description, however the SSM model achieves 13.9% accuracy on the intervention description. All models attain an accuracy of 2.9% when using just the outcome description. The title and abstract individually appear to be good input fields for predicting the CoS, however the concatenation of title and abstract appears to be the most informative input, as this is the input that yields the highest performance across all of the models. Overall, we observe that the baseline simple substring checker is a fairly competitive model against the pretrained ML models, outperforming all the ML models on intervention description, performing the same as the ML models on outcome description, and only falling a few percentage points below even the best ML model on the other inputs. With the exception of the title, intervention description, and outcome description, the ML models in increasing order of complexity, do increasingly better on the CoS extraction task, in the following order from least performant to most performant: ESMS, ESMM, ESML, and ESMT. With the exception of the intervention description and output description inputs, we observe that the ESMT model performs best across all other inputs. Furthermore, we see that the concatenated title and abstract input and the ESMT model combination performs the best across all input-model combinations with 91.0% accuracy. We use this top-performing model by accuracy to construct AI-assisted tools which could assist researchers in the evidence synthesis process in the following slide. \")])])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/ner-res-tools.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Predictions by Pretrained Transformer NER Model for International Development Gray Literature Map and Filter Function\")]),_c('h6',[_vm._v(\"Map of International Development Literature Gray Corpus\")]),_c('p',[_vm._v(\" Using the CoS predictions from the pretrained NER transformer model on the concatenated title and abstract input, we construct a geographical map of the corpus as shown in the left image. For each paper, which had a non-null prediction by the ESMT model, we place a tooltip at the location coordinate associated with the predicted CoS. These location coordinates were pulled using the GeoPy Geocoder object from the GeoPy Python package. We added slight, uniform random jitter to each of the coordinates, so papers with the same predicted CoS don't directly overlap. When a user hovers over the tooltip, they will see the title of the paper associated with that tooltip. The webpage for this map can be downloaded \"),_c('a',{attrs:{\"href\":\"https://drive.google.com/file/d/1Q2P6ouwcDWrnXsq8LMpa4YqCuD7qsbtO/view?usp=sharing\",\"target\":\"_blank\"}},[_vm._v(\"here\")]),_vm._v(\" for view in a browser. \")]),_c('h6',[_vm._v(\"Filtering by Predicted CoS\")]),_c('p',[_vm._v(\" For large corpora of International Development Gray Literature, the utility of the CoS prediction task is most evident by the robust filtering capability it enables. For instance, by concatenating only the title and abstract of papers in the corpus, and using them as input to generate CoS predictions by the pretrained transformer model used in this study, this enables the ability for unlabeled papers in the corpus to be accurately filtered to identify studies which had a specific CoS. \"),_c('strong',[_vm._v(\"This method would greatly reduce the time necessary for manual CoS annotation while also yielding higher accuracy than a simple substring matcher, simplifying a step in the international literature review process with high accuracy.\")]),_vm._v(\" An example of this filtering functionality is shown in the right image for papers in the corpus, which were predicted as having Guatemala as the CoS. The CoS was predicted using the pretrained transformer NER model with the concatenated title and abstract as input. We display the corresponding title and abstract for quick scanning of results for relevancy to research topic. Additionally, we provide the option to filter the corpus for papers which had a predicted CoS as \"),_c('i',[_vm._v(\"None\")]),_vm._v(\". \")])])])]),_c('div',{staticClass:\"carousel-item int-dev-lit-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/int-dev-results.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Conclusion\")]),_c('p',[_vm._v(\" The manual evidence synthesis for international development gray literature is a time-consuming process. We have demonstrated that certain components of the evidence synthesis process in international development gray literature such as filtering corpora for papers which have a specific country of study or grouping similar documents together can benefit greatly from the use of methods of information extraction and unsupervised learning. More specifically, we have utilized a pretrained transformer NER model to accurately predict the country of study for the papers present in the corpus used in this study, thus enabling accurate filtering of the corpus for papers with a specific predicted country of study. After tuning to find the optimal number of clusters in K-Means clustering, we uncovered informative and distinctive clusters of documents with similar content in the corpus. The automation of these components in the evidence synthesis process for international development grey literature mitigates the effort and time that is required for manual evidence synthesis. \")])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12 col-md-8\">  \n            <h3>Information Extraction and Unsupervised Methods for Streamlining Evidence Synthesis in International Development Gray Literature</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"NLPIntLitDevCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#NLPIntLitDevCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#NLPIntLitDevCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#NLPIntLitDevCarousel\" data-bs-slide-to=\"2\"></li>\n                    <li data-bs-target=\"#NLPIntLitDevCarousel\" data-bs-slide-to=\"3\"></li>\n                    <li data-bs-target=\"#NLPIntLitDevCarousel\" data-bs-slide-to=\"4\"></li>\n                    <li data-bs-target=\"#NLPIntLitDevCarousel\" data-bs-slide-to=\"5\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/int-dev-results.png\" alt=\"First slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Project Presentation, Report, and Code</h5>\n                                <p>\n                                    This was a final project for 6.864: Advanced Natural Language Processing. This presentation focuses on\n                                    introducing the project, the specific parts I worked on, and the main results from our analysis.\n                                    <br>\n                                    A brief overview of the motivation, methods,\n                                    and results is available in this <a href=\"./assets/int-dev-gray-lit.pdf\" target=\"_blank\">presentation PDF.</a>\n                                    <br>\n                                    The more thorough report of our methodology, visualizations, and findings is <a href=\"./assets/6_864_Project.pdf\" target=\"_blank\">here.</a>\n                                    <br>\n                                    The code for this project was written in Python. <a href=\"https://github.com/dyllew/6.864-fp\" target=\"_blank\">Here's the GitHub Repo.</a>\n                                </p>\n                                <p>\n                                    For this project, my main contributions were the supervised methodology/Named Entity Recognition stream discussed in the following slides.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <div class=\"row justify-content-center pb-5\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Abstract</h5>\n                                <p>\n                                    In fields like international development, decision-makers prioritize making evidence-based decisions for funding and implementing future projects. \n                                    This aim is made difficult because of the plethora of information being published each year, and the nature of the research corpus as unstructured \n                                    text or gray literature. To make informed decisions and understand the growing corpus of research available, researchers have turned to evidence \n                                    synthesis - the process of compiling information and knowledge from many sources and disciplines to inform decisions. However, the manual evidence \n                                    synthesis process takes extensive time (often 18 months to 3 years) and effort, and may soon be impossible at the worlds increasing rate of research \n                                    output. To address these problems, we employ natural language processing techniques on a international development literature corpus of 244 documents \n                                    to extract information from the title and abstract of international development documents, and to automatically cluster documents based on their content. \n                                    We classify documents by Country of Study using a pretrained transformer Named Entity Recognition model and achieve an accuracy of 91.0%. Using K-Means clustering, \n                                    we uncover informative and distinctive groupings of the documents which share similar semantic content. These methods reduce the time it takes for manual evidence \n                                    synthesis for international development grey literature by enabling country of study filtering and clustering documents by semantic similarity.\n                                </p>\n                                <h5><strong>Research Question</strong></h5>\n                                <strong id=\"research-question\">\n                                    Can we use Natural Language Processing to expedite manual evidence synthesis and uncover broad classes of documents\n                                    in the international development field?\n                                </strong>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Named Entity Recognition (NER) for Country of Study (CoS) Classification - Methodology</h5>\n                                <p> \n                                    The country of study (CoS) associated \n                                    with each paper is quite pertinent to the international development domain. Our dataset is labeled \n                                    with the CoS of each paper in our corpus. However, since our dataset is rather small (244 documents),\n                                    we sought to evaluate whether pretrained NER models which extract a variety of entity types from text, \n                                    could accurately extract the CoS for the papers in our corpus, which have a variety of text fields.\n                                </p>\n                                <h6>Country of Study Extraction and Classification</h6>\n                                <p> \n                                    We create a lower-cased, alphabetically-ordered, list of countries, which we construct using countryinfo\n                                    <a href=\"https://pypi.org/project/countryinfo/\" target=\"_blank\">(Link to CountryInfo PyPI page)</a>, a Python package \n                                    which contains a large dictionary of countries, their alternative names, and ISO information. We ensure \n                                    the strings of the countries present in our corpus match their respective string in the alphabetically-sorted list of countries. \n                                    We note that Myanmar and Kosovo are countries present in our corpus, but are not present in the countryinfo dictionary, so we add \n                                    them to the final list of alphabetically-sorted countries. Since nationality is a type of named entity that NER models typically \n                                    extract in addition to countries, using a comprehensive, open-source nationality-country mapping <a href=\"https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv\" target=\"_blank\">(Demonym-Country Mapping Link)</a>, \n                                    we construct a lower-cased, alphabetically-ordered list of nationalities as well as a dictionary mapping nationality to country. \n                                    We note that we use the words nationality and demonym interchangeably. These lists and dictionary are useful for performing the country of study (CoS) \n                                    classification using extracted entities from input text or determining if a nationality or country is a substring contained in the input text string.\n                                </p>\n                                <h6>Simple Substring Matcher (SSM) Algorithm Baseline  CoS Extraction & Classification</h6>\n                                <p> \n                                    As a baseline to our CoS prediction task, we devise a simple, non-ML, deterministic algorithm, called the Simple Substring Matcher (SSM) Algorithm. This \n                                    method begins by making the input text lower-cased. To predict a CoS, it then scans through the alphabetically-sorted list of countries and classifies \n                                    the first country which is a substring in the input text as the CoS. If no country is found as a substring in the text, the method then scans the alphabetically-sorted \n                                    list of nationalities. If a nationality is found as a substring of the input, the method maps the nationality to the corresponding country and classifies the paper as \n                                    having that country as the CoS. If neither country nor nationality is found as a substring in the text, the method classifies the paper's CoS as a <i>None</i> value. \n                                    We refer to this classification model as the Simple Substring Matcher (SSM) model.\n                                </p>\n                                <h6>CoS Extraction & Classification by Pretrained NER Models</h6>\n                                <p> \n                                    Although we utilize different pretrained NER models in our experiments as shown in the next slide, the process for classifying CoS using predicted entities is the same. \n                                    Each model takes the raw text as input, predicts various non-overlapping entities present in the text into one of several entity categories. For the CoS classification task, \n                                    we only consider the predicted <b>NORP</b> (nationalities, religious, or political groups) entities and the <b>GPE</b> (countries, cities, or states) entities as we assume \n                                    that these categories are the only ones which would contain the country or relevant demonym associated with the CoS. We now begin our discussion of the classification procedure \n                                    for the pretrained NER models.\n\n                                    First, we make all NORP and GPE entities lower-cased. Next, we map any demonyms present among the NORP entities to their corresponding country. We then combine the resulting unique \n                                    NORP and GPE entities into an alphabetically-ordered list. We scan this list of NORP and GPE entities checking if any of them exist in the countries list mentioned above, \n                                    classifying the CoS as the first entity-country match found. If no match is found, we make a final attempt to determine the CoS by providing each of the entities as input to the GeoPy Geocoder <a href=\"https://geopy.readthedocs.io/en/stable/\" target=\"_blank\">(Link to GeoPy API)</a>\n                                    object, which provides an address-location object if a location is found for the provided entity or no value otherwise. We do this for each entity, and if a location is found for a particular entity, \n                                    we classify the paper's CoS as the country associated with the found address-location. If no country is found for all the entities, we classify the paper's CoS as <i>None</i>.  \n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/ner-results.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>NER for Country of Study Classification - Results</h5>\n                                <p>\n                                    In addition to testing different classification models, \n                                    I experimented with different input strings to see how results change with various text fields and concatenations between them. \n                                    These various inputs to the models include the title, abstract, intervention description, outcome description, and various concatenations of these text fields.\n                                    \n                                </p>\n                                <p> \n                                    All of the pretrained spaCy NER models have 0.0% accuracy when using the just the intervention description, \n                                    however the SSM model achieves 13.9% accuracy on the intervention description. All models attain an accuracy of \n                                    2.9% when using just the outcome description. The title and abstract individually appear to be good input fields \n                                    for predicting the CoS, however the concatenation of title and abstract appears to be the most informative input, \n                                    as this is the input that yields the highest performance across all of the models. Overall, we observe that the baseline \n                                    simple substring checker is a fairly competitive model against the pretrained ML models, outperforming all the ML models\n                                    on intervention description, performing the same as the ML models on outcome description, and only falling a few percentage \n                                    points below even the best ML model on the other inputs. With the exception of the title, intervention description, \n                                    and outcome description, the ML models in increasing order of complexity, do increasingly better on the CoS extraction task, \n                                    in the following order from least performant to most performant: ESMS, ESMM, ESML, and ESMT. With the exception of the \n                                    intervention description and output description inputs, we observe that the ESMT model performs best across all other inputs. \n                                    Furthermore, we see that the concatenated title and abstract input and the ESMT model combination performs the best across \n                                    all input-model combinations with 91.0% accuracy. We use this top-performing model by accuracy to construct\n                                    AI-assisted tools which could assist researchers in the evidence synthesis process in the following slide.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/ner-res-tools.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Predictions by Pretrained Transformer NER Model for International Development Gray Literature Map and Filter Function</h5>\n                                <h6>Map of International Development Literature Gray Corpus</h6>\n                                <p> \n                                    Using the CoS predictions from the pretrained NER transformer model on the concatenated title and abstract input,  \n                                    we construct a geographical map of the corpus as shown in the left image. For each paper, which had a non-null prediction \n                                    by the ESMT model, we place a tooltip at the location coordinate associated with the predicted CoS. These location coordinates \n                                    were pulled using the GeoPy Geocoder object from the GeoPy Python package. We added slight, uniform random jitter to each of the coordinates, \n                                    so papers with the same predicted CoS don't directly overlap. When a user hovers over the tooltip, they will see the title of the paper associated with that tooltip.\n                                    The webpage for this map can be downloaded <a href=\"https://drive.google.com/file/d/1Q2P6ouwcDWrnXsq8LMpa4YqCuD7qsbtO/view?usp=sharing\" target=\"_blank\">here</a> for view in a browser.\n                                </p>\n                                <h6>Filtering by Predicted CoS</h6>\n                                <p> \n                                    For large corpora of International Development Gray Literature, the utility of the CoS prediction task is most evident by the robust filtering capability it enables. For instance, by \n                                    concatenating only the title and abstract of papers in the corpus, and using them as input to generate CoS predictions by the pretrained transformer model used in this study, this \n                                    enables the ability for unlabeled papers in the corpus to be accurately filtered to identify studies which had a specific CoS. <strong>This method would greatly reduce the time necessary for \n                                    manual CoS annotation while also yielding higher accuracy than a simple substring matcher, simplifying a step in the international literature review process with high accuracy.</strong> An example \n                                    of this filtering functionality is shown in the right image for papers in the corpus, which were predicted as having Guatemala as the CoS. The CoS was predicted using the pretrained transformer \n                                    NER model with the concatenated title and abstract as input. We display the corresponding title and abstract for quick scanning of results for relevancy to research topic. Additionally, we provide \n                                    the option to filter the corpus for papers which had a predicted CoS as <i>None</i>.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item int-dev-lit-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/int-dev-results.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Conclusion</h5>\n                                <p> \n                                    The manual evidence synthesis for international development gray literature is a time-consuming process. \n                                    We have demonstrated that certain components of the evidence synthesis process in international development gray \n                                    literature such as filtering corpora for papers which have a specific country of study or grouping similar documents \n                                    together can benefit greatly from the use of methods of information extraction and unsupervised learning. More specifically, \n                                    we have utilized a pretrained transformer NER model to accurately predict the country of study for the papers present in the corpus \n                                    used in this study, thus enabling accurate filtering of the corpus for papers with a specific predicted country of study. After tuning \n                                    to find the optimal number of clusters in K-Means clustering, we uncovered informative and distinctive clusters of documents with similar \n                                    content in the corpus. The automation of these components in the evidence synthesis process for international development grey literature \n                                    mitigates the effort and time that is required for manual evidence synthesis. \n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../constants';\nexport default {\n  name: 'NLPIntDevGrayLit',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('NLPIntLitDevCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\nh6 {\n    font-weight: bold;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.int-dev-lit-carousel-item img {\n  height: 40vh;\n  max-height: 500px;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\n@media (max-width: 500px) {\n    h3 {\n        font-size: 4vw;\n    }\n}\n\n@media (max-width: 800px) {\n    .int-dev-lit-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./NLPIntDevGrayLit.vue?vue&type=template&id=cc8558e8&scoped=true&\"\nimport script from \"./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"\nexport * from \"./NLPIntDevGrayLit.vue?vue&type=script&lang=js&\"\nimport style0 from \"./NLPIntDevGrayLit.vue?vue&type=style&index=0&id=cc8558e8&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"cc8558e8\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h3',[_vm._v(\"Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"TaxiCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#TaxiCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TaxiCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TaxiCarousel\",\"data-bs-slide-to\":\"2\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active taxi-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/taxi-fare-and-surge-pred.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Motivation\")]),_c('p',[_vm._v(\" With the advent of ridesharing apps, the New York City taxi industry must provide accurate predictions for taxi fares and demand surges in order to remain competitive. It is important that it provides riders with accurate estimates for the price of trip fare for quality customer service. It is beneficial for the taxi industry to accurately predict locations that will experience increased demand or surges in taxi demand to effectively allocate drivers and cabs to these areas in a timely manner. In this project, we used a large public dataset provided by the NYC Taxi & Limousine Commission containing yellow taxi trips records for every month from 2015-Present. We specifically utilized data from January 2019-June 2019. \")])])])]),_c('div',{staticClass:\"carousel-item taxi-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/fare-surge-graph-pred.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Graph Neural Networks for Taxi Fare and Surge Prediction\")]),_c('p',[_vm._v(\" The Yellow Taxi Trip Records contain data about taxi trips including pickup and dropoff locations, date and time of pickup and dropoff, distance traveled, and the associated fare of the trip. This data has inherent graph structure in which the nodes are the different pickup/dropoff locations and the edges are the directed trips between them. Due to the increasing advances of Graph Neural Networks (GNNs) in recent years and the advent of efficient frameworks like Deep Graph Library (DGL), we evaluated the performance of GNNs against other classical machine learning methods to assess the viability of GNNs as an accurate model which leverages the inherent graph structure in the data used for these tasks. \")])])])]),_c('div',{staticClass:\"carousel-item taxi-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/fare-surge-graph-pred.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Results\")]),_c('p',[_vm._v(\" Using the raw yellow taxi trip data from January 2019-June 2019, we contrived two datasets to pose node (surge prediction) and regression (fare prediction) problems for graphical methods. Using GraphSage GNNs, we explored and benchmarked a new application of GNNs to taxi data against classical ML approaches. For fare prediction, the GNN model performed slightly worse than Linear Regression, Random Forests, and a Fully-Connected Neural Network (FC NN), with the FC NN performing the best, however, it is highly-parameterized. For surge prediction, the GNN performed slightly better than a FC NN. For both taxi prediction tasks, we demonstrated that a less complex (having far fewer parameters) GNN model can perform comparably to a highly-parameterized FC NN. \")])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h3>Graph Neural Networks for NYC Taxi Fare & Demand Surge Prediction</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"TaxiCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#TaxiCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#TaxiCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#TaxiCarousel\" data-bs-slide-to=\"2\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active taxi-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/taxi-fare-and-surge-pred.png\" alt=\"First slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Motivation</h5>\n                                <p>\n                                    With the advent of ridesharing apps, the New York City taxi industry must provide accurate predictions for taxi fares and demand surges in order to remain competitive.\n                                    It is important that it provides riders with accurate estimates for the price of trip fare for quality customer service.\n                                    It is beneficial for the taxi industry to accurately predict locations that will experience increased demand or surges in taxi demand to effectively allocate\n                                    drivers and cabs to these areas in a timely manner. In this project, we used a large public dataset provided by the \n                                    NYC Taxi & Limousine Commission containing yellow taxi trips records for every month from 2015-Present. \n                                    We specifically utilized data from January 2019-June 2019.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item taxi-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/fare-surge-graph-pred.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Graph Neural Networks for Taxi Fare and Surge Prediction</h5>\n                                <p> \n                                    The Yellow Taxi Trip Records contain data about taxi trips including pickup and dropoff locations, date and time of pickup and dropoff,\n                                    distance traveled, and the associated fare of the trip.  This data has inherent graph structure in which the nodes are the different pickup/dropoff locations\n                                    and the edges are the directed trips between them. Due to the increasing advances of Graph Neural Networks (GNNs) in recent years and the advent of efficient frameworks like Deep Graph Library (DGL),\n                                    we evaluated the performance of GNNs against other classical machine learning methods to assess the viability of GNNs as an accurate model which leverages the inherent graph structure in the data used for these tasks.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item taxi-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/fare-surge-graph-pred.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Results</h5>\n                                <p> \n                                    Using the raw yellow taxi trip data from January 2019-June 2019, we contrived two datasets\n                                    to pose node (surge prediction) and regression (fare prediction) problems for graphical methods. Using GraphSage GNNs, we explored and benchmarked\n                                    a new application of GNNs to taxi data against classical ML approaches. For fare prediction, the GNN model performed slightly worse\n                                    than Linear Regression, Random Forests, and a Fully-Connected Neural Network (FC NN), with the FC NN performing the best, however, it is highly-parameterized. For surge prediction, the GNN performed slightly better than a FC NN.\n                                    For both taxi prediction tasks, we demonstrated that a less complex (having far fewer parameters) GNN model can perform comparably to a highly-parameterized FC NN.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../constants';\n\nexport default {\n  // eslint-disable-next-line\n  name: 'Taxi',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('TaxiCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h3, h5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.taxi-carousel-item img {\n  height: 450px;\n  max-height: 500px;\n}\n\n@media (max-width: 800px) {\n    .taxi-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./GNNsTaxiPrediction.vue?vue&type=template&id=0604cdbf&scoped=true&\"\nimport script from \"./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"\nexport * from \"./GNNsTaxiPrediction.vue?vue&type=script&lang=js&\"\nimport style0 from \"./GNNsTaxiPrediction.vue?vue&type=style&index=0&id=0604cdbf&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"0604cdbf\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h1',[_vm._v(\"Trump Campaign Speech Analysis\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"TrumpCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#TrumpCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TrumpCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#TrumpCarousel\",\"data-bs-slide-to\":\"2\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active trump-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/trump-campaign.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Main Puzzle\")]),_c('p',[_vm._v(\" There have been concerns that nationalist, right-wing sentiments have gained momentum over the years of the Trump presidency. Our group wanted to investigate how Donald Trumps rhetoric may have influenced public sentiment on a regional level. To this end, we analyzed Trump's campaign speeches and the tweets by locals from 4 cities he visited on his campaign and Florida, a swing state. \")])])])]),_c('div',{staticClass:\"carousel-item trump-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/negative-positive.jpg\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Most Frequent Negative and Positive Words in Trump's Campaign Speeches\")]),_c('p',[_vm._v(\" Trump's positive sentiment words tend to be adjectives with \\\"great\\\" far exceeding the rest. Among words with negative sentiment, there are more meaningful words related to his speech topics such as \\\"investigation\\\", \\\"defense\\\", \\\"deficit\\\", & \\\"press\\\". \")])])])]),_c('div',{staticClass:\"carousel-item trump-carousel-item\"},[_c('div',{staticClass:\"row justify-content-around pb-3\"},[_c('img',{staticClass:\"col-12 col-md-6 rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/RelativeWordFrequencyDiff.png\")}}),_c('img',{staticClass:\"col-12 col-md-6 pt-3 pt-md-0 rounded img-fluid\",attrs:{\"src\":require(\"../../../public/assets/RelativeWordFreqDiffFlorida.png\")}})]),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Trump's Most Frequently Used Words Across his Entire Campaign & Across Florida Campaign\")]),_c('p',[_vm._v(\" In Trump's speeches across the entire campaign, his most frequent words, normalized on Romney's campaign speeches, include \\\"Hillary\\\", \\\"don't\\\" \\\"great\\\", \\\"deal\\\", as well as words related to his election platform such as \\\"border\\\", \\\"wall\\\", \\\"Mexico\\\", \\\"ISIS\\\", \\\"trade\\\", and \\\"China\\\". Words used to thwart Hillary Clinton's campaign such as \\\"Hillary\\\", \\\"email\\\", \\\"lies\\\", \\\"corrupt\\\", \\\"crook\\\", and \\\"FBI\\\" in regards to Clinton's email scandal appear more frequently in Trump's Florida campaign speeches than across all of his campaign speeches, \"),_c('strong',[_vm._v(\"showing that in swing states, Trump strategizes to mention the scandal more frequently to win voters to tip the scale.\")])])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h1>Trump Campaign Speech Analysis</h1>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"TrumpCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#TrumpCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#TrumpCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#TrumpCarousel\" data-bs-slide-to=\"2\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active trump-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/trump-campaign.png\" alt=\"First slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Main Puzzle</h5>\n                                <p>\n                                    There have been concerns that nationalist, right-wing sentiments have gained momentum over the years of the Trump presidency. \n                                    Our group wanted to investigate how Donald Trumps rhetoric may have influenced public sentiment on a regional level. \n                                    To this end, we analyzed Trump's campaign speeches and the tweets by locals from 4 cities he visited on his campaign and Florida,\n                                    a swing state.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item trump-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/negative-positive.jpg\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Most Frequent Negative and Positive Words in Trump's Campaign Speeches</h5>\n                                <p> \n                                    Trump's positive sentiment words tend to be adjectives with \"great\" far exceeding the rest. Among words with negative sentiment, \n                                    there are more meaningful words related to his speech topics such as \"investigation\", \"defense\", \"deficit\", & \"press\". \n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item trump-carousel-item\">\n                        <div class=\"row justify-content-around pb-3\">\n                            <img class=\"col-12 col-md-6 rounded img-fluid\" src=\"../../../public/assets/RelativeWordFrequencyDiff.png\">\n                            <img class=\"col-12 col-md-6 pt-3 pt-md-0  rounded img-fluid\" src=\"../../../public/assets/RelativeWordFreqDiffFlorida.png\">\n                        </div>\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Trump's Most Frequently Used Words Across his Entire Campaign & Across Florida Campaign</h5>\n                                <p> \n                                    In Trump's speeches across the entire campaign, his most frequent words, normalized on Romney's campaign speeches, include\n                                    \"Hillary\", \"don't\" \"great\", \"deal\", as well as words related to his election platform such as \"border\", \"wall\", \"Mexico\", \"ISIS\", \"trade\", \n                                    and \"China\". Words used to thwart Hillary Clinton's campaign such as \"Hillary\", \"email\", \"lies\", \"corrupt\", \"crook\", and \"FBI\" in regards to Clinton's email scandal \n                                    appear more frequently in Trump's Florida campaign speeches than across all of his campaign speeches, <strong>showing that in swing states, \n                                    Trump strategizes to mention the scandal more frequently to win voters to tip the scale.</strong>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../constants';\n\nexport default {\n  // eslint-disable-next-line\n  name: 'Trump',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('TrumpCarousel')\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1 {\n    color: white;\n}\n\nh5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.trump-carousel-item img {\n  height: 450px;\n  max-height: 500px;\n}\n\n@media (max-width: 800px) {\n    .trump-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./TrumpSpeechAnalysis.vue?vue&type=template&id=4dc29e15&scoped=true&\"\nimport script from \"./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"\nexport * from \"./TrumpSpeechAnalysis.vue?vue&type=script&lang=js&\"\nimport style0 from \"./TrumpSpeechAnalysis.vue?vue&type=style&index=0&id=4dc29e15&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"4dc29e15\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\"},[_c('h3',[_vm._v(\"Evolution of the U.S. TV News Narrative on Climate Change\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"ClimateNewsCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#ClimateNewsCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ClimateNewsCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ClimateNewsCarousel\",\"data-bs-slide-to\":\"2\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ClimateNewsCarousel\",\"data-bs-slide-to\":\"3\"}}),_c('li',{attrs:{\"data-bs-target\":\"#ClimateNewsCarousel\",\"data-bs-slide-to\":\"4\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/final-project-overview.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Project Code, Poster, and Report\")]),_c('p',[_vm._v(\" This was a final project for IDS.131: Statistics, Computation, and Applications. This presentation focuses on introducing the project, the specific parts I worked on, and the main findings from our analysis. \"),_c('br'),_vm._v(\" A brief overview of the methods, visualizations, and results is available in this \"),_c('a',{attrs:{\"href\":\"./assets/IDS131_Poster.pdf\",\"target\":\"_blank\"}},[_vm._v(\"poster PDF.\")]),_c('br'),_vm._v(\" The more thorough report of our findings with all visualizations is \"),_c('a',{attrs:{\"href\":\"./assets/IDS131_Final_Report.pdf\",\"target\":\"_blank\"}},[_vm._v(\"here.\")]),_c('br'),_vm._v(\" The code for this project was written in Python. \"),_c('a',{attrs:{\"href\":\"https://github.com/dyllew/ids.131-fp\",\"target\":\"_blank\"}},[_vm._v(\"Here's the GitHub Repo.\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('div',{staticClass:\"row justify-content-center pb-5\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_c('strong',[_vm._v(\"Motivation & Research Question\")])]),_c('p',[_vm._v(\" Print and televised media reporting on climate change influences the public perception of climate change, which in turn affects support for systemic policies to reduce greenhouse gas emissions and for individual actions to mitigate climate change. Over two thirds of Americans get their news often or sometimes from television. In this analysis, we looked at ten years of data from three television stations: CNN, Fox News, and MSNBC to address the following research question: \")]),_c('br'),_c('strong',{attrs:{\"id\":\"research-question\"}},[_vm._v(\" How has the frequency and content of top American English-speaking news media coverage of climate change evolved in the past ten years (July 2009-January 2020)-and what environmental and political factors have influenced the trends? \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/network_tfidf_wordclouds.png\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Dataset, Exploration, and Preprocessing\")]),_c('p',[_vm._v(\" The data used for this analysis included text transcripts of 15-second audio clips of TV news mentioning of climate change for CNN, MSNBC, and Fox News from July 2009-January 2020, provided by the \"),_c('a',{attrs:{\"href\":\"https://blog.gdeltproject.org/a-new-dataset-for-exploring-climate-change-narratives-on-television-news-2009-2020/\",\"target\":\"_blank\"}},[_vm._v(\"GDELT Project.\")]),_vm._v(\" We also refer to these text transcripts as text \\\"snippets\\\". The features of the data points include time and date of the mention, the TV news network identity, the specific show on the network, and the text snippet of the transcribed audio. This dataset provides the ability to compare the TV networks over time on the subject of climate change in order to answer the research question we have previously posed. We followed standard Natural Language Processing (NLP) text preprocessing by removing punctuation and numbers, converting all letters to lower-case (the data was already provided as lowercase), lemmatizing, removing standard English stopwords & corpus-specific stopwords, and tokenizing the data into words. We conducted our analysis with two distinct analysis streams: Frequency Analysis and Content Analysis. Our Frequency Analysis entailed methods of time-series analysis of climate change mentions by the different TV networks over time and dynamic time-warping & STL decomposition. Content Analysis used methods of TF-IDF document embeddings, Cosine Similarity as a proxy for content similarity between documents, and Topic Modeling & Change-Point Detection. The above figure was created as part of an exploratory component of the TF-IDF Embedding analysis, in which we wished to extract the most important words to each of the networks across the entire corpus. \")])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/networks_and_years_cosine_similarity.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Content Analysis: TF-IDF Document Embedding & Cosine Similarity\")]),_c('h6',[_vm._v(\"Methodology\")]),_c('p',[_vm._v(\" My contribution to this project was primarly focused in the Content Analysis stream, specifically TF-IDF document embeddings & Cosine Similarity between documents. To conduct our content analysis, we needed to featurize the news snippets, or collections of snippets, which form the documents in our corpus. I first formed a document for each TV network in our dataset (e.g. all snippets for CNN), and transformed the documents into a L2-normalized unit vector TF-IDF vector embedding. From this featurization, I formed a document-term matrix, where the rows correspond to the TF-IDF embedding of a document and the column represents a unique word in the corpus (~34k words). Thus, entry i, j corresponds to the normalized TF-IDF score of word j in document i (i.e. word j's relative importance for the ith document). I also constructed document-term matrices for documents representing each year of our dataset (e.g. all snippets in 2009) as well as for documents constructed from networks in specific years (e.g. all CNN snippets in 2015). Finally, for each of the document-term matrices mentioned above, I calculated the pairwise cosine similarity between the document embeddings to yield a measure of content similarity between the documents. A heatmap constructed from the computed cosine similarities between the network in each year documents is shown above. \")]),_c('h6',[_vm._v(\"Main Findings\")]),_c('p',[_vm._v(\" The main findings from my analysis were that the \"),_c('strong',[_vm._v(\"content of climate mentions in the latter years of the dataset, 2016-2020, are most dissimilar to earlier years in the dataset, 2009-2012. We note that these years correspond roughly to Obama's (D) first term as president (2008-2012) and Donald Trump's (R) presidency (2016-2020).\")]),_vm._v(\" Additionally, in the years 2010 & 2018, the content of MSNBC differed greatly from the other news networks in those years and with other networks and itself in other years. This similarly occurred for CNN in 2012 & 2013. Lastly, with the exception of these years, \"),_c('strong',[_vm._v(\"the similarity of content of climate mentions between CNN and MSNBC, the liberal-leaning networks, has been increasing over the years, although the pairwise content similarity between all of the networks is fairly high over time.\")])])])])]),_c('div',{staticClass:\"carousel-item cc-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/final-project-overview.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-9\"},[_c('h5',[_vm._v(\"Results\")]),_c('p',[_vm._v(\" Climate change TV news media coverage frequency and content appears to be significantly driven by political events more so than environmental factors. The frequency of climate change mentions follow similar patterns by network, with clear influence of political events such as the 2009 UNCCC, 2015 Paris Agreement, and 2019 Democratic primary debates driving climate news coverage. This is reflected in the content of the climate mentions over time as words describing the political events occurring at the time tend to be the most important words for all of the networks in that specific year. This is further seen by the tendency of different networks in the same year to have high content similarity. Topic analysis also finds that the majority of the 15 topics found from the topic analysis had significant changes in mean on some of the topics at the time of Donald Trump's inauguration. \")])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div class=\"col-12\">  \n            <h3>Evolution of the U.S. TV News Narrative on Climate Change</h3>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"ClimateNewsCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#ClimateNewsCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#ClimateNewsCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#ClimateNewsCarousel\" data-bs-slide-to=\"2\"></li>\n                    <li data-bs-target=\"#ClimateNewsCarousel\" data-bs-slide-to=\"3\"></li>\n                    <li data-bs-target=\"#ClimateNewsCarousel\" data-bs-slide-to=\"4\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active cc-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/final-project-overview.png\" alt=\"First slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Project Code, Poster, and Report</h5>\n                                <p>\n                                    This was a final project for IDS.131: Statistics, Computation, and Applications. This presentation focuses on\n                                    introducing the project, the specific parts I worked on, and the main findings from our analysis.\n                                    <br>\n                                    A brief overview of the methods, visualizations, \n                                    and results is available in this <a href=\"./assets/IDS131_Poster.pdf\" target=\"_blank\">poster PDF.</a>\n                                    <br>\n                                    The more thorough report of our findings with all visualizations is <a href=\"./assets/IDS131_Final_Report.pdf\" target=\"_blank\">here.</a>\n                                    <br>\n                                    The code for this project was written in Python. <a href=\"https://github.com/dyllew/ids.131-fp\" target=\"_blank\">Here's the GitHub Repo.</a>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <div class=\"row justify-content-center pb-5\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5><strong>Motivation & Research Question</strong></h5>\n                                <p>\n                                    Print and televised media reporting on climate change influences \n                                    the public perception of climate change, which in turn affects support for \n                                    systemic policies to reduce greenhouse gas emissions and for individual actions to \n                                    mitigate climate change. Over two thirds of Americans get their news often or \n                                    sometimes from television. In this analysis, we looked at ten years of \n                                    data from three television stations: CNN, Fox News, and MSNBC to address the \n                                    following research question:\n                                </p>\n                                <br>\n                                <strong id='research-question'>\n                                    How has the frequency and content of \n                                    top American English-speaking news media coverage of climate change \n                                    evolved in the past ten years (July 2009-January 2020)-and what environmental and political factors \n                                    have influenced the trends?\n                                </strong>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/network_tfidf_wordclouds.png\" alt=\"First slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Dataset, Exploration, and Preprocessing</h5>\n                                <p>\n                                    The data used for this analysis included text transcripts of 15-second audio clips of TV news mentioning of\n                                    climate change for CNN, MSNBC, and Fox News from \n                                    July 2009-January 2020, provided by the <a href=\"https://blog.gdeltproject.org/a-new-dataset-for-exploring-climate-change-narratives-on-television-news-2009-2020/\" target=\"_blank\">GDELT Project.</a> We\n                                    also refer to these text transcripts as text \"snippets\".\n                                    The features of the data points include time and date of the mention, \n                                    the TV news network identity, the specific show on the network, and the text snippet of the transcribed audio. \n                                    This dataset provides the ability to compare the TV networks over time on the subject of \n                                    climate change in order to answer the research question we have previously posed. \n                                    We followed standard Natural Language Processing (NLP) text preprocessing by removing punctuation and numbers, \n                                    converting all letters to lower-case (the data was already provided as lowercase), lemmatizing, removing standard English stopwords & \n                                    corpus-specific stopwords, and tokenizing the data into words. We conducted our analysis with two distinct\n                                    analysis streams: Frequency Analysis and Content Analysis. Our Frequency Analysis entailed methods of time-series analysis of climate change mentions\n                                    by the different TV networks over time and dynamic time-warping & STL decomposition. Content Analysis used methods of TF-IDF document embeddings,\n                                    Cosine Similarity as a proxy for content similarity between documents, and Topic Modeling & Change-Point Detection. The above figure was created as part of \n                                    an exploratory component of the TF-IDF Embedding analysis, in which we wished to extract the most important words to each of the networks across the entire corpus.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/networks_and_years_cosine_similarity.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Content Analysis: TF-IDF Document Embedding & Cosine Similarity</h5>\n                                <h6>Methodology</h6>\n                                <p> \n                                    My contribution to this project was primarly focused in the Content Analysis stream, specifically TF-IDF document embeddings & \n                                    Cosine Similarity between documents. To conduct our content analysis, we needed to featurize the news snippets, or collections of snippets, \n                                    which form the documents in our corpus. I first formed a document for each TV network in our dataset\n                                    (e.g. all snippets for CNN), and transformed the documents into a L2-normalized unit vector TF-IDF vector embedding. \n                                    From this featurization, I formed a document-term matrix, where the rows correspond to the TF-IDF embedding of a document and the column \n                                    represents a unique word in the corpus (~34k words). Thus, entry i, j corresponds to the normalized \n                                    TF-IDF score of word j in document i (i.e. word j's relative importance for the ith document). I also constructed document-term matrices for documents representing each year of our dataset\n                                    (e.g. all snippets in 2009) as well as for documents constructed from networks in specific years (e.g. all CNN snippets in 2015). Finally, for each of the document-term matrices mentioned above, \n                                    I calculated the pairwise cosine similarity between the document embeddings to yield a measure of content similarity between the documents. \n                                    A heatmap constructed from the computed cosine similarities between the network in each year documents is shown above. \n                                </p>\n                                <h6>Main Findings</h6>\n                                <p>\n                                    The main findings from my analysis were \n                                    that the <strong>content of climate mentions in the latter years of the dataset, 2016-2020, are most dissimilar to earlier years in the dataset, 2009-2012. We note that these years correspond roughly to Obama's (D) first term as president (2008-2012)\n                                    and Donald Trump's (R) presidency (2016-2020).</strong>\n                                    Additionally, in the years 2010 & 2018, the content of MSNBC differed greatly from the other news networks in those years and with other networks and itself in other years. \n                                    This similarly occurred for CNN in 2012 & 2013. Lastly, with the exception of these years, <strong>the similarity of content of climate mentions between CNN and MSNBC, \n                                    the liberal-leaning networks, has been increasing over the years, although the pairwise content similarity between all of the networks is fairly high over time.</strong>\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item cc-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/final-project-overview.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-9\">\n                                <h5>Results</h5>\n                                <p> \n                                    Climate change TV news media coverage frequency and \n                                    content appears to be significantly driven by political events\n                                    more so than environmental factors. The frequency of climate change mentions \n                                    follow similar patterns by network, with clear influence of political events such as \n                                    the 2009 UNCCC, 2015 Paris Agreement, and 2019 Democratic primary debates driving \n                                    climate news coverage. This is reflected in the content of the \n                                    climate mentions over time as words describing the political events \n                                    occurring at the time tend to be the most important words for all of the networks \n                                    in that specific year. This is further seen by the tendency of different networks in the same \n                                    year to have high content similarity. Topic analysis also finds that the majority of the 15 topics \n                                    found from the topic analysis had significant changes in mean on some of the topics at the time of \n                                    Donald Trump's inauguration.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../constants';\nexport default {\n  name: 'ClimateChangeNews',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel('ClimateNewsCarousel');\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\n#research-question {\n    text-align: center;\n    color: white;\n}\n\na {\n    color: hotpink;\n}\n\na:hover {\n    color: white;\n}\n\n\nh1, h3, h5, h6 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.cc-carousel-item img {\n  max-height: 30vw;\n}\n\n@media (max-width: 800px) {\n    .cc-carousel-item img {\n        max-height: 80vw;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./ClimateChangeNews.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./ClimateChangeNews.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./ClimateChangeNews.vue?vue&type=template&id=478b41a4&scoped=true&\"\nimport script from \"./ClimateChangeNews.vue?vue&type=script&lang=js&\"\nexport * from \"./ClimateChangeNews.vue?vue&type=script&lang=js&\"\nimport style0 from \"./ClimateChangeNews.vue?vue&type=style&index=0&id=478b41a4&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"478b41a4\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"row align-items-center justify-content-center\"},[_c('div',{staticClass:\"col-12\",attrs:{\"id\":\"title\"}},[_c('h1',[_vm._v(\"Boomerang\")])]),_c('div',{staticClass:\"col-md-8\"},[_c('div',{staticClass:\"carousel slide\",attrs:{\"id\":\"BoomerangCarousel\"}},[_c('ol',{staticClass:\"carousel-indicators\"},[_c('li',{staticClass:\"active\",attrs:{\"data-bs-target\":\"#BoomerangCarousel\",\"data-bs-slide-to\":\"0\"}}),_c('li',{attrs:{\"data-bs-target\":\"#BoomerangCarousel\",\"data-bs-slide-to\":\"1\"}}),_c('li',{attrs:{\"data-bs-target\":\"#BoomerangCarousel\",\"data-bs-slide-to\":\"2\"}})]),_c('div',{staticClass:\"carousel-inner\"},[_c('div',{staticClass:\"carousel-item active boomerang-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/boomerang-home.jpg\"),\"alt\":\"First slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Sign Up/Login Page\")]),_c('p',[_vm._v(\"When my final project group decided on how we wanted to split up the tasks for Boomerang, I decided to design & create the sign up/login page & the create account flow. I thought this would be a good section of the application to practice and enhance my visual design skills and to create a UI that was intuitive. \")])])])]),_c('div',{staticClass:\"carousel-item boomerang-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/create-account.png\"),\"alt\":\"Second slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Create Account Part I\")]),_c('p',[_vm._v(\" In the sections of Boomerang I built, I created the front-end using Vue.js which is the same front-end framework I used to build this website. The backend was built using Express.js. The fields above checked for user input to ensure that the account username was not already taken and that the each of the fields was in the correct format, notifying the user instantly on submission if their input was invalid. \")])])])]),_c('div',{staticClass:\"carousel-item boomerang-carousel-item\"},[_c('img',{staticClass:\"rounded img-fluid pb-3\",attrs:{\"src\":require(\"../../../public/assets/join-communities.png\"),\"alt\":\"Third slide\"}}),_c('div',{staticClass:\"row justify-content-center pb-4\"},[_c('div',{staticClass:\"col-12 col-md-7\"},[_c('h5',[_vm._v(\"Create Account Part II\")]),_c('p',[_vm._v(\" Account creation for any app is an important user flow as it lets the user understand both the purpose of an app & how they can engage with it completely. For these reasons, I decided to include descriptions of the main concepts of the application (Communities, Channels, etc.) as this would reduce the time it would take for a user to get immersed in the app. \")])])])])])])])])\n}]\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"row align-items-center justify-content-center\">\n        <div id=\"title\" class=\"col-12\">  \n            <h1>Boomerang</h1>\n        </div>\n        <div class=\"col-md-8\">\n            <div id=\"BoomerangCarousel\" class=\"carousel slide\">\n                <ol class=\"carousel-indicators\">\n                    <li data-bs-target=\"#BoomerangCarousel\" data-bs-slide-to=\"0\" class=\"active\"></li>\n                    <li data-bs-target=\"#BoomerangCarousel\" data-bs-slide-to=\"1\"></li>\n                    <li data-bs-target=\"#BoomerangCarousel\" data-bs-slide-to=\"2\"></li>\n                </ol>\n                <div class=\"carousel-inner\">\n                    <div class=\"carousel-item active boomerang-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/boomerang-home.jpg\" alt=\"First slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Sign Up/Login Page</h5>\n                                <p>When my final project group decided on how we wanted to split up the tasks for Boomerang,\n                                    I decided to design & create the sign up/login page & the create account flow. \n                                    I thought this would be a good section of the application to practice and enhance my visual design skills\n                                    and to create a UI that was intuitive.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item boomerang-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/create-account.png\" alt=\"Second slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Create Account Part I</h5>\n                                <p> \n                                    In the sections of Boomerang I built, I created the front-end using \n                                    Vue.js which is the same front-end framework I used to build this website. \n                                    The backend was built using Express.js. The fields above checked for user input\n                                    to ensure that the account username was not already taken and that the each of the fields was\n                                    in the correct format, notifying the user instantly on submission if their input was invalid.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                    <div class=\"carousel-item boomerang-carousel-item\">\n                        <img class=\"rounded img-fluid pb-3\" src=\"../../../public/assets/join-communities.png\" alt=\"Third slide\">\n                        <div class=\"row justify-content-center pb-4\">\n                            <div class=\"col-12 col-md-7\">\n                                <h5>Create Account Part II</h5>\n                                <p> \n                                    Account creation for any app is an important user flow as it\n                                    lets the user understand both the purpose of an app & how they can engage with it completely.\n                                    For these reasons, I decided to include descriptions of the main concepts of the application (Communities, Channels, etc.)\n                                    as this would reduce the time it would take for a user to get immersed in the app.\n                                </p>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>\n</template>\n\n<script>\nimport { scrollUpFunc, enableSwipeOnCarousel } from '../../constants';\nexport default {\n  // eslint-disable-next-line\n  name: 'Boomerang',\n  mounted() {\n    scrollUpFunc();\n    enableSwipeOnCarousel(\"BoomerangCarousel\");\n  }\n}\n</script>\n\n<style scoped>\n\np {\n    text-align: left;\n}\n\nh1, h5 {\n    color: white;\n}\n\n.carousel-text {\n    margin-top: 10px;\n    margin-bottom: 40px;\n}\n\n.boomerang-carousel-item img {\n  height: 450px;\n  max-height: 500px;\n}\n\n@media (max-width: 800px) {\n    .boomerang-carousel-item img {\n        height: 450px;\n        max-height: 200px;\n    }\n}\n\n</style>\n","import mod from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Boomerang.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Boomerang.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Boomerang.vue?vue&type=template&id=5bf81500&scoped=true&\"\nimport script from \"./Boomerang.vue?vue&type=script&lang=js&\"\nexport * from \"./Boomerang.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Boomerang.vue?vue&type=style&index=0&id=5bf81500&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"5bf81500\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"col pt-5 justify-content-center\"},[_c('h2',[_vm._v(\"Uh Oh! Looks like you went to a page that doesn't exist on dyllew.github.io\")]),_c('router-link',{staticClass:\"router-link\",attrs:{\"to\":\"/\"}},[_vm._v(\"Click this link to go home.\")])],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","<template>\n    <div class=\"col pt-5 justify-content-center\">\n      <h2>Uh Oh! Looks like you went to a page that doesn't exist on dyllew.github.io</h2>\n      <router-link class=\"router-link\" to=\"/\">Click this link to go home.</router-link>\n    </div>  \n</template>\n\n<script>\nexport default {\n  name: 'NotFoundComponent'\n}\n</script>\n\n<style scoped>\n\n.router-link {\n  color: #61DAFB;\n  font-size: 30px;\n}\n\n.router-link:hover {\n  color: whitesmoke;\n}\n\n.router-link-active {\n  color: white;\n  text-decoration: underline;\n}\n\n\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./NotFoundComponent.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./NotFoundComponent.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./NotFoundComponent.vue?vue&type=template&id=4ffa6c61&scoped=true&\"\nimport script from \"./NotFoundComponent.vue?vue&type=script&lang=js&\"\nexport * from \"./NotFoundComponent.vue?vue&type=script&lang=js&\"\nimport style0 from \"./NotFoundComponent.vue?vue&type=style&index=0&id=4ffa6c61&prod&scoped=true&lang=css&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"4ffa6c61\",\n  null\n  \n)\n\nexport default component.exports","import Vue from 'vue';\nimport VueRouter from 'vue-router';\n\nVue.use(VueRouter);\n\n// import components below\nimport Home from './components/Home'\nimport About from './components/About';\nimport Projects from './components/Projects';\nimport Artwork from './components/Artwork';\nimport Resume from './components/Resume';\nimport MLForCrowdsourcedCrisisData from './components/project-pages/ml-for-crowdsourced-crisis-data/MLForCrowdsourcedCrisisData';\nimport ImageAnalysisCarousel from './components/project-pages/ml-for-crowdsourced-crisis-data/ImageAnalysisCarousel';\nimport TextAnalysisCarousel from './components/project-pages/ml-for-crowdsourced-crisis-data/TextAnalysisCarousel';\nimport NLPIntDevGrayLit from './components/project-pages/NLPIntDevGrayLit';\nimport GNNsTaxiPrediction from './components/project-pages/GNNsTaxiPrediction';\nimport TrumpSpeechAnalysis from './components/project-pages/TrumpSpeechAnalysis';\nimport ClimateChangeNews from './components/project-pages/ClimateChangeNews';\nimport Boomerang from './components/project-pages/Boomerang';\nimport NotFoundComponent from './components/NotFoundComponent';\n\n// store router -> components mappings\nconst router = [\n    {\n        path: '/',\n        component: Home\n    },\n    {\n        path: '/about',\n        component: About\n    },\n    {\n        path: '/projects',\n        component: Projects\n    },\n    {\n        path: '/projects/boomerang',\n        component: Boomerang\n    },\n    {\n        path: '/projects/ml-for-crowdsourced-crisis-data',\n        component: MLForCrowdsourcedCrisisData\n    },\n    {\n        path: '/projects/ml-for-crowdsourced-crisis-data/image-analysis-module',\n        component: ImageAnalysisCarousel\n    },\n    {\n        path: '/projects/ml-for-crowdsourced-crisis-data/text-analysis-module',\n        component: TextAnalysisCarousel\n    },\n    {\n        path: '/projects/nlp-for-int-dev-gray-lit',\n        component: NLPIntDevGrayLit\n    },\n    {\n        path: '/projects/trump-speech-analysis',\n        component: TrumpSpeechAnalysis\n    },\n    {\n        path: '/projects/gnns-taxi-prediction',\n        component: GNNsTaxiPrediction\n    },\n    {\n        path: '/projects/climate-change-news',\n        component: ClimateChangeNews\n    },\n    {\n        path: '/artwork',\n        component: Artwork\n    },\n    {\n        path: '/resume',\n        component: Resume\n    },\n    { \n        path: '/404', \n        component: NotFoundComponent\n    },  \n    {\n        path: '*',\n        redirect: '/404'\n    }\n\n];\n\nconst vueRouter = new VueRouter({\n    mode: 'hash',\n    routes: router\n});\n\nexport default vueRouter;","import Vue from 'vue';\nimport { BootstrapVue, IconsPlugin } from 'bootstrap-vue';\nimport App from './App.vue';\n\nimport router from './router';\nimport 'bootstrap/dist/css/bootstrap.css';\nimport 'bootstrap-vue/dist/bootstrap-vue.css';\n// import 'jquery/src/jquery.js';\nimport 'bootstrap/dist/js/bootstrap.min.js';\n\nVue.config.productionTip = false;\nVue.use(BootstrapVue);\nVue.use(IconsPlugin);\n\nnew Vue({\n  router,\n  render: h => h(App)\n}).$mount('#app')\n","var map = {\n\t\"./17_835_Poster.pdf\": 7963,\n\t\"./6_864_Project.pdf\": 7670,\n\t\"./Dylan_Lewis_Resume.pdf\": 8955,\n\t\"./FrequencyPlot.png\": 7921,\n\t\"./IDS131_Final_Report.pdf\": 7939,\n\t\"./IDS131_Poster.pdf\": 6996,\n\t\"./NER.png\": 2593,\n\t\"./RelativeWordFreqDiffFlorida.png\": 216,\n\t\"./RelativeWordFrequencyDiff.png\": 73,\n\t\"./adjacent-projects.png\": 5846,\n\t\"./agg-metrics-fc.png\": 7230,\n\t\"./algo-selection-table.png\": 7833,\n\t\"./bag-of-word-features.png\": 1533,\n\t\"./bert-features.png\": 4831,\n\t\"./bert-preprocessing.png\": 5926,\n\t\"./boomerang-home.jpg\": 526,\n\t\"./character_box_and_whisk_fc_rm.png\": 1941,\n\t\"./cluster-labels.png\": 2130,\n\t\"./clustering-hyperparameters.png\": 6503,\n\t\"./clustering-pipeline.png\": 7828,\n\t\"./create-account.png\": 1386,\n\t\"./damage_severity_confusion_matrix.png\": 9228,\n\t\"./damage_severity_per_class_metric.png\": 345,\n\t\"./ds-fc.png\": 3667,\n\t\"./ds-train.png\": 531,\n\t\"./dylan-n-leo.jpg\": 8056,\n\t\"./elbow-plot.png\": 4039,\n\t\"./fare-surge-graph-pred.png\": 3930,\n\t\"./feelings.jpg\": 1818,\n\t\"./final-assessment.png\": 2949,\n\t\"./final-project-overview.png\": 8885,\n\t\"./flood_confusion_matrix.png\": 7914,\n\t\"./flood_presence_per_class_metric.png\": 9165,\n\t\"./fp-fc.png\": 6518,\n\t\"./fp-train.png\": 5945,\n\t\"./hc-fc.png\": 523,\n\t\"./hc-train.png\": 1073,\n\t\"./human-risk-aucpr.png\": 2786,\n\t\"./human-risk-cm-svm.png\": 23,\n\t\"./human-risk-data-splits.png\": 488,\n\t\"./human-risk-diagram.png\": 1613,\n\t\"./human-risk-label-distribution.png\": 6249,\n\t\"./human-risk-model-evaluation.png\": 6385,\n\t\"./human-risk-per-class-metric.png\": 7465,\n\t\"./humanitarian_categories_confusion_matrix.png\": 5176,\n\t\"./humanitarian_categories_per_class_metric.png\": 1730,\n\t\"./iaa.png\": 3244,\n\t\"./identified-elbow.png\": 9202,\n\t\"./image-analysis-module-modified.png\": 7942,\n\t\"./image-analysis-module.png\": 5348,\n\t\"./in-fc.png\": 1760,\n\t\"./in-train.png\": 304,\n\t\"./informativeness_confusion_matrix.png\": 2224,\n\t\"./informativeness_per_class_metric.png\": 2257,\n\t\"./int-dev-gray-lit.pdf\": 6038,\n\t\"./int-dev-results.png\": 6404,\n\t\"./join-communities.png\": 4511,\n\t\"./labeled-clusters.png\": 5937,\n\t\"./leo_n_me.jpg\": 1924,\n\t\"./linkedin-profpic.jpg\": 8257,\n\t\"./masters-thesis-overview.png\": 2696,\n\t\"./n-gram-preprocessing.png\": 443,\n\t\"./negative-positive.jpg\": 7503,\n\t\"./ner-res-tools.png\": 8246,\n\t\"./ner-results.png\": 9112,\n\t\"./nested-cv-graph.png\": 2281,\n\t\"./nested-cv-table.png\": 8595,\n\t\"./nested-cv.png\": 2817,\n\t\"./network_cosine_similarity.png\": 1289,\n\t\"./network_tfidf_wordclouds.png\": 5823,\n\t\"./networks_and_years_cosine_similarity.png\": 2414,\n\t\"./pika-gif.gif\": 2156,\n\t\"./portrait.jpg\": 6792,\n\t\"./preliminary-assessment.png\": 4047,\n\t\"./project-collage.png\": 1871,\n\t\"./qualitative-summaries.png\": 5668,\n\t\"./query-subset.png\": 9394,\n\t\"./reptile.png\": 3617,\n\t\"./resume.png\": 6606,\n\t\"./svm-hyperparameters.png\": 8305,\n\t\"./taxi-fare-and-surge-pred.png\": 6818,\n\t\"./taxi-proj-thumbnail.png\": 2340,\n\t\"./test-set-eval.png\": 7173,\n\t\"./text-analysis-module.png\": 2187,\n\t\"./text-features-for-clustering.png\": 2781,\n\t\"./text-preprocessing.png\": 2544,\n\t\"./tf-idf-features.png\": 6287,\n\t\"./tfidf-features.png\": 4608,\n\t\"./tfidf-networks.png\": 2747,\n\t\"./trump-campaign.png\": 3081,\n\t\"./txt-data-collection.png\": 9608,\n\t\"./unlabeled-clusters.png\": 1548,\n\t\"./workshop-insights.png\": 1120,\n\t\"./workshop-preface-questions.png\": 937,\n\t\"./years_cosine_similarity.png\": 4203\n};\n\n\nfunction webpackContext(req) {\n\tvar id = webpackContextResolve(req);\n\treturn __webpack_require__(id);\n}\nfunction webpackContextResolve(req) {\n\tif(!__webpack_require__.o(map, req)) {\n\t\tvar e = new Error(\"Cannot find module '\" + req + \"'\");\n\t\te.code = 'MODULE_NOT_FOUND';\n\t\tthrow e;\n\t}\n\treturn map[req];\n}\nwebpackContext.keys = function webpackContextKeys() {\n\treturn Object.keys(map);\n};\nwebpackContext.resolve = webpackContextResolve;\nmodule.exports = webpackContext;\nwebpackContext.id = 9211;","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n","var deferred = [];\n__webpack_require__.O = function(result, chunkIds, fn, priority) {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every(function(key) { return __webpack_require__.O[key](chunkIds[j]); })) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","// define getter functions for harmony exports\n__webpack_require__.d = function(exports, definition) {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.o = function(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); }","// define __esModule on exports\n__webpack_require__.r = function(exports) {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.p = \"/\";","// no baseURI\n\n// object to store loaded and loading chunks\n// undefined = chunk not loaded, null = chunk preloaded/prefetched\n// [resolve, reject, Promise] = chunk loading, 0 = chunk loaded\nvar installedChunks = {\n\t143: 0\n};\n\n// no chunk on demand loading\n\n// no prefetching\n\n// no preloaded\n\n// no HMR\n\n// no HMR manifest\n\n__webpack_require__.O.j = function(chunkId) { return installedChunks[chunkId] === 0; };\n\n// install a JSONP callback for chunk loading\nvar webpackJsonpCallback = function(parentChunkLoadingFunction, data) {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\t// add \"moreModules\" to the modules object,\n\t// then flag all \"chunkIds\" as loaded and fire callback\n\tvar moduleId, chunkId, i = 0;\n\tif(chunkIds.some(function(id) { return installedChunks[id] !== 0; })) {\n\t\tfor(moduleId in moreModules) {\n\t\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t\t}\n\t\t}\n\t\tif(runtime) var result = runtime(__webpack_require__);\n\t}\n\tif(parentChunkLoadingFunction) parentChunkLoadingFunction(data);\n\tfor(;i < chunkIds.length; i++) {\n\t\tchunkId = chunkIds[i];\n\t\tif(__webpack_require__.o(installedChunks, chunkId) && installedChunks[chunkId]) {\n\t\t\tinstalledChunks[chunkId][0]();\n\t\t}\n\t\tinstalledChunks[chunkId] = 0;\n\t}\n\treturn __webpack_require__.O(result);\n}\n\nvar chunkLoadingGlobal = self[\"webpackChunkdyllew\"] = self[\"webpackChunkdyllew\"] || [];\nchunkLoadingGlobal.forEach(webpackJsonpCallback.bind(null, 0));\nchunkLoadingGlobal.push = webpackJsonpCallback.bind(null, chunkLoadingGlobal.push.bind(chunkLoadingGlobal));","// startup\n// Load entry module and return exports\n// This entry module depends on other loaded chunks and execution need to be delayed\nvar __webpack_exports__ = __webpack_require__.O(undefined, [998], function() { return __webpack_require__(2846); })\n__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n"],"names":["render","_vm","this","_c","_self","staticClass","attrs","$route","path","_e","staticRenderFns","on","goHome","_v","_m","name","methods","component","components","Header","NavBar","goToAbout","require","goToResume","goToProjects","goToArtwork","data","windowWidth","scrollUp","MAIN_PROJECTS","id","link","src","imgFilename","title","desc","projectWebsite","btnText","url","sizeClass","ML_MODULES","projectBtnText","scrollUpFunc","behavior","window","scrollTo","top","left","enableSwipeOnCarousel","carouselID","carouselElem","document","getElementById","carousel","Carousel","addEventListener","event","xClick","touches","pageX","touchMoveHandler","makeTouchMoveHandler","once","removeEventListener","xMove","sensitivityInPx","Math","floor","next","prev","_l","projects","project","key","_s","class","getImgURL","$event","goToProjectPage","props","computed","ProjectCard","modules","module","mounted","Vue","VueRouter","router","Home","About","Projects","Boomerang","MLForCrowdsourcedCrisisData","ImageAnalysisCarousel","TextAnalysisCarousel","NLPIntDevGrayLit","TrumpSpeechAnalysis","GNNsTaxiPrediction","ClimateChangeNews","Artwork","Resume","NotFoundComponent","redirect","vueRouter","mode","routes","BootstrapVue","IconsPlugin","h","App","$mount","map","webpackContext","req","webpackContextResolve","__webpack_require__","o","e","Error","code","keys","Object","resolve","exports","__webpack_module_cache__","moduleId","cachedModule","undefined","__webpack_modules__","call","m","deferred","O","result","chunkIds","fn","priority","notFulfilled","Infinity","i","length","fulfilled","j","every","splice","r","d","definition","defineProperty","enumerable","get","g","globalThis","Function","obj","prop","prototype","hasOwnProperty","Symbol","toStringTag","value","p","installedChunks","chunkId","webpackJsonpCallback","parentChunkLoadingFunction","moreModules","runtime","some","chunkLoadingGlobal","self","forEach","bind","push","__webpack_exports__"],"sourceRoot":""}